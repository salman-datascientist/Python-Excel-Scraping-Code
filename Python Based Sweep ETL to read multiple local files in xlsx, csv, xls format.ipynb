{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0094ccd4-3af2-4bae-af22-2297c87eeeef",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Notes for running this script\n",
    "\n",
    "\n",
    "* Re-Save .CSV files as .XLSX files format (script does not handle .CSV files)\n",
    "* Re-Save .xls (old excel format) to new .XLSX file format\n",
    "* First Row in every sheet/worktab should always be a header\n",
    "* Sensitive information should be reviewed and masked (for changes in the code need to be eplicitly mentioned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bc1e08f-951a-4e2c-add1-1e954c941e06",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Files and Folders Cleanup Maintanence Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bdd2e0f-56f1-49c3-b833-a9358a01761f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#folder_path = 'insert here'\n",
    "\n",
    "#try:\n",
    "#    #os.rmdir(folder_path)\n",
    "#    shutil.rmtree(folder_path)\n",
    "#    print(f\"Successfully removed empty folder: {folder_path}\")\n",
    "#except OSError as e:\n",
    "#    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9520fa94-bbec-4157-b3e1-6d79140ff8fa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Email Campaigns published on sharepoint\n",
    "\n",
    "https://towardsdatascience.com/cleansing-and-transforming-schema-drifted-csv-files-into-relational-data-in-azure-databricks-519e82ea84ff\n",
    "\n",
    "\n",
    "https://adb-1411802526182681.1.azuredatabricks.net/?o=1411802526182681#notebook/696832424309190/command/4102857815304094"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f10e0a7-15bd-4f00-baa1-2ac47d4d66f6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##  Beginners guide for text preprocessing in NLP\n",
    "\n",
    "* https://swatimeena989.medium.com/beginners-guide-for-preprocessing-text-data-f3156bec85ca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9891fb39-84ff-4a5e-b01e-384bd61f5de0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Import Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ffe3997-c375-4ec5-8383-7b15aa94a790",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip available: \u001B[0m\u001B[31;49m22.2.2\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.3.1\u001B[0m\r\n\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "%pip install rapidfuzz --quiet --ignore-installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86c19674-d0b4-4936-aa1c-376648fd050d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install xlrd --quiet --ignore-installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d23434c-5b29-4107-abc1-68ef9b05be76",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import openpyxl\n",
    "import xlrd\n",
    "import shutil\n",
    "import datetime\n",
    "import re              # regular expressions\n",
    "import rapidfuzz\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Set the display option to show all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Set the display option to show the full content of 'LongString' column\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00cdd6b6-fbc1-4614-81ad-de1830b1efe5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Import Python Libraries for NLP tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "231dbbad-be0d-40da-9af5-86fbd57b68a9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import unicodedata\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "661ed60e-4066-427c-bb5d-5c8eee8b558a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Set Default folder for reading and moving files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a23a31da-bc4e-4046-bc69-688036e6109b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set the working directory to the directory containing the list of Excel files\n",
    "os.chdir('insert here')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8e53a3b-9bc1-404b-817b-035af9ce2900",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Source Files Location where files new files are uploaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61526d42-789e-490e-9976-8c7c86f711f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dbfs/FileStore/shared_uploads/salman.8.ahmed@gsk.com/SLE MUN Email Campaign Monthly Refresh/place_source_files_inbox/\n"
     ]
    }
   ],
   "source": [
    "# Define the folder path where you want to search for Excel files\n",
    "folder_path = \"insert here\" # Replace with the actual destination directory path with forward slash in the end\n",
    "print(folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ca095df-710c-4601-8de3-bc8b1eaa7bfc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Error Files Location where files are moved when Error occured reading files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91540cd6-b2e1-495e-b3de-5c4508d23f76",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dbfs/FileStore/shared_uploads/salman.8.ahmed@gsk.com/SLE MUN Email Campaign Monthly Refresh/error_files/\n"
     ]
    }
   ],
   "source": [
    "# Specify the destination directory where you want to move the files if libraries are not able to read or open the files\n",
    "error_folder_path = \"insert here\"  # Replace with the actual destination directory path with forward slash in the end\n",
    "print(error_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbdd153b-aec2-49ef-9445-0c4eb63d7eac",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Successful Files Location where files are moved after Successful Read of file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cc0d61a-7d8f-42dc-afd2-b8dd7b3ab9ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dbfs/FileStore/shared_uploads/salman.8.ahmed@gsk.com/SLE MUN Email Campaign Monthly Refresh/successful_read_files/\n"
     ]
    }
   ],
   "source": [
    "# Specify the destination directory where you want to move the files when libraries opened and read the files completely\n",
    "successful_read_files_foder_path = \"insert here\"  # Replace with the actual destination directory path with forward slash in the end\n",
    "print(successful_read_files_foder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a883c741-133e-4599-9919-a3fd7b738f45",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## New Folder Creation using Date Time Stamp in Error and Successful folders\n",
    "\n",
    "* Check new files are there in inbox folder\n",
    "* Create a new folder with current date time stamp\n",
    "* Move the files to the new folder with current date time stamp - to check how many files were read and check the audit log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9cb8df3-4852-433d-b500-73699f5e7ecf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get a list of all files in the folder\n",
    "file_list = os.listdir(folder_path)\n",
    "\n",
    "# Initialize a variable to track if there are any files in the folder\n",
    "excel_files_exist = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa09e8b3-a07c-4d92-b052-416b03074086",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folder place_source_files_inbox is empty or contains only subdirectories.\n"
     ]
    }
   ],
   "source": [
    "# Define a list of valid Excel file extensions\n",
    "excel_extensions = ['.xlsx', '.xls', '.csv']\n",
    "\n",
    "# Iterate through the items to check if any of them are files\n",
    "for file_name in file_list:\n",
    "  # Check if the file extension is in the list of valid Excel extensions\n",
    "  if any(file_name.endswith(ext) for ext in excel_extensions):\n",
    "    excel_files_exist = True\n",
    "    break  # Exit the loop as soon as an Excel file is found\n",
    "\n",
    "# Check if excel files exist in the folder\n",
    "if excel_files_exist:\n",
    "  print(\"There are new files in the folder place_source_files_inbox.\")\n",
    "else:\n",
    "  print(\"The folder place_source_files_inbox is empty or contains only subdirectories.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "546e9af2-a6e7-4e4d-acb1-e035f8e9e824",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New folders not created as source folder is empty.\n"
     ]
    }
   ],
   "source": [
    "if excel_files_exist:\n",
    "\n",
    "    # Get the current date and time\n",
    "    current_datetime = datetime.datetime.now()\n",
    "\n",
    "    # Format the current date and time as a string (e.g., \"2023-10-26_145855\" for October 26, 2023, 14:58:55)\n",
    "    timestamp = current_datetime.strftime(\"%Y-%m-%d_%H%M%S\")\n",
    "\n",
    "    # Create the new folder with the timestamp\n",
    "    new_folder_name_error = os.path.join(error_folder_path, timestamp)\n",
    "\n",
    "    try:\n",
    "        os.mkdir(new_folder_name_error)\n",
    "        print(f\"Created folder in Error Folder: {new_folder_name_error}\")\n",
    "    except OSError as e:\n",
    "        print(f\"Failed to create folder in Error Folder: {new_folder_name_error}\")\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    # Create the new folder with the timestamp\n",
    "    new_folder_name_success = os.path.join(successful_read_files_foder_path, timestamp)\n",
    "\n",
    "    try:\n",
    "        os.mkdir(new_folder_name_success)\n",
    "        print(f\"Created folder in Successful Read Folder: {new_folder_name_success}\")\n",
    "    except OSError as e:\n",
    "        print(f\"Failed to create folder in Successful Read Folder: {new_folder_name_success}\")\n",
    "        print(f\"Error: {e}\")\n",
    "else:\n",
    "    print(\"New folders not created as source folder is empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b02a78a-241b-428a-8b73-e972a108d836",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Step 1: Script to read all .xls, .xlsx and .csv files from the location and keep only the files that can be read properly\n",
    "\n",
    "* Step 1: If read file lead to success then move to success folder\n",
    "* Step 2: If read file lead to error then move to error folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34b8f1f2-6904-4b83-83b9-8d801777bb10",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read File Logic was not run as source folder is empty.\n"
     ]
    }
   ],
   "source": [
    "if excel_files_exist:\n",
    "    # Use the glob.glob() function to find all Excel files in the folder\n",
    "    excel_files = glob.glob(os.path.join(folder_path, \"*.xlsx\")) + glob.glob(os.path.join(folder_path, \"*.xls\")) + glob.glob(os.path.join(folder_path, \"*.csv\"))\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    for excel_file_path in excel_files:\n",
    "\n",
    "        file_name_with_extension = os.path.basename(excel_file_path)\n",
    "        file_full_path = folder_path + file_name_with_extension\n",
    "\n",
    "        # Check the file extension as previous code did not run correctly\n",
    "        file_extension = os.path.splitext(excel_file_path)[1].lower()\n",
    "        print(file_extension)\n",
    "        print(file_name_with_extension)\n",
    "        print(file_full_path)\n",
    "    \n",
    "        if file_extension == \".xls\":\n",
    "            # Handle .xls files using xlrd\n",
    "            print(\"This is an XLS (Excel 2003) file. Attempting to Read the file\")\n",
    "            try:\n",
    "                workbook_xlrd = xlrd.open_workbook(file_full_path)\n",
    "                i = i+1 \n",
    "                # Check the number of worksheets using xlrd\n",
    "                sheet_count_xlrd = len(workbook_xlrd.sheet_names())\n",
    "        \n",
    "                if sheet_count_xlrd > 1:\n",
    "                    # Iterate through all sheets in the workbook_xlrd\n",
    "                    #for sheet in workbook.sheets():\n",
    "                    for sheet_name in workbook_xlrd.sheet_names():\n",
    "                        sheet = workbook_xlrd.sheet_by_name(sheet_name)\n",
    "                        max_row = sheet.nrows\n",
    "                        max_column = sheet.ncols\n",
    "                        print(f\"The Excel file '{file_full_path}' (using xlrd) has {sheet_count_xlrd} worksheets and {sheet_name} has {max_row} rows and {max_column} columns.\")\n",
    "                elif sheet_count_xlrd == 1:\n",
    "                    # Iterate through the single sheet in the workbook_xlrd\n",
    "                    #for sheet in workbook.sheets():\n",
    "                    for sheet_name in workbook_xlrd.sheet_names():\n",
    "                        sheet = workbook_xlrd.sheet_by_name(sheet_name)\n",
    "                        max_row = sheet.nrows\n",
    "                        max_column = sheet.ncols\n",
    "                        print(f\"The Excel file '{file_full_path}' (using xlrd) has only one worksheet and has {max_row} rows and {max_column} columns.\")\n",
    "                else:\n",
    "                    print(f\"The Excel file '{file_full_path}' (using xlrd) is empty.\")\n",
    "                \n",
    "                # Construct the full path to the destination file in the new directory\n",
    "                destination_file_path = os.path.join(new_folder_name_success, file_name_with_extension)\n",
    "                shutil.move(file_full_path, destination_file_path)\n",
    "                print(f\"File '{file_name_with_extension}' moved to '{new_folder_name_success}'\")\n",
    "            except xlrd.XLRDError as e:\n",
    "                print(f\"An error occurred while trying to read the Excel file: {e}\")\n",
    "                destination_file_path = os.path.join(new_folder_name_error, file_name_with_extension)\n",
    "                # Move the file to the destination directory\n",
    "                shutil.move(file_full_path, destination_file_path)\n",
    "                print(f\"File '{file_name_with_extension}' moved to '{new_folder_name_error}'\")     \n",
    "            except FileNotFoundError:    \n",
    "                print(f\"The file '{file_full_path}' was not found.\")\n",
    "                destination_file_path = os.path.join(new_folder_name_error, file_name_with_extension)\n",
    "                # Move the file to the destination directory\n",
    "                shutil.move(file_full_path, destination_file_path)\n",
    "                print(f\"File '{file_name_with_extension}' moved to '{new_folder_name_error}'\")          \n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {e}\")\n",
    "                # Construct the full path to the destination file in the new directory\n",
    "                destination_file_path = os.path.join(new_folder_name_error, file_name_with_extension)\n",
    "                # Move the file to the destination directory\n",
    "                shutil.move(file_full_path, destination_file_path)\n",
    "                print(f\"File '{file_name_with_extension}' moved to '{new_folder_name_error}'\")\n",
    "            finally:\n",
    "                # Close the workbook if it was successfully opened\n",
    "                if 'workbook_xlrd' in locals():\n",
    "                    workbook_xlrd.release_resources()        \n",
    "        elif file_extension == \".xlsx\":\n",
    "            # Handle .xlsx files using openpyxl\n",
    "            print(\"This is an XLSX (Excel 2007 or later) file. Attempting to Read the file\")\n",
    "            # Load the Excel workbook in .xlsx format\n",
    "            try:\n",
    "                workbook = openpyxl.load_workbook(file_full_path, data_only=True, read_only=True) #important condition to ignore formatting of data\n",
    "                i = i+1\n",
    "                # Check the number of worksheets\n",
    "                sheet_count = len(workbook.sheetnames)\n",
    "                if sheet_count > 1:\n",
    "                    # Iterate through all sheets in the workbook\n",
    "                    for sheet in workbook:\n",
    "                        max_row = sheet.max_row\n",
    "                        max_column = sheet.max_column\n",
    "                        sheet_name = sheet.title\n",
    "                        print(f\"The Excel file '{file_full_path}' (using openxl) has {sheet_count} worksheets and {sheet_name} has {max_row} rows and {max_column} columns.\")\n",
    "                elif sheet_count == 1:\n",
    "                    for sheet in workbook:\n",
    "                        max_row = sheet.max_row\n",
    "                        max_column = sheet.max_column\n",
    "                        sheet_name = sheet.title\n",
    "                        print(f\"The Excel file '{file_full_path}' (using openxl)  has only one worksheet and has {max_row} rows and {max_column} columns.\")\n",
    "                else:\n",
    "                    print(f\"The Excel file '{file_full_path}' (using openxl) is empty.\")\n",
    "\n",
    "                # Close the workbook\n",
    "                workbook.close()\n",
    "                # Construct the full path to the destination file in the new directory\n",
    "                destination_file_path = os.path.join(new_folder_name_success, file_name_with_extension)\n",
    "                shutil.move(file_full_path, destination_file_path)\n",
    "                print(f\"File '{file_name_with_extension}' moved to '{new_folder_name_success}'\")\n",
    "            except FileNotFoundError:\n",
    "                print(f\"The file '{file_full_path}' was not found.\")\n",
    "                # Construct the full path to the destination file in the new directory\n",
    "                destination_file_path = os.path.join(new_folder_name_error, file_name_with_extension)\n",
    "                # Move the file to the destination directory\n",
    "                shutil.move(file_full_path, destination_file_path)          \n",
    "                print(f\"File '{file_name_with_extension}' moved to '{new_folder_name_error}'\")\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {e}\")\n",
    "                # Construct the full path to the destination file in the new directory\n",
    "                destination_file_path = os.path.join(new_folder_name_error, file_name_with_extension)\n",
    "                # Move the file to the destination directory\n",
    "                shutil.move(file_full_path, destination_file_path)          \n",
    "                print(f\"File '{file_name_with_extension}' moved to '{new_folder_name_error}'\")\n",
    "            finally:\n",
    "                #Close the workbook\n",
    "                workbook.close()\n",
    "        elif file_extension == \".csv\":\n",
    "            # Handle .csv files using read_csv\n",
    "            print(\"This is an CSV file. Attempting to Read the file\")\n",
    "            try:\n",
    "                #csv_file = pd.read_csv(file_full_path, data_only=True, read_only=True) #important condition to ignore formatting of data \n",
    "                csv_file = pd.read_csv(file_full_path) #important condition to ignore formatting of data \n",
    "                i = i + 1\n",
    "                # Getting the number of rows and columns\n",
    "                num_rows, num_columns = csv_file.shape\n",
    "                if num_rows > 1 and num_columns > 1:\n",
    "                    print(f\"The CSV file '{file_full_path}' (using read_csv) has {num_rows} rows and {num_columns} columns.\")\n",
    "                else:\n",
    "                    print(f\"The CSV file '{file_full_path}' (using read_csv) is empty.\")\n",
    "                # Construct the full path to the destination file in the new directory\n",
    "                destination_file_path = os.path.join(new_folder_name_success, file_name_with_extension)\n",
    "                shutil.move(file_full_path, destination_file_path)\n",
    "                print(f\"File '{file_name_with_extension}' moved to '{new_folder_name_success}'\")\n",
    "            except FileNotFoundError:\n",
    "                print(f\"The file '{file_full_path}' was not found.\")\n",
    "                # Construct the full path to the destination file in the new directory\n",
    "                destination_file_path = os.path.join(new_folder_name_error, file_name_with_extension)\n",
    "                # Move the file to the destination directory\n",
    "                shutil.move(file_full_path, destination_file_path)          \n",
    "                print(f\"File '{file_name_with_extension}' moved to '{new_folder_name_error}'\")\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {e}\")\n",
    "                # Construct the full path to the destination file in the new directory\n",
    "                destination_file_path = os.path.join(new_folder_name_error, file_name_with_extension)\n",
    "                # Move the file to the destination directory\n",
    "                shutil.move(file_full_path, destination_file_path)          \n",
    "                print(f\"File '{file_name_with_extension}' moved to '{new_folder_name_error}'\")                \n",
    "        else:\n",
    "            # Handle other file types or show an error message\n",
    "            print(\"This is neither a CSV nor an XLS nor an XLSX file.\")\n",
    "            # Construct the full path to the destination file in the new directory\n",
    "            destination_file_path = os.path.join(new_folder_name_error, file_name_with_extension)\n",
    "            # Move the file to the destination directory\n",
    "            shutil.move(file_full_path, destination_file_path)          \n",
    "            print(f\"File '{file_name_with_extension}' moved to '{new_folder_name_error}'\")  \n",
    "\n",
    "    print(f\"The total number of files successfully read is '{i}' files.\")\n",
    "else:\n",
    "    print(\"Read File Logic was not run as source folder is empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63df29c8-b1e4-4862-9de7-7aae01cf9383",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Step 2: Read all the good excel files into pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3055ff97-017c-4e0f-9aac-7e9a0fc0061a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(new_folder_name_success)\n",
    "print(new_folder_name_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "622bd4d2-3d29-4388-a13a-ee0514dd0d21",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Step 3: Read all the good excel files into pandas dataframe dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df24632d-603d-4b54-9d0a-31e8ba33c4ab",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### ETL Bronze Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b253886a-cc90-47bc-935f-1923d954a9c3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".xlsx\nPROD_A338_LN_Unmet_Needs_11302022.xlsx\n/dbfs/FileStore/shared_uploads/salman.8.ahmed@gsk.com/SLE MUN Email Campaign Monthly Refresh/successful_read_files/2023-11-29_152902/PROD_A338_LN_Unmet_Needs_11302022.xlsx\nThis is an XLSX (Excel 2007 or later) file. Attempting to Read the file\n'/dbfs/FileStore/shared_uploads/salman.8.ahmed@gsk.com/SLE MUN Email Campaign Monthly Refresh/successful_read_files/2023-11-29_152902/PROD_A338_LN_Unmet_Needs_11302022.xlsx' has '<openpyxl.worksheet._read_only.ReadOnlyWorksheet object at 0x7f5736b1df60>' with '6597' rows and '32' columns.\n.xlsx\nPROD_A338_LN_Unmet_Needs_12072022.xlsx\n/dbfs/FileStore/shared_uploads/salman.8.ahmed@gsk.com/SLE MUN Email Campaign Monthly Refresh/successful_read_files/2023-11-29_152902/PROD_A338_LN_Unmet_Needs_12072022.xlsx\nThis is an XLSX (Excel 2007 or later) file. Attempting to Read the file\n'/dbfs/FileStore/shared_uploads/salman.8.ahmed@gsk.com/SLE MUN Email Campaign Monthly Refresh/successful_read_files/2023-11-29_152902/PROD_A338_LN_Unmet_Needs_12072022.xlsx' has '<openpyxl.worksheet._read_only.ReadOnlyWorksheet object at 0x7f57367f0640>' with '6597' rows and '32' columns.\n.xlsx\nPROD_A357_Post_ASN_12092022.xlsx\n/dbfs/FileStore/shared_uploads/salman.8.ahmed@gsk.com/SLE MUN Email Campaign Monthly Refresh/successful_read_files/2023-11-29_152902/PROD_A357_Post_ASN_12092022.xlsx\nThis is an XLSX (Excel 2007 or later) file. Attempting to Read the file\n'/dbfs/FileStore/shared_uploads/salman.8.ahmed@gsk.com/SLE MUN Email Campaign Monthly Refresh/successful_read_files/2023-11-29_152902/PROD_A357_Post_ASN_12092022.xlsx' has '<openpyxl.worksheet._read_only.ReadOnlyWorksheet object at 0x7f573666dc00>' with '10864' rows and '32' columns.\n.xlsx\nPROD_A375_LN_Unmet_Needs_Email3_12212022.xlsx\n/dbfs/FileStore/shared_uploads/salman.8.ahmed@gsk.com/SLE MUN Email Campaign Monthly Refresh/successful_read_files/2023-11-29_152902/PROD_A375_LN_Unmet_Needs_Email3_12212022.xlsx\nThis is an XLSX (Excel 2007 or later) file. Attempting to Read the file\n'/dbfs/FileStore/shared_uploads/salman.8.ahmed@gsk.com/SLE MUN Email Campaign Monthly Refresh/successful_read_files/2023-11-29_152902/PROD_A375_LN_Unmet_Needs_Email3_12212022.xlsx' has '<openpyxl.worksheet._read_only.ReadOnlyWorksheet object at 0x7f573688cbe0>' with '6597' rows and '32' columns.\n.xlsx\nPROD_A375_LN_Unmet_Needs_Email_12152022.xlsx\n/dbfs/FileStore/shared_uploads/salman.8.ahmed@gsk.com/SLE MUN Email Campaign Monthly Refresh/successful_read_files/2023-11-29_152902/PROD_A375_LN_Unmet_Needs_Email_12152022.xlsx\nThis is an XLSX (Excel 2007 or later) file. Attempting to Read the file\n'/dbfs/FileStore/shared_uploads/salman.8.ahmed@gsk.com/SLE MUN Email Campaign Monthly Refresh/successful_read_files/2023-11-29_152902/PROD_A375_LN_Unmet_Needs_Email_12152022.xlsx' has '<openpyxl.worksheet._read_only.ReadOnlyWorksheet object at 0x7f5734acf2b0>' with '6597' rows and '32' columns.\n.xlsx\nPROD_A416_LN_Unmet_Needs_Email4_01112023.xlsx\n/dbfs/FileStore/shared_uploads/salman.8.ahmed@gsk.com/SLE MUN Email Campaign Monthly Refresh/successful_read_files/2023-11-29_152902/PROD_A416_LN_Unmet_Needs_Email4_01112023.xlsx\nThis is an XLSX (Excel 2007 or later) file. Attempting to Read the file\n'/dbfs/FileStore/shared_uploads/salman.8.ahmed@gsk.com/SLE MUN Email Campaign Monthly Refresh/successful_read_files/2023-11-29_152902/PROD_A416_LN_Unmet_Needs_Email4_01112023.xlsx' has '<openpyxl.worksheet._read_only.ReadOnlyWorksheet object at 0x7f573688c700>' with '6597' rows and '32' columns.\n.xlsx\nPROD_A416_LN_Unmet_Needs_Email4_01182023.xlsx\n/dbfs/FileStore/shared_uploads/salman.8.ahmed@gsk.com/SLE MUN Email Campaign Monthly Refresh/successful_read_files/2023-11-29_152902/PROD_A416_LN_Unmet_Needs_Email4_01182023.xlsx\nThis is an XLSX (Excel 2007 or later) file. Attempting to Read the file\n'/dbfs/FileStore/shared_uploads/salman.8.ahmed@gsk.com/SLE MUN Email Campaign Monthly Refresh/successful_read_files/2023-11-29_152902/PROD_A416_LN_Unmet_Needs_Email4_01182023.xlsx' has '<openpyxl.worksheet._read_only.ReadOnlyWorksheet object at 0x7f572fd022f0>' with '6597' rows and '32' columns.\n.xlsx\nPROD_A417_Post_ACR_01092022.xlsx\n/dbfs/FileStore/shared_uploads/salman.8.ahmed@gsk.com/SLE MUN Email Campaign Monthly Refresh/successful_read_files/2023-11-29_152902/PROD_A417_Post_ACR_01092022.xlsx\nThis is an XLSX (Excel 2007 or later) file. Attempting to Read the file\n'/dbfs/FileStore/shared_uploads/salman.8.ahmed@gsk.com/SLE MUN Email Campaign Monthly Refresh/successful_read_files/2023-11-29_152902/PROD_A417_Post_ACR_01092022.xlsx' has '<openpyxl.worksheet._read_only.ReadOnlyWorksheet object at 0x7f57346e2f20>' with '105' rows and '28' columns.\n.xlsx\nPROD_A450_LN_Unmet_Needs_EM5_02142023.xlsx\n/dbfs/FileStore/shared_uploads/salman.8.ahmed@gsk.com/SLE MUN Email Campaign Monthly Refresh/successful_read_files/2023-11-29_152902/PROD_A450_LN_Unmet_Needs_EM5_02142023.xlsx\nThis is an XLSX (Excel 2007 or later) file. Attempting to Read the file\n'/dbfs/FileStore/shared_uploads/salman.8.ahmed@gsk.com/SLE MUN Email Campaign Monthly Refresh/successful_read_files/2023-11-29_152902/PROD_A450_LN_Unmet_Needs_EM5_02142023.xlsx' has '<openpyxl.worksheet._read_only.ReadOnlyWorksheet object at 0x7f57346e2380>' with '6597' rows and '31' columns.\n.xlsx\nPROD_A450_LN_Unmet_Needs_EM5_02212023.xlsx\n/dbfs/FileStore/shared_uploads/salman.8.ahmed@gsk.com/SLE MUN Email Campaign Monthly Refresh/successful_read_files/2023-11-29_152902/PROD_A450_LN_Unmet_Needs_EM5_02212023.xlsx\nThis is an XLSX (Excel 2007 or later) file. Attempting to Read the file\n'/dbfs/FileStore/shared_uploads/salman.8.ahmed@gsk.com/SLE MUN Email Campaign Monthly Refresh/successful_read_files/2023-11-29_152902/PROD_A450_LN_Unmet_Needs_EM5_02212023.xlsx' has '<openpyxl.worksheet._read_only.ReadOnlyWorksheet object at 0x7f5736abbd90>' with '6597' rows and '32' columns.\n.xlsx\nPROD_A694_Anemia_CKD_Email_09012023.xlsx\n/dbfs/FileStore/shared_uploads/salman.8.ahmed@gsk.com/SLE MUN Email Campaign Monthly Refresh/successful_read_files/2023-11-29_152902/PROD_A694_Anemia_CKD_Email_09012023.xlsx\nThis is an XLSX (Excel 2007 or later) file. Attempting to Read the file\n'/dbfs/FileStore/shared_uploads/salman.8.ahmed@gsk.com/SLE MUN Email Campaign Monthly Refresh/successful_read_files/2023-11-29_152902/PROD_A694_Anemia_CKD_Email_09012023.xlsx' has '<openpyxl.worksheet._read_only.ReadOnlyWorksheet object at 0x7f57367f3010>' with '6253' rows and '32' columns.\n.xlsx\nPROD_A695_MedAffairs_shinglesrheumatology_10202023.xlsx\n/dbfs/FileStore/shared_uploads/salman.8.ahmed@gsk.com/SLE MUN Email Campaign Monthly Refresh/successful_read_files/2023-11-29_152902/PROD_A695_MedAffairs_shinglesrheumatology_10202023.xlsx\nThis is an XLSX (Excel 2007 or later) file. Attempting to Read the file\n'/dbfs/FileStore/shared_uploads/salman.8.ahmed@gsk.com/SLE MUN Email Campaign Monthly Refresh/successful_read_files/2023-11-29_152902/PROD_A695_MedAffairs_shinglesrheumatology_10202023.xlsx' has '<openpyxl.worksheet._read_only.ReadOnlyWorksheet object at 0x7f57342b1690>' with '6207' rows and '32' columns.\n.xlsx\nPROD_A738_MedicalAffairs_DMD_AIMXR_OrganDamage_10122023.xlsx\n/dbfs/FileStore/shared_uploads/salman.8.ahmed@gsk.com/SLE MUN Email Campaign Monthly Refresh/successful_read_files/2023-11-29_152902/PROD_A738_MedicalAffairs_DMD_AIMXR_OrganDamage_10122023.xlsx\nThis is an XLSX (Excel 2007 or later) file. Attempting to Read the file\n'/dbfs/FileStore/shared_uploads/salman.8.ahmed@gsk.com/SLE MUN Email Campaign Monthly Refresh/successful_read_files/2023-11-29_152902/PROD_A738_MedicalAffairs_DMD_AIMXR_OrganDamage_10122023.xlsx' has '<openpyxl.worksheet._read_only.ReadOnlyWorksheet object at 0x7f573666ae60>' with '27' rows and '27' columns.\n.xlsx\nPROD_A740_MedicalAffairs_DMD_AIMXR_DiseaseModification_10122023.xlsx\n/dbfs/FileStore/shared_uploads/salman.8.ahmed@gsk.com/SLE MUN Email Campaign Monthly Refresh/successful_read_files/2023-11-29_152902/PROD_A740_MedicalAffairs_DMD_AIMXR_DiseaseModification_10122023.xlsx\nThis is an XLSX (Excel 2007 or later) file. Attempting to Read the file\n'/dbfs/FileStore/shared_uploads/salman.8.ahmed@gsk.com/SLE MUN Email Campaign Monthly Refresh/successful_read_files/2023-11-29_152902/PROD_A740_MedicalAffairs_DMD_AIMXR_DiseaseModification_10122023.xlsx' has '<openpyxl.worksheet._read_only.ReadOnlyWorksheet object at 0x7f572e8b3b50>' with '21' rows and '26' columns.\n.xlsx\nPROD_A741_MedicalAffairs_SLE_DMD_AIMXR_EarlyUse_10122023.xlsx\n/dbfs/FileStore/shared_uploads/salman.8.ahmed@gsk.com/SLE MUN Email Campaign Monthly Refresh/successful_read_files/2023-11-29_152902/PROD_A741_MedicalAffairs_SLE_DMD_AIMXR_EarlyUse_10122023.xlsx\nThis is an XLSX (Excel 2007 or later) file. Attempting to Read the file\n'/dbfs/FileStore/shared_uploads/salman.8.ahmed@gsk.com/SLE MUN Email Campaign Monthly Refresh/successful_read_files/2023-11-29_152902/PROD_A741_MedicalAffairs_SLE_DMD_AIMXR_EarlyUse_10122023.xlsx' has '<openpyxl.worksheet._read_only.ReadOnlyWorksheet object at 0x7f5735f9f010>' with '27' rows and '28' columns.\n.xlsx\nPROD_A742_MedicalAffairs_SLE_ACR_FinalDE20231024.xlsx\n/dbfs/FileStore/shared_uploads/salman.8.ahmed@gsk.com/SLE MUN Email Campaign Monthly Refresh/successful_read_files/2023-11-29_152902/PROD_A742_MedicalAffairs_SLE_ACR_FinalDE20231024.xlsx\nThis is an XLSX (Excel 2007 or later) file. Attempting to Read the file\n'/dbfs/FileStore/shared_uploads/salman.8.ahmed@gsk.com/SLE MUN Email Campaign Monthly Refresh/successful_read_files/2023-11-29_152902/PROD_A742_MedicalAffairs_SLE_ACR_FinalDE20231024.xlsx' has '<openpyxl.worksheet._read_only.ReadOnlyWorksheet object at 0x7f5736a48d00>' with '7348' rows and '35' columns.\n.xlsx\nProd_A743_PRE_ASN_Email_10182023.xlsx\n/dbfs/FileStore/shared_uploads/salman.8.ahmed@gsk.com/SLE MUN Email Campaign Monthly Refresh/successful_read_files/2023-11-29_152902/Prod_A743_PRE_ASN_Email_10182023.xlsx\nThis is an XLSX (Excel 2007 or later) file. Attempting to Read the file\n'/dbfs/FileStore/shared_uploads/salman.8.ahmed@gsk.com/SLE MUN Email Campaign Monthly Refresh/successful_read_files/2023-11-29_152902/Prod_A743_PRE_ASN_Email_10182023.xlsx' has '<openpyxl.worksheet._read_only.ReadOnlyWorksheet object at 0x7f5736b59e10>' with '8878' rows and '32' columns.\n"
     ]
    }
   ],
   "source": [
    "# Step 1 : Use the glob.glob() function to find all Excel files in the folder\n",
    "excel_files = glob.glob(os.path.join(new_folder_name_success, \"*.xlsx\")) + glob.glob(os.path.join(new_folder_name_success, \"*.xls\")) + glob.glob(os.path.join(new_folder_name_success, \"*.csv\"))\n",
    "\n",
    "# Partial Column Names  (do not delete)\n",
    "partial_column_names = [\"MobileNumber\", \"FirstName\", \"MiddleName\", \"LastName\", \"Addr1\", \"Addr2\", \"City\", \"State\", \"Zip\", \"OptOutURL\", \"Var1\", \"Var2\", \"Var3\", \"Var4\", \"CID\", \"SubscriberKey\", \"CreatedDate\", \"LastModifiedDate\", \"CampaignName\", \"SuppressionType\", \"CampaignRunID\", \"HighLowIP\", \"Sent\", \"sentdate\", \"Open\", \"open date\", \"click\", \"clickdate\", \"Hardbounce\", \"softbounce\", \"blockedbounce\", \"unsub\"]\n",
    "\n",
    "# List of strings to remove\n",
    "strings_to_remove = [\".xlsx\", \".xls\", \".csv\"]\n",
    "\n",
    "# Create a regular expression pattern by joining the strings with the \"|\" (OR) operator\n",
    "pattern = '|'.join(map(re.escape, strings_to_remove))\n",
    "\n",
    "# Create an empty dictionary to store Bronze DataFrames\n",
    "bronze_dataframes = {}\n",
    "\n",
    "i = 0\n",
    "\n",
    "# Step 2: Iterate through the files\n",
    "for excel_file_path in excel_files:\n",
    "\n",
    "  file_name_with_extension = os.path.basename(excel_file_path)\n",
    "  file_full_path = new_folder_name_success + file_name_with_extension\n",
    "\n",
    "  # Check the file extension as previous code did not run correctly\n",
    "  file_extension = os.path.splitext(excel_file_path)[1].lower()\n",
    "  print(file_extension)\n",
    "  print(file_name_with_extension)\n",
    "  print(file_full_path)\n",
    "  # Use re.sub() to remove the strings\n",
    "  file_name_without_extension = re.sub(pattern, '', file_name_with_extension)\n",
    "    \n",
    "  if file_extension == \".xls\":\n",
    "    # Handle .xls files using xlrd\n",
    "    print(\"This is an XLS (Excel 2003) file. Attempting to Read the file\")\n",
    "    workbook_xlrd = xlrd.open_workbook(file_full_path)\n",
    "    # Check the number of worksheets using xlrd\n",
    "    sheet_count_xlrd = len(workbook_xlrd.sheet_names())\n",
    "\n",
    "    if sheet_count_xlrd > 1:\n",
    "      # Iterate through all sheets in the workbook_xlrd\n",
    "      for sheet_name in workbook_xlrd.sheet_names():\n",
    "        sheet = workbook_xlrd.sheet_by_name(sheet_name)\n",
    "        max_row = sheet.nrows\n",
    "        max_column = sheet.ncols\n",
    "        print(f\"'{file_full_path}' has '{sheet}' with '{max_row}' rows and '{max_column}' columns.\")\n",
    "        if max_column > 1 and max_row > 1:\n",
    "          try:\n",
    "            # Read all worksheets into a dictionary of DataFrames\n",
    "            df = pd.read_excel(excel_file_path, sheet_name=sheet_name, header=0, engine='xlrd', dtype=str)\n",
    "\n",
    "            # Step 4: Check for partial column name matches\n",
    "            matching_columns = [col for col in df.columns if any(partial in col for partial in partial_column_names)]\n",
    "\n",
    "            if matching_columns:\n",
    "                # Step 5: Store the DataFrame with file name and worksheet name as keys\n",
    "                key = f\"{file_name_without_extension}_{sheet_name}\"\n",
    "                bronze_dataframes[key] = df\n",
    "          \n",
    "          except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "    elif sheet_count_xlrd == 1:\n",
    "      # Iterate through all sheets in the workbook_xlrd\n",
    "      for sheet_name in workbook_xlrd.sheet_names():\n",
    "        sheet = workbook_xlrd.sheet_by_name(sheet_name)\n",
    "        max_row = sheet.nrows\n",
    "        max_column = sheet.ncols\n",
    "        print(f\"'{file_full_path}' has '{sheet}' with '{max_row}' rows and '{max_column}' columns.\")\n",
    "        if max_column > 1 and max_row > 1:\n",
    "          try:\n",
    "            # Read all worksheets into a dictionary of DataFrames\n",
    "            df = pd.read_excel(excel_file_path, sheet_name=sheet_name, header=0, engine='xlrd', dtype=str)\n",
    "\n",
    "            # Step 4: Check for partial column name matches\n",
    "            matching_columns = [col for col in df.columns if any(partial in col for partial in partial_column_names)]\n",
    "\n",
    "            if matching_columns:\n",
    "              # Step 5: Store the DataFrame with file name and worksheet name as keys\n",
    "              key = f\"{file_name_without_extension}_{sheet_name}\"\n",
    "              bronze_dataframes[key] = df\n",
    "          \n",
    "          except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")                 \n",
    "\n",
    "    # Close the workbook if it was successfully opened\n",
    "    if 'workbook_xlrd' in locals():\n",
    "      workbook_xlrd.release_resources()  \n",
    "\n",
    "  elif file_extension == \".xlsx\":\n",
    "    # Handle .xlsx files using openpyxl\n",
    "    print(\"This is an XLSX (Excel 2007 or later) file. Attempting to Read the file\")\n",
    "    workbook_xlsx = openpyxl.load_workbook(file_full_path, data_only=True, read_only=True)           #important condition to ignore formatting of data\n",
    "    # Check the number of worksheets\n",
    "    sheet_count_xlsx = len(workbook_xlsx.sheetnames)\n",
    "\n",
    "    if sheet_count_xlsx > 1:\n",
    "      # Iterate through all sheets in the workbook_xlsx  \n",
    "      for sheet in workbook_xlsx:\n",
    "        sheet_name = sheet.title\n",
    "        max_row = sheet.max_row\n",
    "        max_column = sheet.max_column\n",
    "        print(f\"'{file_full_path}' has '{sheet}' with '{max_row}' rows and '{max_column}' columns.\")\n",
    "        if max_column > 1 and max_row > 1:\n",
    "          try:\n",
    "            # Read all worksheets into a dictionary of DataFrames\n",
    "            df = pd.read_excel(excel_file_path, sheet_name=sheet_name, header=0, engine='openpyxl', dtype=str)\n",
    "\n",
    "            # Step 4: Check for partial column name matches\n",
    "            matching_columns = [col for col in df.columns if any(partial in col for partial in partial_column_names)]\n",
    "\n",
    "            if matching_columns:\n",
    "              # Step 5: Store the DataFrame with file name and worksheet name as keys\n",
    "              key = f\"{file_name_without_extension}_{sheet_name}\"\n",
    "              bronze_dataframes[key] = df\n",
    "          \n",
    "          except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "    elif sheet_count_xlsx == 1:\n",
    "      # Iterate through all sheets in the workbook_xlsx\n",
    "      for sheet in workbook_xlsx:\n",
    "        sheet_name = sheet.title\n",
    "        max_row = sheet.max_row\n",
    "        max_column = sheet.max_column\n",
    "        print(f\"'{file_full_path}' has '{sheet}' with '{max_row}' rows and '{max_column}' columns.\")\n",
    "        if max_column > 1 and max_row > 1:\n",
    "          try:\n",
    "            # Read all worksheets into a dictionary of DataFrames\n",
    "            df = pd.read_excel(excel_file_path, sheet_name=sheet_name, header=0, engine='openpyxl', dtype=str)\n",
    "\n",
    "            # Step 4: Check for partial column name matches\n",
    "            matching_columns = [col for col in df.columns if any(partial in col for partial in partial_column_names)]\n",
    "\n",
    "            if matching_columns:\n",
    "              # Step 5: Store the DataFrame with file name and worksheet name as keys\n",
    "              key = f\"{file_name_without_extension}_{sheet_name}\"\n",
    "              bronze_dataframes[key] = df\n",
    "          except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "  elif file_extension == \".csv\":\n",
    "    df = pd.read_csv(excel_file_path, header=0, index_col=False, dtype=str)\n",
    "\n",
    "    # Get the number of rows and columns\n",
    "    max_row, max_column = df.shape\n",
    "    print(f\"'{file_full_path}' has '{sheet}' with '{max_row}' rows and '{max_column}' columns.\")\n",
    "    if max_column > 1 and max_row > 1:\n",
    "      try:\n",
    "        # Step 4: Check for partial column name matches\n",
    "        matching_columns = [col for col in df.columns if any(partial in col for partial in partial_column_names)]\n",
    "\n",
    "        if matching_columns:\n",
    "          # Step 5: Store the DataFrame with file name and worksheet name as keys\n",
    "          key = f\"{file_name_without_extension}\"\n",
    "          bronze_dataframes[key] = df\n",
    "      except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "390741a1-ab09-4519-837f-3393bfd94859",
     "showTitle": true,
     "title": "Audit Step to check number of files read into a data dictionary called as bronze layer"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: PROD_A338_LN_Unmet_Needs_11302022_PROD_A338_LN_Unmet_Needs_EM2_St, Data Type: <class 'pandas.core.frame.DataFrame'>\nKey: PROD_A338_LN_Unmet_Needs_12072022_PROD_A338_LN_Unmet_Needs_EM2_St, Data Type: <class 'pandas.core.frame.DataFrame'>\nKey: PROD_A357_Post_ASN_12092022_PROD_A357_Post_ASN_12092022, Data Type: <class 'pandas.core.frame.DataFrame'>\nKey: PROD_A375_LN_Unmet_Needs_Email3_12212022_PROD_A375_LN_Unmet_Needs_Email3, Data Type: <class 'pandas.core.frame.DataFrame'>\nKey: PROD_A375_LN_Unmet_Needs_Email_12152022_PROD_A375_LN_Unmet_Needs_Email3, Data Type: <class 'pandas.core.frame.DataFrame'>\nKey: PROD_A416_LN_Unmet_Needs_Email4_01112023_PROD_A416_LN_Unmet_Needs_Email4, Data Type: <class 'pandas.core.frame.DataFrame'>\nKey: PROD_A416_LN_Unmet_Needs_Email4_01182023_PROD_A416_LN_Unmet_Needs_Email4, Data Type: <class 'pandas.core.frame.DataFrame'>\nKey: PROD_A417_Post_ACR_01092022_PROD_A417_Post_ACR_STAGE_DE2023, Data Type: <class 'pandas.core.frame.DataFrame'>\nKey: PROD_A450_LN_Unmet_Needs_EM5_02142023_PROD_A450_LN_Unmet_Needs_EM5_St, Data Type: <class 'pandas.core.frame.DataFrame'>\nKey: PROD_A450_LN_Unmet_Needs_EM5_02212023_PROD_A450_LN_Unmet_Needs_EM5_St, Data Type: <class 'pandas.core.frame.DataFrame'>\nKey: PROD_A694_Anemia_CKD_Email_09012023_PROD_A694_Anemia_CKD_Email_Acti, Data Type: <class 'pandas.core.frame.DataFrame'>\nKey: PROD_A695_MedAffairs_shinglesrheumatology_10202023_PROD_A695_MedAffairs_shinglesrh, Data Type: <class 'pandas.core.frame.DataFrame'>\nKey: PROD_A738_MedicalAffairs_DMD_AIMXR_OrganDamage_10122023_PROD_A738_MedicalAffairs_DMD_AI, Data Type: <class 'pandas.core.frame.DataFrame'>\nKey: PROD_A740_MedicalAffairs_DMD_AIMXR_DiseaseModification_10122023_PROD_A740_MedicalAffairs_DMD_AI, Data Type: <class 'pandas.core.frame.DataFrame'>\nKey: PROD_A741_MedicalAffairs_SLE_DMD_AIMXR_EarlyUse_10122023_PROD_A741_MedicalAffairs_SLE_DM, Data Type: <class 'pandas.core.frame.DataFrame'>\nKey: PROD_A742_MedicalAffairs_SLE_ACR_FinalDE20231024_PROD_A742_MedicalAffairs_SLE_AC, Data Type: <class 'pandas.core.frame.DataFrame'>\nKey: Prod_A743_PRE_ASN_Email_10182023_Prod_A743_PRE_ASN_Email_StageDE, Data Type: <class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# Get the keys as a view object\n",
    "#keys_view = bronze_dataframes.keys()\n",
    "\n",
    "# Convert the view object to a list if needed\n",
    "#keys_list = list(keys_view)\n",
    "\n",
    "# Print the keys\n",
    "#print(keys_list)\n",
    "\n",
    "# Print for better layout\n",
    "#for key in bronze_dataframes.keys(): \n",
    "#    print(key)\n",
    "\n",
    "# Iterate through the dictionary and check the data type of each value\n",
    "for key, value in bronze_dataframes.items():\n",
    "    data_type = type(value)\n",
    "    print(f\"Key: {key}, Data Type: {data_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "577ce4dd-3633-4ef8-9b1b-bdcfd349a4ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Name: PROD_A338_LN_Unmet_Needs_11302022_PROD_A338_LN_Unmet_Needs_EM2_St\n  MobileNumber   FirstName MiddleName             LastName Addr1 Addr2 City  \\\n0          NaN  MUTHUKUMAR        NaN           THANGAMANI   NaN   NaN  NaN   \n1          NaN       TANYA        NaN                 TANG   NaN   NaN  NaN   \n2          NaN      CARLOS        NaN    CLAUDIO RODRIGUEZ   NaN   NaN  NaN   \n3          NaN        ANIL        NaN               RAMESH   NaN   NaN  NaN   \n4          NaN      DEBBIE        NaN               KURIAN   NaN   NaN  NaN   \n5          NaN      NAUMAN        NaN                TAHIR   NaN   NaN  NaN   \n6          NaN       MARIO        NaN  ROBLES FRANCESCHINI   NaN   NaN  NaN   \n7          NaN      ELLIOT        NaN               CHAREN   NaN   NaN  NaN   \n8          NaN      THEJAS        NaN                SWAMY   NaN   NaN  NaN   \n9          NaN        REJI        NaN                 NAIR   NaN   NaN  NaN   \n\n  State  Zip OptOutURL Var1 Var2 Var3 Var4      CID  \\\n0   NaN  NaN       NaN  NaN   MD  NaN  NaN  3183200   \n1   NaN  NaN       NaN  NaN   DO  NaN  NaN  3185280   \n2   NaN  NaN       NaN  NaN   MD  NaN  NaN  3197899   \n3   NaN  NaN       NaN  NaN   MD  NaN  NaN  3213692   \n4   NaN  NaN       NaN  NaN   MD  NaN  NaN  3216656   \n5   NaN  NaN       NaN  NaN   MD  NaN  NaN  3248135   \n6   NaN  NaN       NaN  NaN   MD  NaN  NaN  3265559   \n7   NaN  NaN       NaN  NaN   MD  NaN  NaN  3267422   \n8   NaN  NaN       NaN  NaN   MD  NaN  NaN  3291593   \n9   NaN  NaN       NaN  NaN   MD  NaN  NaN  3292238   \n\n                                SubscriberKey          CreatedDate  \\\n0  ADHOC-BD56F9C6-E6C7-4BF2-8CF0-443F0D7D85DC  2022-11-30 08:10:00   \n1  ADHOC-3266D95D-953F-411B-BACC-F52C897F3F1A  2022-11-30 08:10:00   \n2  ADHOC-7534B849-A5ED-4366-8115-02D7FE395E21  2022-11-30 08:10:00   \n3  ADHOC-F6EB4600-33B6-4B0E-AB77-DCC81D93F4AF  2022-11-30 08:10:00   \n4  ADHOC-A10F150D-8203-4375-9CAA-54E3EBB92B8E  2022-11-30 08:10:00   \n5  ADHOC-F6583522-2261-4553-8B25-9D6C56E98F29  2022-11-30 08:10:00   \n6  ADHOC-BDD1EFE9-0652-419E-9628-FDE158F93445  2022-11-30 08:10:00   \n7  ADHOC-E721CA04-5192-459F-A948-375A05A1ED9D  2022-11-30 08:10:00   \n8  ADHOC-F1D8F26A-F65B-4053-ACB7-F148301DDFF2  2022-11-30 08:10:00   \n9  ADHOC-0FC07E67-891C-4AA3-BA4A-12FE7CD54D53  2022-11-30 08:10:00   \n\n      LastModifiedDate CampaignName       SuppressionType CampaignRunID  \\\n0  2022-11-30 07:13:00   A338-ADHOC                   NaN          A338   \n1  2022-11-30 07:13:00   A338-ADHOC          Gigya OptOut          A338   \n2  2022-11-30 07:13:00   A338-ADHOC                   NaN          A338   \n3  2022-11-30 07:13:00   A338-ADHOC                   NaN          A338   \n4  2022-11-30 07:17:00   A338-ADHOC  Blacklist/Quarantine          A338   \n5  2022-11-30 07:13:00   A338-ADHOC                   NaN          A338   \n6  2022-11-30 07:13:00   A338-ADHOC                   NaN          A338   \n7  2022-11-30 07:13:00   A338-ADHOC                   NaN          A338   \n8  2022-11-30 07:13:00   A338-ADHOC                   NaN          A338   \n9  2022-11-30 07:13:00   A338-ADHOC                   NaN          A338   \n\n  HighLowIP Sent             sentdate Open open date click clickdate  \\\n0      High    Y  2022-11-30 07:30:00  NaN       NaN   NaN       NaN   \n1      High  NaN                  NaN  NaN       NaN   NaN       NaN   \n2      High    Y  2022-11-30 07:30:00  NaN       NaN   NaN       NaN   \n3      High    Y  2022-11-30 07:30:00  NaN       NaN   NaN       NaN   \n4      High  NaN                  NaN  NaN       NaN   NaN       NaN   \n5      High    Y  2022-11-30 07:30:00  NaN       NaN   NaN       NaN   \n6      High    Y  2022-11-30 07:30:00  NaN       NaN   NaN       NaN   \n7      High    Y  2022-11-30 07:30:00  NaN       NaN   NaN       NaN   \n8      High    Y  2022-11-30 07:30:00  NaN       NaN   NaN       NaN   \n9      High    Y  2022-11-30 07:30:00  NaN       NaN   NaN       NaN   \n\n  Hardbounce softbounce blockedbounce unsub  \n0        NaN        NaN           NaN   NaN  \n1        NaN        NaN           NaN   NaN  \n2        NaN        NaN           NaN   NaN  \n3        NaN        NaN           NaN   NaN  \n4        NaN        NaN           NaN   NaN  \n5        NaN        NaN           NaN   NaN  \n6        NaN        NaN           NaN   NaN  \n7        NaN        NaN           NaN   NaN  \n8        NaN        NaN           NaN   NaN  \n9        NaN        NaN           NaN   NaN  \n========================================\nDataFrame Name: PROD_A338_LN_Unmet_Needs_12072022_PROD_A338_LN_Unmet_Needs_EM2_St\n  MobileNumber   FirstName MiddleName             LastName Addr1 Addr2 City  \\\n0          NaN  MUTHUKUMAR        NaN           THANGAMANI   NaN   NaN  NaN   \n1          NaN       TANYA        NaN                 TANG   NaN   NaN  NaN   \n2          NaN      CARLOS        NaN    CLAUDIO RODRIGUEZ   NaN   NaN  NaN   \n3          NaN        ANIL        NaN               RAMESH   NaN   NaN  NaN   \n4          NaN      DEBBIE        NaN               KURIAN   NaN   NaN  NaN   \n5          NaN      NAUMAN        NaN                TAHIR   NaN   NaN  NaN   \n6          NaN       MARIO        NaN  ROBLES FRANCESCHINI   NaN   NaN  NaN   \n7          NaN      ELLIOT        NaN               CHAREN   NaN   NaN  NaN   \n8          NaN      THEJAS        NaN                SWAMY   NaN   NaN  NaN   \n9          NaN        REJI        NaN                 NAIR   NaN   NaN  NaN   \n\n  State  Zip OptOutURL Var1 Var2 Var3 Var4      CID  \\\n0   NaN  NaN       NaN  NaN   MD  NaN  NaN  3183200   \n1   NaN  NaN       NaN  NaN   DO  NaN  NaN  3185280   \n2   NaN  NaN       NaN  NaN   MD  NaN  NaN  3197899   \n3   NaN  NaN       NaN  NaN   MD  NaN  NaN  3213692   \n4   NaN  NaN       NaN  NaN   MD  NaN  NaN  3216656   \n5   NaN  NaN       NaN  NaN   MD  NaN  NaN  3248135   \n6   NaN  NaN       NaN  NaN   MD  NaN  NaN  3265559   \n7   NaN  NaN       NaN  NaN   MD  NaN  NaN  3267422   \n8   NaN  NaN       NaN  NaN   MD  NaN  NaN  3291593   \n9   NaN  NaN       NaN  NaN   MD  NaN  NaN  3292238   \n\n                                SubscriberKey          CreatedDate  \\\n0  ADHOC-BD56F9C6-E6C7-4BF2-8CF0-443F0D7D85DC  2022-11-30 08:10:00   \n1  ADHOC-3266D95D-953F-411B-BACC-F52C897F3F1A  2022-11-30 08:10:00   \n2  ADHOC-7534B849-A5ED-4366-8115-02D7FE395E21  2022-11-30 08:10:00   \n3  ADHOC-F6EB4600-33B6-4B0E-AB77-DCC81D93F4AF  2022-11-30 08:10:00   \n4  ADHOC-A10F150D-8203-4375-9CAA-54E3EBB92B8E  2022-11-30 08:10:00   \n5  ADHOC-F6583522-2261-4553-8B25-9D6C56E98F29  2022-11-30 08:10:00   \n6  ADHOC-BDD1EFE9-0652-419E-9628-FDE158F93445  2022-11-30 08:10:00   \n7  ADHOC-E721CA04-5192-459F-A948-375A05A1ED9D  2022-11-30 08:10:00   \n8  ADHOC-F1D8F26A-F65B-4053-ACB7-F148301DDFF2  2022-11-30 08:10:00   \n9  ADHOC-0FC07E67-891C-4AA3-BA4A-12FE7CD54D53  2022-11-30 08:10:00   \n\n      LastModifiedDate CampaignName       SuppressionType CampaignRunID  \\\n0  2022-11-30 07:13:00   A338-ADHOC                   NaN          A338   \n1  2022-11-30 07:13:00   A338-ADHOC          Gigya OptOut          A338   \n2  2022-11-30 07:13:00   A338-ADHOC                   NaN          A338   \n3  2022-11-30 07:13:00   A338-ADHOC                   NaN          A338   \n4  2022-11-30 07:17:00   A338-ADHOC  Blacklist/Quarantine          A338   \n5  2022-11-30 07:13:00   A338-ADHOC                   NaN          A338   \n6  2022-11-30 07:13:00   A338-ADHOC                   NaN          A338   \n7  2022-11-30 07:13:00   A338-ADHOC                   NaN          A338   \n8  2022-11-30 07:13:00   A338-ADHOC                   NaN          A338   \n9  2022-11-30 07:13:00   A338-ADHOC                   NaN          A338   \n\n  HighLowIP Sent             sentdate Open            open date click  \\\n0      High    Y  2022-12-07 11:04:00  NaN                  NaN   NaN   \n1      High  NaN                  NaN  NaN                  NaN   NaN   \n2      High    Y  2022-12-07 07:00:00  NaN                  NaN   NaN   \n3      High    Y  2022-12-07 08:01:00  NaN                  NaN   NaN   \n4      High  NaN                  NaN  NaN                  NaN   NaN   \n5      High    Y  2022-12-07 09:02:00  NaN                  NaN   NaN   \n6      High    Y  2022-12-07 08:01:00  NaN                  NaN   NaN   \n7      High    Y  2022-12-07 08:01:00    Y  2022-12-07 23:43:00   NaN   \n8      High    Y  2022-12-07 07:00:00  NaN                  NaN   NaN   \n9      High    Y  2022-12-07 08:01:00  NaN                  NaN   NaN   \n\n  clickdate Hardbounce softbounce blockedbounce unsub  \n0       NaN        NaN        NaN           NaN   NaN  \n1       NaN        NaN        NaN           NaN   NaN  \n2       NaN        NaN        NaN           NaN   NaN  \n3       NaN        NaN        NaN           NaN   NaN  \n4       NaN        NaN        NaN           NaN   NaN  \n5       NaN        NaN        NaN           NaN   NaN  \n6       NaN        NaN        NaN           NaN   NaN  \n7       NaN        NaN        NaN           NaN   NaN  \n8       NaN        NaN        NaN           NaN   NaN  \n9       NaN        NaN        NaN           NaN   NaN  \n========================================\nDataFrame Name: PROD_A357_Post_ASN_12092022_PROD_A357_Post_ASN_12092022\n  MobileNumber FirstName MiddleName    LastName Addr1 Addr2 City State  Zip  \\\n0          NaN     MARIA        NaN       EGIDI   NaN   NaN  NaN   NaN  NaN   \n1          NaN   GREGORY        NaN      MERTEN   NaN   NaN  NaN   NaN  NaN   \n2          NaN    LORIEN        NaN   DALRYMPLE   NaN   NaN  NaN   NaN  NaN   \n3          NaN  SVETLANA        NaN       JONES   NaN   NaN  NaN   NaN  NaN   \n4          NaN   MICHAEL        NaN   NURENBERG   NaN   NaN  NaN   NaN  NaN   \n5          NaN     ANITA        NaN      GOFRAN   NaN   NaN  NaN   NaN  NaN   \n6          NaN     SAMIR        NaN      PARIKH   NaN   NaN  NaN   NaN  NaN   \n7          NaN     NANCY        NaN    FINNIGAN   NaN   NaN  NaN   NaN  NaN   \n8          NaN     AHMED        NaN  ELSHARKAWI   NaN   NaN  NaN   NaN  NaN   \n9          NaN     NEENA        NaN      BHAGAT   NaN   NaN  NaN   NaN  NaN   \n\n  OptOutURL Var1 Var2 Var3 Var4      CID  \\\n0       NaN  NaN  NaN  NaN  NaN  2245649   \n1       NaN  NaN  NaN  NaN  NaN  2255446   \n2       NaN  NaN  NaN  NaN  NaN  2259356   \n3       NaN  NaN  NaN  NaN  NaN  2355107   \n4       NaN  NaN  NaN  NaN  NaN  2557293   \n5       NaN  NaN  NaN  NaN  NaN  2557890   \n6       NaN  NaN  NaN  NaN  NaN  2562389   \n7       NaN  NaN  NaN  NaN  NaN  2613222   \n8       NaN  NaN  NaN  NaN  NaN  2707545   \n9       NaN  NaN  NaN  NaN  NaN  2709910   \n\n                                SubscriberKey          CreatedDate  \\\n0  ADHOC-20FA6B19-8CCC-472F-8733-C8607F5A2776  2022-12-09 07:46:00   \n1  ADHOC-D357AAAD-09B2-4668-8193-A74E14FB0291  2022-12-09 07:46:00   \n2  ADHOC-1B20F4ED-FC88-406E-8E90-8A2E57E43B85  2022-12-09 07:46:00   \n3  ADHOC-FE96A9DC-D4D1-40A6-AC14-3A005B8EBFA9  2022-12-09 07:46:00   \n4  ADHOC-BC097BE8-0926-4868-8304-637260BDF65B  2022-12-09 07:46:00   \n5  ADHOC-9E3DD3BE-6DFA-438F-B165-9DF6C7E5BE9C  2022-12-09 07:46:00   \n6  ADHOC-57AB33A2-EB6F-4527-8A14-8600CAD66B61  2022-12-09 07:46:00   \n7  ADHOC-642B9DEB-9A99-4FC2-829C-33FBE7E1BBD4  2022-12-09 07:46:00   \n8  ADHOC-45360DD2-A27C-4D82-A6AF-87632CE7F9A3  2022-12-09 07:46:00   \n9  ADHOC-A02EBC56-C28C-4AFE-8F2B-DEF3F30F4384  2022-12-09 07:46:00   \n\n      LastModifiedDate CampaignName       SuppressionType CampaignRunID  \\\n0  2022-12-09 06:47:00   A357-ADHOC                   NaN          A357   \n1  2022-12-09 06:47:00   A357-ADHOC                   NaN          A357   \n2  2022-12-09 06:47:00   A357-ADHOC                   NaN          A357   \n3  2022-12-09 06:47:00   A357-ADHOC                   NaN          A357   \n4  2022-12-09 06:49:00   A357-ADHOC  Blacklist/Quarantine          A357   \n5  2022-12-09 06:47:00   A357-ADHOC                   NaN          A357   \n6  2022-12-09 06:47:00   A357-ADHOC                   NaN          A357   \n7  2022-12-09 06:47:00   A357-ADHOC                   NaN          A357   \n8  2022-12-09 06:49:00   A357-ADHOC  Blacklist/Quarantine          A357   \n9  2022-12-09 06:47:00   A357-ADHOC                   NaN          A357   \n\n  HighLowIP Sent             sentdate Open            open date click  \\\n0      High    Y  2022-12-09 00:00:00  NaN                  NaN   NaN   \n1      High    Y  2022-12-09 00:00:00  NaN                  NaN   NaN   \n2      High    Y  2022-12-09 00:00:00  NaN                  NaN   NaN   \n3      High    Y  2022-12-09 00:00:00  NaN                  NaN   NaN   \n4      High  NaN                  NaN  NaN                  NaN   NaN   \n5      High    Y  2022-12-09 00:00:00  NaN                  NaN   NaN   \n6      High    Y  2022-12-09 00:00:00  NaN                  NaN   NaN   \n7      High    Y  2022-12-09 00:00:00    Y  2022-12-09 00:00:00   NaN   \n8      High  NaN                  NaN  NaN                  NaN   NaN   \n9      High    Y  2022-12-09 00:00:00  NaN                  NaN   NaN   \n\n  clickdate Hardbounce softbounce blockedbounce unsub  \n0       NaN        NaN        NaN           NaN   NaN  \n1       NaN        NaN        NaN           NaN   NaN  \n2       NaN        NaN        NaN           NaN   NaN  \n3       NaN        NaN        NaN           NaN   NaN  \n4       NaN        NaN        NaN           NaN   NaN  \n5       NaN        NaN        NaN           NaN   NaN  \n6       NaN        NaN        NaN           NaN   NaN  \n7       NaN        NaN        NaN           NaN   NaN  \n8       NaN        NaN        NaN           NaN   NaN  \n9       NaN        NaN        NaN           NaN   NaN  \n========================================\nDataFrame Name: PROD_A375_LN_Unmet_Needs_Email3_12212022_PROD_A375_LN_Unmet_Needs_Email3\n  MobileNumber  FirstName MiddleName LastName Addr1 Addr2 City State  Zip  \\\n0          NaN      RIZMA        NaN    BAJWA   NaN   NaN  NaN   NaN  NaN   \n1          NaN   MUHAMMAD        NaN    AFZAL   NaN   NaN  NaN   NaN  NaN   \n2          NaN    REJEESH        NaN  VASUDEV   NaN   NaN  NaN   NaN  NaN   \n3          NaN    MUSTAFA        NaN    SAIDI   NaN   NaN  NaN   NaN  NaN   \n4          NaN       ABID        NaN    ASLAM   NaN   NaN  NaN   NaN  NaN   \n5          NaN  ELIZABETH        NaN    BRANT   NaN   NaN  NaN   NaN  NaN   \n6          NaN       EZRA        NaN   ISRAEL   NaN   NaN  NaN   NaN  NaN   \n7          NaN      NANCY        NaN     TRAN   NaN   NaN  NaN   NaN  NaN   \n8          NaN   REGINALD        NaN   JOSEPH   NaN   NaN  NaN   NaN  NaN   \n9          NaN      ABDUL        NaN     KHAN   NaN   NaN  NaN   NaN  NaN   \n\n  OptOutURL Var1 Var2 Var3 Var4      CID  \\\n0       NaN  NaN  NaN  NaN  NaN  3145561   \n1       NaN  NaN  NaN  NaN  NaN  3146716   \n2       NaN  NaN  NaN  NaN  NaN  3203306   \n3       NaN  NaN  NaN  NaN  NaN  3243930   \n4       NaN  NaN  NaN  NaN  NaN  3248007   \n5       NaN  NaN  NaN  NaN  NaN  3267702   \n6       NaN  NaN  NaN  NaN  NaN  3267967   \n7       NaN  NaN  NaN  NaN  NaN  3272324   \n8       NaN  NaN  NaN  NaN  NaN  3272781   \n9       NaN  NaN  NaN  NaN  NaN  3284882   \n\n                                SubscriberKey          CreatedDate  \\\n0  ADHOC-4F2A7239-4D6B-4BC5-AFB3-747519FDBD53  2022-12-15 04:29:07   \n1  ADHOC-96120F97-A9FF-4C69-8B5E-BC469795414C  2022-12-15 04:29:07   \n2  ADHOC-13E256C5-CAAA-4900-A2B5-EC00451DE58A  2022-12-15 04:29:07   \n3  ADHOC-9E79FAC1-6A87-4A70-BE51-2366996EABC7  2022-12-15 04:29:07   \n4  ADHOC-82433F06-E663-4054-87DE-E89E3384F2E1  2022-12-15 04:29:07   \n5  ADHOC-5D378E8D-2F65-456A-8FBC-6C4244D74F1C  2022-12-15 04:29:07   \n6  ADHOC-A52AB82D-C2DD-4F8F-859C-0CD1ADB4AC9F  2022-12-15 04:29:07   \n7  ADHOC-8BC949CF-D53F-46FE-B5E5-CE37B5D4DB49  2022-12-15 04:29:07   \n8  ADHOC-56E27500-5AE5-4D97-95A4-60D2C39716CF  2022-12-15 04:29:07   \n9  ADHOC-1645AE36-EF3F-43A3-B6A8-3F28C44F8283  2022-12-15 04:29:07   \n\n      LastModifiedDate CampaignName       SuppressionType CampaignRunID  \\\n0  2022-12-15 03:29:46   A375-ADHOC                   NaN          A375   \n1  2022-12-15 03:29:46   A375-ADHOC                   NaN          A375   \n2  2022-12-15 03:29:46   A375-ADHOC                   NaN          A375   \n3  2022-12-15 03:29:46   A375-ADHOC                   NaN          A375   \n4  2022-12-15 03:31:09   A375-ADHOC  Blacklist/Quarantine          A375   \n5  2022-12-15 03:29:46   A375-ADHOC                   NaN          A375   \n6  2022-12-15 03:29:46   A375-ADHOC                   NaN          A375   \n7  2022-12-15 03:29:46   A375-ADHOC                   NaN          A375   \n8  2022-12-15 03:29:46   A375-ADHOC                   NaN          A375   \n9  2022-12-15 03:29:46   A375-ADHOC                   NaN          A375   \n\n  HighLowIP Sent             sentdate Open open date click clickdate  \\\n0      High  NaN                  NaN  NaN       NaN   NaN       NaN   \n1      High  NaN                  NaN  NaN       NaN   NaN       NaN   \n2      High    Y  2022-12-21 09:26:00  NaN       NaN   NaN       NaN   \n3      High    Y  2022-12-21 09:26:00  NaN       NaN   NaN       NaN   \n4      High  NaN                  NaN  NaN       NaN   NaN       NaN   \n5      High    Y  2022-12-21 09:26:00  NaN       NaN   NaN       NaN   \n6      High    Y  2022-12-21 09:26:00  NaN       NaN   NaN       NaN   \n7      High    Y  2022-12-21 09:26:00  NaN       NaN   NaN       NaN   \n8      High    Y  2022-12-21 09:26:00  NaN       NaN   NaN       NaN   \n9      High    Y  2022-12-21 09:26:00  NaN       NaN   NaN       NaN   \n\n  Hardbounce softbounce blockedbounce unsub  \n0        NaN        NaN           NaN   NaN  \n1        NaN        NaN           NaN   NaN  \n2        NaN        NaN           NaN   NaN  \n3        NaN        NaN           NaN   NaN  \n4        NaN        NaN           NaN   NaN  \n5        NaN        NaN           NaN   NaN  \n6        NaN        NaN           NaN   NaN  \n7        NaN        NaN           NaN   NaN  \n8        NaN        NaN           NaN   NaN  \n9        NaN        NaN           NaN   NaN  \n========================================\nDataFrame Name: PROD_A375_LN_Unmet_Needs_Email_12152022_PROD_A375_LN_Unmet_Needs_Email3\n  MobileNumber  FirstName MiddleName LastName Addr1 Addr2 City State  Zip  \\\n0          NaN      RIZMA        NaN    BAJWA   NaN   NaN  NaN   NaN  NaN   \n1          NaN   MUHAMMAD        NaN    AFZAL   NaN   NaN  NaN   NaN  NaN   \n2          NaN    REJEESH        NaN  VASUDEV   NaN   NaN  NaN   NaN  NaN   \n3          NaN    MUSTAFA        NaN    SAIDI   NaN   NaN  NaN   NaN  NaN   \n4          NaN       ABID        NaN    ASLAM   NaN   NaN  NaN   NaN  NaN   \n5          NaN  ELIZABETH        NaN    BRANT   NaN   NaN  NaN   NaN  NaN   \n6          NaN       EZRA        NaN   ISRAEL   NaN   NaN  NaN   NaN  NaN   \n7          NaN      NANCY        NaN     TRAN   NaN   NaN  NaN   NaN  NaN   \n8          NaN   REGINALD        NaN   JOSEPH   NaN   NaN  NaN   NaN  NaN   \n9          NaN      ABDUL        NaN     KHAN   NaN   NaN  NaN   NaN  NaN   \n\n  OptOutURL Var1 Var2 Var3 Var4      CID  \\\n0       NaN  NaN  NaN  NaN  NaN  3145561   \n1       NaN  NaN  NaN  NaN  NaN  3146716   \n2       NaN  NaN  NaN  NaN  NaN  3203306   \n3       NaN  NaN  NaN  NaN  NaN  3243930   \n4       NaN  NaN  NaN  NaN  NaN  3248007   \n5       NaN  NaN  NaN  NaN  NaN  3267702   \n6       NaN  NaN  NaN  NaN  NaN  3267967   \n7       NaN  NaN  NaN  NaN  NaN  3272324   \n8       NaN  NaN  NaN  NaN  NaN  3272781   \n9       NaN  NaN  NaN  NaN  NaN  3284882   \n\n                                SubscriberKey          CreatedDate  \\\n0  ADHOC-4F2A7239-4D6B-4BC5-AFB3-747519FDBD53  2022-12-15 04:29:07   \n1  ADHOC-96120F97-A9FF-4C69-8B5E-BC469795414C  2022-12-15 04:29:07   \n2  ADHOC-13E256C5-CAAA-4900-A2B5-EC00451DE58A  2022-12-15 04:29:07   \n3  ADHOC-9E79FAC1-6A87-4A70-BE51-2366996EABC7  2022-12-15 04:29:07   \n4  ADHOC-82433F06-E663-4054-87DE-E89E3384F2E1  2022-12-15 04:29:07   \n5  ADHOC-5D378E8D-2F65-456A-8FBC-6C4244D74F1C  2022-12-15 04:29:07   \n6  ADHOC-A52AB82D-C2DD-4F8F-859C-0CD1ADB4AC9F  2022-12-15 04:29:07   \n7  ADHOC-8BC949CF-D53F-46FE-B5E5-CE37B5D4DB49  2022-12-15 04:29:07   \n8  ADHOC-56E27500-5AE5-4D97-95A4-60D2C39716CF  2022-12-15 04:29:07   \n9  ADHOC-1645AE36-EF3F-43A3-B6A8-3F28C44F8283  2022-12-15 04:29:07   \n\n      LastModifiedDate CampaignName       SuppressionType CampaignRunID  \\\n0  2022-12-15 03:29:46   A375-ADHOC                   NaN          A375   \n1  2022-12-15 03:29:46   A375-ADHOC                   NaN          A375   \n2  2022-12-15 03:29:46   A375-ADHOC                   NaN          A375   \n3  2022-12-15 03:29:46   A375-ADHOC                   NaN          A375   \n4  2022-12-15 03:31:09   A375-ADHOC  Blacklist/Quarantine          A375   \n5  2022-12-15 03:29:46   A375-ADHOC                   NaN          A375   \n6  2022-12-15 03:29:46   A375-ADHOC                   NaN          A375   \n7  2022-12-15 03:29:46   A375-ADHOC                   NaN          A375   \n8  2022-12-15 03:29:46   A375-ADHOC                   NaN          A375   \n9  2022-12-15 03:29:46   A375-ADHOC                   NaN          A375   \n\n  HighLowIP Sent             sentdate Open            open date click  \\\n0      High    Y  2022-12-15 07:05:00    Y  2022-12-15 15:59:00   NaN   \n1      High    Y  2022-12-15 07:05:00    Y  2022-12-15 17:07:00   NaN   \n2      High    Y  2022-12-15 07:05:00  NaN                  NaN   NaN   \n3      High    Y  2022-12-15 07:05:00  NaN                  NaN   NaN   \n4      High  NaN                  NaN  NaN                  NaN   NaN   \n5      High    Y  2022-12-15 07:05:00  NaN                  NaN   NaN   \n6      High    Y  2022-12-15 07:05:00  NaN                  NaN   NaN   \n7      High    Y  2022-12-15 07:05:00  NaN                  NaN   NaN   \n8      High    Y  2022-12-15 07:05:00  NaN                  NaN   NaN   \n9      High    Y  2022-12-15 07:05:00  NaN                  NaN   NaN   \n\n  clickdate Hardbounce softbounce blockedbounce unsub  \n0       NaN        NaN        NaN           NaN   NaN  \n1       NaN        NaN        NaN           NaN   NaN  \n2       NaN        NaN        NaN           NaN   NaN  \n3       NaN        NaN        NaN           NaN   NaN  \n4       NaN        NaN        NaN           NaN   NaN  \n5       NaN        NaN        NaN           NaN   NaN  \n6       NaN        NaN        NaN           NaN   NaN  \n7       NaN        NaN        NaN           NaN   NaN  \n8       NaN        NaN        NaN           NaN   NaN  \n9       NaN        NaN        NaN           NaN   NaN  \n========================================\nDataFrame Name: PROD_A416_LN_Unmet_Needs_Email4_01112023_PROD_A416_LN_Unmet_Needs_Email4\n  MobileNumber FirstName MiddleName     LastName Addr1 Addr2 City State  Zip  \\\n0          NaN      JOHN        NaN  BROUILLETTE   NaN   NaN  NaN   NaN  NaN   \n1          NaN   TANJELA        NaN      JACKSON   NaN   NaN  NaN   NaN  NaN   \n2          NaN      PAUL        NaN       DREYER   NaN   NaN  NaN   NaN  NaN   \n3          NaN   RANDALL        NaN        WHITE   NaN   NaN  NaN   NaN  NaN   \n4          NaN     SUMAN        NaN        REDDY   NaN   NaN  NaN   NaN  NaN   \n5          NaN    ROBERT        NaN       FARKAS   NaN   NaN  NaN   NaN  NaN   \n6          NaN      JOEL        NaN    HELDERMAN   NaN   NaN  NaN   NaN  NaN   \n7          NaN   ANTHONY        NaN       VALERI   NaN   NaN  NaN   NaN  NaN   \n8          NaN    JOSHUA        NaN       KAPLAN   NaN   NaN  NaN   NaN  NaN   \n9          NaN    STEVEN        NaN     FISHBANE   NaN   NaN  NaN   NaN  NaN   \n\n  OptOutURL Var1 Var2 Var3 Var4     CID  \\\n0       NaN  NaN  NaN  NaN  NaN  122713   \n1       NaN  NaN  NaN  NaN  NaN  135887   \n2       NaN  NaN  NaN  NaN  NaN  163791   \n3       NaN  NaN  NaN  NaN  NaN  225978   \n4       NaN  NaN  NaN  NaN  NaN  334351   \n5       NaN  NaN  NaN  NaN  NaN  353402   \n6       NaN  NaN  NaN  NaN  NaN  359803   \n7       NaN  NaN  NaN  NaN  NaN  361977   \n8       NaN  NaN  NaN  NaN  NaN  391250   \n9       NaN  NaN  NaN  NaN  NaN  399405   \n\n                                SubscriberKey          CreatedDate  \\\n0  ADHOC-1F728155-5EE7-42D2-A963-2A96D4C7128B  2023-01-18 07:34:00   \n1  ADHOC-E864B616-2D22-4F15-9C1B-09F981D2B95A  2023-01-18 07:34:00   \n2  ADHOC-B2FBDAA6-203C-4748-BA56-9A7245DCB8FB  2023-01-18 07:34:00   \n3  ADHOC-149B7FBB-F0E2-4136-A92D-3C420CDFA0BB  2023-01-18 07:34:00   \n4  ADHOC-4A6195A4-CAEE-4ED5-A332-9F581DA343FA  2023-01-18 07:34:00   \n5  ADHOC-14E08880-E841-47CB-8152-FC9EB8CFE730  2023-01-18 07:34:00   \n6  ADHOC-D1F88634-5442-4444-8622-5286C5CE5CCE  2023-01-18 07:34:00   \n7  ADHOC-BC6D4D5D-167F-4FD3-A006-A6EA6CFA2A1B  2023-01-18 07:34:00   \n8  ADHOC-2CC4B9E1-D453-4B7A-8BA8-D2AC3EADCED9  2023-01-18 07:34:00   \n9  ADHOC-81B7AEBF-B7D0-42E3-8FEA-121643A16229  2023-01-18 07:34:00   \n\n      LastModifiedDate CampaignName SuppressionType CampaignRunID HighLowIP  \\\n0  2023-01-18 06:36:00   A416-ADHOC             NaN          A416      High   \n1  2023-01-18 06:36:00   A416-ADHOC             NaN          A416      High   \n2  2023-01-18 06:36:00   A416-ADHOC             NaN          A416      High   \n3  2023-01-18 06:36:00   A416-ADHOC\n\n*** WARNING: max output size exceeded, skipping output. ***\n\nogy    Valid Record   \n5  2023-10-20 04:29:53  A695_MedAffairs_shinglesrheumatology    Valid Record   \n6  2023-10-20 04:29:53  A695_MedAffairs_shinglesrheumatology    Valid Record   \n7  2023-10-20 04:29:53  A695_MedAffairs_shinglesrheumatology    Valid Record   \n8  2023-10-20 04:29:53  A695_MedAffairs_shinglesrheumatology    Valid Record   \n9  2023-10-20 04:29:53  A695_MedAffairs_shinglesrheumatology    Valid Record   \n\n  CampaignRunID HighLowIP Sent             sentdate Open            open date  \\\n0          A695      High    Y  2023-10-20 07:05:00  NaN                  NaN   \n1          A695      High  NaN                  NaN  NaN                  NaN   \n2          A695      High  NaN                  NaN  NaN                  NaN   \n3          A695      High    Y  2023-10-20 07:05:00  NaN                  NaN   \n4          A695      High    Y  2023-10-20 07:05:00  NaN                  NaN   \n5          A695      High    Y  2023-10-20 07:05:00  NaN                  NaN   \n6          A695      High    Y  2023-10-20 07:05:00    Y  2023-10-20 07:05:00   \n7          A695      High    Y  2023-10-20 07:05:00  NaN                  NaN   \n8          A695      High    Y  2023-10-20 07:05:00  NaN                  NaN   \n9          A695      High    Y  2023-10-20 07:05:00  NaN                  NaN   \n\n  click clickdate Hardbounce softbounce blockedbounce unsub  \n0   NaN       NaN        NaN        NaN           NaN   NaN  \n1   NaN       NaN        NaN        NaN           NaN   NaN  \n2   NaN       NaN        NaN        NaN           NaN   NaN  \n3   NaN       NaN        NaN        NaN           NaN   NaN  \n4   NaN       NaN        NaN        NaN           NaN   NaN  \n5   NaN       NaN        NaN        NaN           NaN   NaN  \n6   NaN       NaN        NaN        NaN           NaN   NaN  \n7   NaN       NaN        NaN        NaN           NaN   NaN  \n8   NaN       NaN        NaN        NaN           NaN   NaN  \n9   NaN       NaN        NaN        NaN           NaN   NaN  \n========================================\nDataFrame Name: PROD_A738_MedicalAffairs_DMD_AIMXR_OrganDamage_10122023_PROD_A738_MedicalAffairs_DMD_AI\n  MobileNumber FirstName MiddleName LastName Addr1 Addr2 City State  Zip  \\\n0          NaN       NaN        NaN      NaN   NaN   NaN  NaN   NaN  NaN   \n1          NaN       NaN        NaN      NaN   NaN   NaN  NaN   NaN  NaN   \n2          NaN       NaN        NaN      NaN   NaN   NaN  NaN   NaN  NaN   \n3          NaN       NaN        NaN      NaN   NaN   NaN  NaN   NaN  NaN   \n4          NaN       NaN        NaN      NaN   NaN   NaN  NaN   NaN  NaN   \n5          NaN       NaN        NaN      NaN   NaN   NaN  NaN   NaN  NaN   \n6          NaN       NaN        NaN      NaN   NaN   NaN  NaN   NaN  NaN   \n7          NaN       NaN        NaN      NaN   NaN   NaN  NaN   NaN  NaN   \n8          NaN       NaN        NaN      NaN   NaN   NaN  NaN   NaN  NaN   \n9          NaN       NaN        NaN      NaN   NaN   NaN  NaN   NaN  NaN   \n\n  OptOutURL        Var1 Var2 Var3 Var4         CID  \\\n0       NaN  1528232394  NaN  NaN  NaN      987416   \n1       NaN  1578630869  NaN  NaN  NaN      622458   \n2       NaN  1073704425  NaN  NaN  NaN     3160479   \n3       NaN  1164765764  NaN  NaN  NaN  2018927813   \n4       NaN  1013903327  NaN  NaN  NaN     2609135   \n5       NaN  1083808695  NaN  NaN  NaN     3248175   \n6       NaN  1205039997  NaN  NaN  NaN      950162   \n7       NaN  1033467196  NaN  NaN  NaN  2018127733   \n8       NaN  1760424899  NaN  NaN  NaN      160181   \n9       NaN  1225263833  NaN  NaN  NaN     3111429   \n\n                                SubscriberKey          CreatedDate  \\\n0  ADHOC-77DCF1D3-FFEB-4280-8C3C-4B5E8A95C2A4  2023-10-12 06:44:01   \n1  ADHOC-A2DC7B44-6CF4-4512-995E-73F47314978D  2023-10-12 06:44:01   \n2  ADHOC-BA5748C4-08D6-41E2-A346-2EF261C2D12F  2023-10-12 06:44:01   \n3  ADHOC-17B27F2D-B835-4E4E-A739-11FEC0237B5D  2023-10-12 06:44:01   \n4  ADHOC-3058D953-707C-4F63-88BF-89E7395BF817  2023-10-12 06:44:01   \n5  ADHOC-416EC061-8EFF-4151-99EB-E351AF64BCC1  2023-10-12 06:44:01   \n6  ADHOC-E2120B1E-60A8-425B-8788-19C2C6E8C5DF  2023-10-12 06:44:01   \n7  ADHOC-2908593C-76FD-4A96-9129-8CCE98FFB329  2023-10-12 06:44:01   \n8  ADHOC-23307DC6-4F6E-4217-ADE3-6F3D2D4D2B6E  2023-10-12 06:44:01   \n9  ADHOC-2B7E5A04-87B1-4F79-9F7A-417C4B259AAD  2023-10-12 06:44:01   \n\n      LastModifiedDate                               CampaignName  \\\n0  2023-10-12 04:44:48  A738_MedicalAffairs_DMD_AIMXR_OrganDamage   \n1  2023-10-12 04:44:48  A738_MedicalAffairs_DMD_AIMXR_OrganDamage   \n2  2023-10-12 04:44:48  A738_MedicalAffairs_DMD_AIMXR_OrganDamage   \n3  2023-10-12 04:44:48  A738_MedicalAffairs_DMD_AIMXR_OrganDamage   \n4  2023-10-12 04:44:48  A738_MedicalAffairs_DMD_AIMXR_OrganDamage   \n5  2023-10-12 04:44:48  A738_MedicalAffairs_DMD_AIMXR_OrganDamage   \n6  2023-10-12 04:44:48  A738_MedicalAffairs_DMD_AIMXR_OrganDamage   \n7  2023-10-12 04:44:48  A738_MedicalAffairs_DMD_AIMXR_OrganDamage   \n8  2023-10-12 04:44:48  A738_MedicalAffairs_DMD_AIMXR_OrganDamage   \n9  2023-10-12 04:44:48  A738_MedicalAffairs_DMD_AIMXR_OrganDamage   \n\n  SuppressionType CampaignRunID HighLowIP Sent             sentdate Open  \\\n0    Valid Record          A738      HIGH    Y  2023-10-12 07:05:00  NaN   \n1    Valid Record          A738      HIGH    Y  2023-10-12 07:05:00  NaN   \n2    Valid Record          A738      HIGH    Y  2023-10-12 07:05:00    Y   \n3    Valid Record          A738      HIGH    Y  2023-10-12 07:05:00  NaN   \n4    Valid Record          A738      HIGH    Y  2023-10-12 07:05:00    Y   \n5    Valid Record          A738      HIGH  NaN                  NaN  NaN   \n6    Valid Record          A738      HIGH    Y  2023-10-12 07:05:00  NaN   \n7    Valid Record          A738      HIGH    Y  2023-10-12 07:05:00  NaN   \n8    Valid Record          A738      HIGH  NaN                  NaN  NaN   \n9    Valid Record          A738      HIGH    Y  2023-10-12 07:05:00    Y   \n\n             open date           Hardbounce  \n0                  NaN                  NaN  \n1                  NaN  2023-10-12 07:12:00  \n2  2023-10-12 20:17:00                  NaN  \n3                  NaN                  NaN  \n4  2023-10-12 15:40:00                  NaN  \n5                  NaN                  NaN  \n6                  NaN                  NaN  \n7                  NaN  2023-10-12 07:12:00  \n8                  NaN                  NaN  \n9  2023-10-15 22:59:00                  NaN  \n========================================\nDataFrame Name: PROD_A740_MedicalAffairs_DMD_AIMXR_DiseaseModification_10122023_PROD_A740_MedicalAffairs_DMD_AI\n  MobileNumber FirstName MiddleName LastName Addr1 Addr2 City State  Zip  \\\n0          NaN       NaN        NaN      NaN   NaN   NaN  NaN   NaN  NaN   \n1          NaN       NaN        NaN      NaN   NaN   NaN  NaN   NaN  NaN   \n2          NaN       NaN        NaN      NaN   NaN   NaN  NaN   NaN  NaN   \n3          NaN       NaN        NaN      NaN   NaN   NaN  NaN   NaN  NaN   \n4          NaN       NaN        NaN      NaN   NaN   NaN  NaN   NaN  NaN   \n5          NaN       NaN        NaN      NaN   NaN   NaN  NaN   NaN  NaN   \n6          NaN       NaN        NaN      NaN   NaN   NaN  NaN   NaN  NaN   \n7          NaN       NaN        NaN      NaN   NaN   NaN  NaN   NaN  NaN   \n8          NaN       NaN        NaN      NaN   NaN   NaN  NaN   NaN  NaN   \n9          NaN       NaN        NaN      NaN   NaN   NaN  NaN   NaN  NaN   \n\n  OptOutURL        Var1 Var2 Var3 Var4         CID  \\\n0       NaN  1619921541  NaN  NaN  NaN      463207   \n1       NaN  1255524393  NaN  NaN  NaN     2704106   \n2       NaN  1548291156  NaN  NaN  NaN      737816   \n3       NaN  1467415265  NaN  NaN  NaN      804086   \n4       NaN  1912267535  NaN  NaN  NaN     3371430   \n5       NaN  1609878362  NaN  NaN  NaN      385556   \n6       NaN  1447250220  NaN  NaN  NaN      124936   \n7       NaN  1861633083  NaN  NaN  NaN     3213034   \n8       NaN  1497751556  NaN  NaN  NaN  2030914368   \n9       NaN  1750382800  NaN  NaN  NaN     2550199   \n\n                                SubscriberKey          CreatedDate  \\\n0  ADHOC-1850F549-CE6D-4026-BC32-995DAD3A66A0  2023-10-12 06:44:00   \n1  ADHOC-C79228AB-3056-4D5A-B929-5A574FF89DC0  2023-10-12 06:44:00   \n2  ADHOC-92E84BB4-20E9-419C-A147-30AF037BF540  2023-10-12 06:44:00   \n3  ADHOC-F6B8E781-7B39-4296-B0FE-A3A23ED3DEC5  2023-10-12 06:44:00   \n4  ADHOC-F5FCFC17-2308-44C8-AD83-824DDF8C6A7D  2023-10-12 06:44:00   \n5  ADHOC-D658DDF2-69AC-4C80-A3B3-482E8499D79E  2023-10-12 06:44:00   \n6  ADHOC-1FFDDD2A-949A-482F-8D8F-B08CAB404E8B  2023-10-12 06:44:00   \n7  ADHOC-2045F05C-3551-42E3-942A-0F8E23118369  2023-10-12 06:44:00   \n8  ADHOC-7E0CDCC5-A351-4A41-858E-C1878DA72C1B  2023-10-12 06:44:00   \n9  ADHOC-65F82FF5-97BA-4B2C-BB22-2AB2501ED14D  2023-10-12 06:44:00   \n\n      LastModifiedDate                                       CampaignName  \\\n0  2023-10-12 04:44:00  A740_MedicalAffairs_DMD_AIMXR_DiseaseModification   \n1  2023-10-12 04:44:00  A740_MedicalAffairs_DMD_AIMXR_DiseaseModification   \n2  2023-10-12 04:44:00  A740_MedicalAffairs_DMD_AIMXR_DiseaseModification   \n3  2023-10-12 04:44:00  A740_MedicalAffairs_DMD_AIMXR_DiseaseModification   \n4  2023-10-12 04:44:00  A740_MedicalAffairs_DMD_AIMXR_DiseaseModification   \n5  2023-10-12 04:44:00  A740_MedicalAffairs_DMD_AIMXR_DiseaseModification   \n6  2023-10-12 04:44:00  A740_MedicalAffairs_DMD_AIMXR_DiseaseModification   \n7  2023-10-12 04:44:00  A740_MedicalAffairs_DMD_AIMXR_DiseaseModification   \n8  2023-10-12 04:44:00  A740_MedicalAffairs_DMD_AIMXR_DiseaseModification   \n9  2023-10-12 04:44:00  A740_MedicalAffairs_DMD_AIMXR_DiseaseModification   \n\n  SuppressionType CampaignRunID HighLowIP Sent             sentdate Open  \\\n0    Valid Record          A740      HIGH    Y  2023-10-12 00:00:00  NaN   \n1    Valid Record          A740      HIGH    Y  2023-10-12 00:00:00  NaN   \n2    Valid Record          A740      HIGH    Y  2023-10-12 00:00:00  NaN   \n3    Valid Record          A740      HIGH  NaN                  NaN  NaN   \n4    Valid Record          A740      HIGH    Y  2023-10-12 00:00:00  NaN   \n5    Valid Record          A740      HIGH    Y  2023-10-12 00:00:00  NaN   \n6    Valid Record          A740      HIGH    Y  2023-10-12 00:00:00  NaN   \n7    Valid Record          A740      HIGH    Y  2023-10-12 00:00:00  NaN   \n8    Valid Record          A740      HIGH    Y  2023-10-12 00:00:00  NaN   \n9    Valid Record          A740      HIGH    Y  2023-10-12 00:00:00    Y   \n\n             open date  \n0                  NaN  \n1                  NaN  \n2                  NaN  \n3                  NaN  \n4                  NaN  \n5                  NaN  \n6                  NaN  \n7                  NaN  \n8                  NaN  \n9  2023-10-12 00:00:00  \n========================================\nDataFrame Name: PROD_A741_MedicalAffairs_SLE_DMD_AIMXR_EarlyUse_10122023_PROD_A741_MedicalAffairs_SLE_DM\n  MobileNumber FirstName MiddleName LastName Addr1 Addr2 City State  Zip  \\\n0          NaN       NaN        NaN      NaN   NaN   NaN  NaN   NaN  NaN   \n1          NaN       NaN        NaN      NaN   NaN   NaN  NaN   NaN  NaN   \n2          NaN       NaN        NaN      NaN   NaN   NaN  NaN   NaN  NaN   \n3          NaN       NaN        NaN      NaN   NaN   NaN  NaN   NaN  NaN   \n4          NaN       NaN        NaN      NaN   NaN   NaN  NaN   NaN  NaN   \n5          NaN       NaN        NaN      NaN   NaN   NaN  NaN   NaN  NaN   \n6          NaN       NaN        NaN      NaN   NaN   NaN  NaN   NaN  NaN   \n7          NaN       NaN        NaN      NaN   NaN   NaN  NaN   NaN  NaN   \n8          NaN       NaN        NaN      NaN   NaN   NaN  NaN   NaN  NaN   \n9          NaN       NaN        NaN      NaN   NaN   NaN  NaN   NaN  NaN   \n\n  OptOutURL        Var1 Var2 Var3 Var4         CID  \\\n0       NaN  1972946143  NaN  NaN  NaN  2019477122   \n1       NaN  1093910432  NaN  NaN  NaN     2911377   \n2       NaN  1730353590  NaN  NaN  NaN      993048   \n3       NaN  1841387974  NaN  NaN  NaN      355954   \n4       NaN  1275767261  NaN  NaN  NaN     3327378   \n5       NaN  1609962026  NaN  NaN  NaN       61718   \n6       NaN  1508946385  NaN  NaN  NaN      731596   \n7       NaN  1881839694  NaN  NaN  NaN       90028   \n8       NaN  1639477136  NaN  NaN  NaN     3407958   \n9       NaN  1861665986  NaN  NaN  NaN     3278855   \n\n                                SubscriberKey          CreatedDate  \\\n0  ADHOC-C3330856-9BD9-46E3-A4E0-E2B74BB47FA8  2023-10-12 06:07:21   \n1  ADHOC-59044247-70D0-48F8-ACA7-72A54BB7B0A0  2023-10-12 06:07:21   \n2  ADHOC-D186FBDC-360D-4EA6-8F83-7B62262AD695  2023-10-12 06:07:21   \n3  ADHOC-CB6521AF-9F15-4BB1-8E50-87A01407915C  2023-10-12 06:07:21   \n4  ADHOC-DD7B51ED-5AE2-436D-BD89-D33C11B864E1  2023-10-12 06:07:21   \n5  ADHOC-18F13D51-5DBF-4140-A716-9737EEE12280  2023-10-12 06:07:21   \n6  ADHOC-4DC42C5C-5934-4540-BC2C-49D36C744B89  2023-10-12 06:07:21   \n7  ADHOC-7AF34AA8-1D1E-4317-881D-DAFEAD897D65  2023-10-12 06:07:21   \n8  ADHOC-93437540-D199-4DE7-B802-AA653A7F082C  2023-10-12 06:07:21   \n9  ADHOC-1B7B9790-6E39-4AD8-818C-8845B23816D4  2023-10-12 06:07:21   \n\n      LastModifiedDate                                CampaignName  \\\n0  2023-10-12 04:12:52  A741_MedicalAffairs_SLE_DMD_AIMXR_EarlyUse   \n1  2023-10-12 04:12:52  A741_MedicalAffairs_SLE_DMD_AIMXR_EarlyUse   \n2  2023-10-12 04:12:52  A741_MedicalAffairs_SLE_DMD_AIMXR_EarlyUse   \n3  2023-10-12 04:12:52  A741_MedicalAffairs_SLE_DMD_AIMXR_EarlyUse   \n4  2023-10-12 04:12:52  A741_MedicalAffairs_SLE_DMD_AIMXR_EarlyUse   \n5  2023-10-12 04:12:52  A741_MedicalAffairs_SLE_DMD_AIMXR_EarlyUse   \n6  2023-10-12 04:12:52  A741_MedicalAffairs_SLE_DMD_AIMXR_EarlyUse   \n7  2023-10-12 04:12:52  A741_MedicalAffairs_SLE_DMD_AIMXR_EarlyUse   \n8  2023-10-12 04:12:52  A741_MedicalAffairs_SLE_DMD_AIMXR_EarlyUse   \n9  2023-10-12 04:12:52  A741_MedicalAffairs_SLE_DMD_AIMXR_EarlyUse   \n\n  SuppressionType CampaignRunID HighLowIP Sent             sentdate Open  \\\n0    Valid Record          A741      HIGH    Y  2023-10-12 07:03:00  NaN   \n1    Valid Record          A741      HIGH    Y  2023-10-12 07:03:00  NaN   \n2    Bounced/Held          A741      HIGH  NaN                  NaN  NaN   \n3    Valid Record          A741      HIGH    Y  2023-10-12 07:03:00  NaN   \n4    Valid Record          A741      HIGH    Y  2023-10-12 07:03:00  NaN   \n5    Valid Record          A741      HIGH    Y  2023-10-12 07:03:00    Y   \n6    Valid Record          A741      HIGH    Y  2023-10-12 07:03:00    Y   \n7    Valid Record          A741      HIGH    Y  2023-10-12 07:03:00  NaN   \n8    Valid Record          A741      HIGH    Y  2023-10-12 07:03:00  NaN   \n9    Valid Record          A741      HIGH    Y  2023-10-12 07:03:00    Y   \n\n             open date click clickdate  \n0                  NaN   NaN       NaN  \n1                  NaN   NaN       NaN  \n2                  NaN   NaN       NaN  \n3                  NaN   NaN       NaN  \n4                  NaN   NaN       NaN  \n5  2023-10-12 08:46:00   NaN       NaN  \n6  2023-10-12 07:07:00   NaN       NaN  \n7                  NaN   NaN       NaN  \n8                  NaN   NaN       NaN  \n9  2023-10-12 07:03:00   NaN       NaN  \n========================================\nDataFrame Name: PROD_A742_MedicalAffairs_SLE_ACR_FinalDE20231024_PROD_A742_MedicalAffairs_SLE_AC\n                       EmailAddress MobileNumber FirstName MiddleName  \\\n0  jeffrey.peller@hcahealthcare.com          NaN       NaN        NaN   \n1   jeffrey_rosenberg@trihealth.com          NaN       NaN        NaN   \n2       jeffreyadamfriedland@me.com          NaN       NaN        NaN   \n3                 jeffryu@yahoo.com          NaN       NaN        NaN   \n4              jeffyeargain@att.net          NaN       NaN        NaN   \n5                 jeffzmd@yahoo.com          NaN       NaN        NaN   \n6    jefrey_lieberman@obstetrix.com          NaN       NaN        NaN   \n7              jefuente@hotmail.com          NaN       NaN        NaN   \n8               jehlich@hotmail.com          NaN       NaN        NaN   \n9            jejune2000@hotmail.com          NaN       NaN        NaN   \n\n  LastName Addr1 Addr2 City State  Zip OptOutURL Var1 Var2 Var3 Var4      CID  \\\n0      NaN   NaN   NaN  NaN   NaN  NaN       NaN  NaN  NaN  NaN  NaN   312620   \n1      NaN   NaN   NaN  NaN   NaN  NaN       NaN  NaN  NaN  NaN  NaN   838510   \n2      NaN   NaN   NaN  NaN   NaN  NaN       NaN  NaN  NaN  NaN  NaN   122419   \n3      NaN   NaN   NaN  NaN   NaN  NaN       NaN  NaN  NaN  NaN  NaN   602680   \n4      NaN   NaN   NaN  NaN   NaN  NaN       NaN  NaN  NaN  NaN  NaN   401708   \n5      NaN   NaN   NaN  NaN   NaN  NaN       NaN  NaN  NaN  NaN  NaN   540053   \n6      NaN   NaN   NaN  NaN   NaN  NaN       NaN  NaN  NaN  NaN  NaN   432082   \n7      NaN   NaN   NaN  NaN   NaN  NaN       NaN  NaN  NaN  NaN  NaN   744095   \n8      NaN   NaN   NaN  NaN   NaN  NaN       NaN  NaN  NaN  NaN  NaN   313003   \n9      NaN   NaN   NaN  NaN   NaN  NaN       NaN  NaN  NaN  NaN  NaN  3192164   \n\n                                SubscriberKey          CreatedDate  \\\n0  ADHOC-72263C5A-5A59-4EB4-987D-873391B2BD51  Oct 19 2023  5:08AM   \n1  ADHOC-473B10DB-F479-455B-8DB6-647426AC3289  Oct 19 2023  5:08AM   \n2  ADHOC-34DFBE2D-F4B3-421D-B7F2-4EABBA29E14C  Oct 19 2023  5:08AM   \n3  ADHOC-35893C8C-AD96-42C5-9823-6A74A808C484  Oct 19 2023  5:08AM   \n4  ADHOC-ACFE457F-43C5-4CCB-BFC7-314C450A3D73  Oct 19 2023  5:08AM   \n5  ADHOC-96BC4120-4795-41B3-BA6A-F026E45B6879  Oct 19 2023  5:08AM   \n6  ADHOC-95424215-A3EE-462D-8550-3A0D6AC67A67  Oct 19 2023  5:08AM   \n7  ADHOC-2013F794-66DF-4D95-8BC5-9DB6537416C9  Oct 19 2023  5:08AM   \n8  ADHOC-D4AD84D4-8019-4917-BE52-5F839C5C587B  Oct 19 2023  5:08AM   \n9  ADHOC-7354DFD1-817F-4BB0-9565-EE8FC801BF12  Oct 19 2023  5:08AM   \n\n      LastModifiedDate                            CampaignName  \\\n0  Oct 19 2023  3:26AM  PROD_A742_MedicalAffairs_SLE_ACR_email   \n1  Oct 19 2023  3:26AM  PROD_A742_MedicalAffairs_SLE_ACR_email   \n2  Oct 19 2023  3:26AM  PROD_A742_MedicalAffairs_SLE_ACR_email   \n3  Oct 19 2023  3:26AM  PROD_A742_MedicalAffairs_SLE_ACR_email   \n4  Oct 19 2023  3:26AM  PROD_A742_MedicalAffairs_SLE_ACR_email   \n5  Oct 19 2023  3:26AM  PROD_A742_MedicalAffairs_SLE_ACR_email   \n6  Oct 19 2023  3:26AM  PROD_A742_MedicalAffairs_SLE_ACR_email   \n7  Oct 19 2023  3:26AM  PROD_A742_MedicalAffairs_SLE_ACR_email   \n8  Oct 19 2023  3:26AM  PROD_A742_MedicalAffairs_SLE_ACR_email   \n9  Oct 19 2023  3:26AM  PROD_A742_MedicalAffairs_SLE_ACR_email   \n\n  InitialSendDate ReSendDate CampaignRunID HighLowIP IsSEED Sent  \\\n0             NaN        NaN          A742      High    NaN    Y   \n1             NaN        NaN          A742      High    NaN    Y   \n2             NaN        NaN          A742      High    NaN    Y   \n3             NaN        NaN          A742      High    NaN    Y   \n4             NaN        NaN          A742      High    NaN    Y   \n5             NaN        NaN          A742      High    NaN    Y   \n6             NaN        NaN          A742      High    NaN  NaN   \n7             NaN        NaN          A742      High    NaN    Y   \n8             NaN        NaN          A742      High    NaN    Y   \n9             NaN        NaN          A742      High    NaN    Y   \n\n              sentdate Open            open date click clickdate Hardbounce  \\\n0  2023-10-19 07:05:00  NaN                  NaN   NaN       NaN        NaN   \n1  2023-10-19 07:05:00  NaN                  NaN   NaN       NaN        NaN   \n2  2023-10-19 07:05:00  NaN                  NaN   NaN       NaN        NaN   \n3  2023-10-19 07:05:00    Y  2023-10-19 15:53:00   NaN       NaN        NaN   \n4  2023-10-19 07:05:00  NaN                  NaN   NaN       NaN        NaN   \n5  2023-10-19 07:05:00    Y  2023-10-19 19:46:00   NaN       NaN        NaN   \n6                  NaN  NaN                  NaN   NaN       NaN        NaN   \n7  2023-10-19 07:05:00    Y  2023-10-20 12:11:00   NaN       NaN        NaN   \n8  2023-10-19 07:05:00  NaN                  NaN   NaN       NaN        NaN   \n9  2023-10-19 07:05:00  NaN                  NaN   NaN       NaN        NaN   \n\n  softbounce blockedbounce unsub  \n0        NaN           NaN   NaN  \n1        NaN           NaN   NaN  \n2        NaN           NaN   NaN  \n3        NaN           NaN   NaN  \n4        NaN           NaN   NaN  \n5        NaN           NaN   NaN  \n6        NaN           NaN   NaN  \n7        NaN           NaN   NaN  \n8        NaN           NaN   NaN  \n9        NaN           NaN   NaN  \n========================================\nDataFrame Name: Prod_A743_PRE_ASN_Email_10182023_Prod_A743_PRE_ASN_Email_StageDE\n  MobileNumber  FirstName MiddleName      LastName Addr1 Addr2 City State  \\\n0          NaN      IMRAN        NaN         SHUJA   NaN   NaN  NaN   NaN   \n1          NaN      IMRAN        NaN        JAWAID   NaN   NaN  NaN   NaN   \n2          NaN      VINNY        NaN         ANAND   NaN   NaN  NaN   NaN   \n3          NaN  INDRANEEL        NaN      MOGARALA   NaN   NaN  NaN   NaN   \n4          NaN    ORLANDO        NaN  CAMACHO GECK   NaN   NaN  NaN   NaN   \n5          NaN       DEON        NaN   MIDDLEBROOK   NaN   NaN  NaN   NaN   \n6          NaN    RICHARD        NaN        WALKER   NaN   NaN  NaN   NaN   \n7          NaN    MICHAEL        NaN         HEALY   NaN   NaN  NaN   NaN   \n8          NaN     ANTIEM        NaN           BUI   NaN   NaN  NaN   NaN   \n9          NaN   AMBEREEN        NaN           JAN   NaN   NaN  NaN   NaN   \n\n   Zip OptOutURL Var1 Var2 Var3 Var4         CID  \\\n0  NaN       NaN  NaN  NaN  NaN  NaN  2021140067   \n1  NaN       NaN  NaN  NaN  NaN  NaN  2023053865   \n2  NaN       NaN  NaN  NaN  NaN  NaN      666710   \n3  NaN       NaN  NaN  NaN  NaN  NaN     3054905   \n4  NaN       NaN  NaN  NaN  NaN  NaN  2033634412   \n5  NaN       NaN  NaN  NaN  NaN  NaN      279165   \n6  NaN       NaN  NaN  NaN  NaN  NaN      429813   \n7  NaN       NaN  NaN  NaN  NaN  NaN      311602   \n8  NaN       NaN  NaN  NaN  NaN  NaN  2019133007   \n9  NaN       NaN  NaN  NaN  NaN  NaN      740637   \n\n                                SubscriberKey          CreatedDate  \\\n0  ADHOC-12134071-A2F4-4496-817A-D489878EA01C  2023-10-18 04:28:03   \n1  ADHOC-D2D61A01-6735-41CD-AEE5-B6A69083E5B5  2023-10-18 04:28:03   \n2  ADHOC-6D0AB25F-F2D2-4CFC-B68F-D4EDD9D4BF52  2023-10-18 04:28:03   \n3  ADHOC-F8A4E636-8619-4FB2-9F16-3A14A3CF0DE1  2023-10-18 04:28:03   \n4  ADHOC-1EAD0F62-CB36-46EA-8886-7F25F4FF924E  2023-10-18 04:28:03   \n5  ADHOC-495E53B4-AFC2-4A20-8635-13086E7B98DC  2023-10-18 04:28:03   \n6  ADHOC-0BDE8158-252E-437C-9E3F-DF6BD7412D23  2023-10-18 04:28:03   \n7  ADHOC-32F6E34F-0C8B-4759-92C2-9323163D4ED1  2023-10-18 04:28:03   \n8  ADHOC-16D6B1CD-0420-4091-9037-E68219CF8126  2023-10-18 04:28:03   \n9  ADHOC-BC4E3A54-19BB-4CF1-9EDB-73013DDD4BAE  2023-10-18 04:28:03   \n\n      LastModifiedDate        CampaignName SuppressionType CampaignRunID  \\\n0  2023-10-18 02:29:08  A743_PRE_ASN_Email    Valid Record          A743   \n1  2023-10-18 02:29:08  A743_PRE_ASN_Email    Valid Record          A743   \n2  2023-10-18 02:29:08  A743_PRE_ASN_Email    Valid Record          A743   \n3  2023-10-18 02:29:08  A743_PRE_ASN_Email    Valid Record          A743   \n4  2023-10-18 02:29:08  A743_PRE_ASN_Email    Valid Record          A743   \n5  2023-10-18 02:29:08  A743_PRE_ASN_Email    Valid Record          A743   \n6  2023-10-18 02:29:08  A743_PRE_ASN_Email    Valid Record          A743   \n7  2023-10-18 02:29:08  A743_PRE_ASN_Email    Valid Record          A743   \n8  2023-10-18 02:29:08  A743_PRE_ASN_Email    Bounced/Held          A743   \n9  2023-10-18 02:29:08  A743_PRE_ASN_Email    Valid Record          A743   \n\n  HighLowIP Sent             sentdate Open open date click clickdate  \\\n0      High    Y  2023-10-18 07:05:00  NaN       NaN   NaN       NaN   \n1      High    Y  2023-10-18 07:05:00  NaN       NaN   NaN       NaN   \n2      High    Y  2023-10-18 07:05:00  NaN       NaN   NaN       NaN   \n3      High    Y  2023-10-18 07:05:00  NaN       NaN   NaN       NaN   \n4      High  NaN                  NaN  NaN       NaN   NaN       NaN   \n5      High    Y  2023-10-18 07:05:00  NaN       NaN   NaN       NaN   \n6      High  NaN                  NaN  NaN       NaN   NaN       NaN   \n7      High    Y  2023-10-18 07:05:00  NaN       NaN   NaN       NaN   \n8      High  NaN                  NaN  NaN       NaN   NaN       NaN   \n9      High    Y  2023-10-18 07:05:00  NaN       NaN   NaN       NaN   \n\n  Hardbounce softbounce blockedbounce unsub  \n0        NaN        NaN           NaN   NaN  \n1        NaN        NaN           NaN   NaN  \n2        NaN        NaN           NaN   NaN  \n3        NaN        NaN           NaN   NaN  \n4        NaN        NaN           NaN   NaN  \n5        NaN        NaN           NaN   NaN  \n6        NaN        NaN           NaN   NaN  \n7        NaN        NaN           NaN   NaN  \n8        NaN        NaN           NaN   NaN  \n9        NaN        NaN           NaN   NaN  \n========================================\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the dictionary of DataFrames\n",
    "for key, df in bronze_dataframes.items():\n",
    "    print(f\"DataFrame Name: {key}\")\n",
    "    print(df.head(10))  # This will print the DataFrame\n",
    "    print(\"=\" * 40)  # Separator line for better readability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da82ad45-938b-40e6-bde2-b1eb769970b3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### ETL Silver Layer\n",
    "\n",
    "Transforming and selecting the datacolumns that is requried to make into the table\n",
    "\n",
    "Looking for columns from each file\n",
    "\n",
    "* EmailAddress\n",
    "* MobileNumber\n",
    "* FirstName\n",
    "* MiddleName\n",
    "* LastName\n",
    "* Addr1\n",
    "* Addr2\n",
    "* City\n",
    "* State\n",
    "* Zip\n",
    "* OptOutURL\n",
    "* Var1\n",
    "* Var2\n",
    "* Var3\n",
    "* Var4\n",
    "* CID\n",
    "* SubscriberKey\n",
    "* CreatedDate\n",
    "* LastModifiedDate\n",
    "* CampaignName\n",
    "* SuppressionType\n",
    "* CampaignRunID\n",
    "* HighLowIP\n",
    "* Sent\n",
    "* sentdate\n",
    "* Open\n",
    "* opendate\n",
    "* click\n",
    "* clickdate\n",
    "* Hardbounce\n",
    "* softbounce\n",
    "* blockedbounce\n",
    "* unsub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0044d6b3-8cf2-40c1-957c-ef76d0e6d7ea",
     "showTitle": true,
     "title": "One time SQL to create empty table by the name sle_mun_data_silver_layer"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": null
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "\n",
    "//*\n",
    "drop table if exists sle_mun_data_silver_layer;\n",
    "\n",
    "-- Creates a Delta table\n",
    "CREATE TABLE IF NOT EXISTS sle_mun_data_silver_layer ( rowid BIGINT NOT NULL,\n",
    "emailaddress STRING,\n",
    "mobilenumber STRING,\n",
    "firstname STRING,\n",
    "middlename STRING,\n",
    "lastname STRING,\n",
    "addr1 STRING,\n",
    "addr2 STRING,\n",
    "city STRING,\n",
    "state STRING,\n",
    "zip STRING,\n",
    "optouturl STRING,\n",
    "var1 STRING,\n",
    "var2 STRING,\n",
    "var3 STRING,\n",
    "var4 STRING,\n",
    "cid STRING,\n",
    "subscriberkey STRING,\n",
    "createddate STRING,\n",
    "lastmodifieddate STRING,\n",
    "campaignname STRING,\n",
    "suppressiontype STRING,\n",
    "campaignrunid STRING,\n",
    "highlowip STRING,\n",
    "sent STRING,\n",
    "sentdate STRING,\n",
    "open STRING,\n",
    "opendate STRING,\n",
    "click STRING,\n",
    "clickdate STRING,\n",
    "hardbounce STRING,\n",
    "softbounce STRING,\n",
    "blockedbounce STRING,\n",
    "unsub STRING,\n",
    "file_name STRING NOT NULL)\n",
    "USING delta\n",
    "PARTITIONED BY (file_name)\n",
    "//*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bd6eba9-251b-47e0-a205-991caf4409ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Define search column names\n",
    "search_column_names = [\"EmailAddress\", \"MobileNumber\", \"FirstName\", \"MiddleName\", \"LastName\", \"Addr1\", \"Addr2\", \"City\", \"State\", \"Zip\", \"OptOutURL\", \"Var1\", \"Var2\", \"Var3\", \"Var4\", \"CID\", \"SubscriberKey\", \"CreatedDate\", \"LastModifiedDate\", \"CampaignName\", \"SuppressionType\",\n",
    "                        \"CampaignRunID\", \"HighLowIP\", \"Sent\", \"sentdate\", \"Open\", \"open date\", \"click\", \"clickdate\", \"Hardbounce\", \"softbounce\", \"blockedbounce\", \"unsub\"]\n",
    "\n",
    "search_column_name_emailaddress = ['EmailAddress', 'emailaddress', 'EMAILADDRESS']\n",
    "search_column_name_mobilenumber = ['MobileNumber', 'mobilenumber', 'MOBILENUMBER']\n",
    "search_column_name_firstname = ['FirstName', 'firstname', 'FIRSTNAME']\n",
    "search_column_name_middlename = ['MiddleName', 'middlename', 'MIDDLENAME']\n",
    "search_column_name_lastname = ['LastName', 'lastname', 'LASTNAME']\n",
    "search_column_name_addr1 = ['Addr1', 'addr1', 'ADDR1']\n",
    "search_column_name_addr2 = ['Addr2', 'addr2', 'ADDR2']\n",
    "search_column_name_city = ['City', 'city', 'CITY']\n",
    "search_column_name_state = ['State', 'state', 'STATE']\n",
    "search_column_name_zip = ['Zip', 'zip', 'ZIP']\n",
    "search_column_name_optouturl = ['OptOutURL', 'optouturl', 'OPTOUTURL']\n",
    "search_column_name_var1 = ['Var1', 'var1', 'VAR1']\n",
    "search_column_name_var2 = ['Var2', 'var2', 'VAR2']\n",
    "search_column_name_var3 = ['Var3', 'var3', 'VAR3']\n",
    "search_column_name_var4 = ['Var4', 'var4', 'VAR4']\n",
    "search_column_name_cid = ['CID', 'cid']\n",
    "search_column_name_subscriberkey = ['SubscriberKey', 'subscriberkey', 'SUBSCRIBERKEY']\n",
    "search_column_name_createddate = ['CreatedDate', 'createddate', 'CREATEDDATE']\n",
    "search_column_name_lastmodifieddate = ['LastModifiedDate', 'lastmodifieddate', 'LASTMODIFIEDDATE']\n",
    "search_column_name_campaignname = ['CampaignName', 'campaignname', 'CAMPAIGNNAME']\n",
    "search_column_name_suppressiontype = ['SuppressionType', 'suppressiontype', 'SUPPRESSIONTYPE']\n",
    "search_column_name_campaignrunid = ['CampaignRunID', 'campaignrunid', 'CAMPAIGNRUNID']\n",
    "search_column_name_highlowip = ['HighLowIP', 'highlowip', 'HIGHLOWIP']\n",
    "search_column_name_sent = ['Sent', 'sent', 'SENT']\n",
    "search_column_name_sentdate = ['sentdate', 'SENTDATE']\n",
    "search_column_name_open = ['Open', 'open', 'OPEN']\n",
    "search_column_name_opendate = ['open date', 'OPEN DATE', 'opendate', 'OPENDATE']\n",
    "search_column_name_click = ['click', 'CLICK']\n",
    "search_column_name_clickdate = ['clickdate', 'CLICKDATE']\n",
    "search_column_name_hardbounce = ['Hardbounce', 'HARDBOUNCE']\n",
    "search_column_name_softbounce = ['softbounce', 'SOFTBOUNCE']\n",
    "search_column_name_blockedbounce = ['blockedbounce', 'BLOCKEDBOUNCE']\n",
    "search_column_name_unsub = ['unsub', 'UNSUB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa286ed3-6628-4832-a9e5-3a60f1d77801",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 2: Define function to scrape from the excel the best matching column name with the maximum number of rows\n",
    "from rapidfuzz import fuzz, utils\n",
    "\n",
    "'''\n",
    "regex_pattern = r'[^a-zA-Z]'\n",
    "\n",
    "def clean_column(column_name_input):\n",
    "   # Use re.sub to replace the matched keywords with an empty string\n",
    "  column_name_output = re.sub(regex_pattern, '', column_name_input)\n",
    "  column_name_output = column_name_output.lower()\n",
    "  column_name_output = column_name_output.strip()\n",
    "  return column_name_output\n",
    "'''\n",
    "\n",
    "def fuzzy_column_match(df, column_name):\n",
    "    print(df)  \n",
    "    #start\n",
    "    matches = [(col, fuzz.ratio(column_name, col)) for col in df.columns]\n",
    "    best_match = max(matches, key=lambda x: x[1]) # get the best value from the maximum value of ratio \n",
    "\n",
    "    # Count non-null, non-NaN, and non-empty rows in the specified column\n",
    "    x_count = len(df[df[best_match[0]].notnull() & (df[best_match[0]] != \"\") & (df[best_match[0]] != \" \") ])\n",
    "\n",
    "    data_to_add = {\"Column_Name_Search_Query\": column_name, \"Dataframe_Column_Name_Found\": best_match[0], \"Dataframe_Column_Fuzzy_Match_Score\": best_match[1], \"Dataframe_Column_Row_Count\": x_count}\n",
    "    #print(data_to_add)\n",
    "    return data_to_add # sending the dictionary back\n",
    "    #end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ce2ce8a-5048-4136-8f2b-712fb220311d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROD_A338_LN_Unmet_Needs_11302022_PROD_A338_LN_Unmet_Needs_EM2_St\n     MobileNumber   FirstName MiddleName           LastName Addr1 Addr2 City  \\\n0             NaN  MUTHUKUMAR        NaN         THANGAMANI   NaN   NaN  NaN   \n1             NaN       TANYA        NaN               TANG   NaN   NaN  NaN   \n2             NaN      CARLOS        NaN  CLAUDIO RODRIGUEZ   NaN   NaN  NaN   \n3             NaN        ANIL        NaN             RAMESH   NaN   NaN  NaN   \n4             NaN      DEBBIE        NaN             KURIAN   NaN   NaN  NaN   \n...           ...         ...        ...                ...   ...   ...  ...   \n6591          NaN        ERIC        NaN              CHANG   NaN   NaN  NaN   \n6592          NaN       RAJAT        NaN              LAMBA   NaN   NaN  NaN   \n6593          NaN      YASSER        NaN            ALDURRA   NaN   NaN  NaN   \n6594          NaN       SURJU        NaN              PATEL   NaN   NaN  NaN   \n6595          NaN    BUTHAYNA        NaN             DINARY   NaN   NaN  NaN   \n\n     State  Zip OptOutURL Var1 Var2 Var3 Var4         CID  \\\n0      NaN  NaN       NaN  NaN   MD  NaN  NaN     3183200   \n1      NaN  NaN       NaN  NaN   DO  NaN  NaN     3185280   \n2      NaN  NaN       NaN  NaN   MD  NaN  NaN     3197899   \n3      NaN  NaN       NaN  NaN   MD  NaN  NaN     3213692   \n4      NaN  NaN       NaN  NaN   MD  NaN  NaN     3216656   \n...    ...  ...       ...  ...  ...  ...  ...         ...   \n6591   NaN  NaN       NaN  NaN   MD  NaN  NaN  1002417132   \n6592   NaN  NaN       NaN  NaN   MD  NaN  NaN  2003651016   \n6593   NaN  NaN       NaN  NaN   MD  NaN  NaN  2003881037   \n6594   NaN  NaN       NaN  NaN   MD  NaN  NaN  2003927133   \n6595   NaN  NaN       NaN  NaN   MD  NaN  NaN  2007648581   \n\n                                   SubscriberKey          CreatedDate  \\\n0     ADHOC-BD56F9C6-E6C7-4BF2-8CF0-443F0D7D85DC  2022-11-30 08:10:00   \n1     ADHOC-3266D95D-953F-411B-BACC-F52C897F3F1A  2022-11-30 08:10:00   \n2     ADHOC-7534B849-A5ED-4366-8115-02D7FE395E21  2022-11-30 08:10:00   \n3     ADHOC-F6EB4600-33B6-4B0E-AB77-DCC81D93F4AF  2022-11-30 08:10:00   \n4     ADHOC-A10F150D-8203-4375-9CAA-54E3EBB92B8E  2022-11-30 08:10:00   \n...                                          ...                  ...   \n6591  ADHOC-92622E2E-49EF-45EC-9AD8-CCE9087C0047  2022-11-30 08:10:00   \n6592  ADHOC-1C37E2CF-33DC-46B0-BF3F-B48BB7438F73  2022-11-30 08:10:00   \n6593  ADHOC-636C0019-2AB9-4155-8B62-572EB3C9042B  2022-11-30 08:10:00   \n6594  ADHOC-A9A8A65C-14E7-4106-9560-7198674D6CDD  2022-11-30 08:10:00   \n6595  ADHOC-75FEDD73-A956-4655-B9E5-EC91657C1772  2022-11-30 08:10:00   \n\n         LastModifiedDate CampaignName       SuppressionType CampaignRunID  \\\n0     2022-11-30 07:13:00   A338-ADHOC                   NaN          A338   \n1     2022-11-30 07:13:00   A338-ADHOC          Gigya OptOut          A338   \n2     2022-11-30 07:13:00   A338-ADHOC                   NaN          A338   \n3     2022-11-30 07:13:00   A338-ADHOC                   NaN          A338   \n4     2022-11-30 07:17:00   A338-ADHOC  Blacklist/Quarantine          A338   \n...                   ...          ...                   ...           ...   \n6591  2022-11-30 07:13:00   A338-ADHOC                   NaN          A338   \n6592  2022-11-30 07:13:00   A338-ADHOC                   NaN          A338   \n6593  2022-11-30 07:13:00   A338-ADHOC                   NaN          A338   \n6594  2022-11-30 07:13:00   A338-ADHOC                   NaN          A338   \n6595  2022-11-30 07:13:00   A338-ADHOC                   NaN          A338   \n\n     HighLowIP Sent             sentdate Open            open date click  \\\n0         High    Y  2022-11-30 07:30:00  NaN                  NaN   NaN   \n1         High  NaN                  NaN  NaN                  NaN   NaN   \n2         High    Y  2022-11-30 07:30:00  NaN                  NaN   NaN   \n3         High    Y  2022-11-30 07:30:00  NaN                  NaN   NaN   \n4         High  NaN                  NaN  NaN                  NaN   NaN   \n...        ...  ...                  ...  ...                  ...   ...   \n6591      High    Y  2022-11-30 08:31:00    Y  2022-11-30 11:58:00   NaN   \n6592      High    Y  2022-11-30 08:31:00    Y  2022-11-30 17:45:00   NaN   \n6593      High    Y  2022-11-30 08:31:00  NaN                  NaN   NaN   \n6594      High    Y  2022-11-30 08:31:00  NaN                  NaN   NaN   \n6595      High    Y  2022-11-30 10:33:00  NaN                  NaN   NaN   \n\n     clickdate Hardbounce softbounce blockedbounce unsub  \n0          NaN        NaN        NaN           NaN   NaN  \n1          NaN        NaN        NaN           NaN   NaN  \n2          NaN        NaN        NaN           NaN   NaN  \n3          NaN        NaN        NaN           NaN   NaN  \n4          NaN        NaN        NaN           NaN   NaN  \n...        ...        ...        ...           ...   ...  \n6591       NaN        NaN        NaN           NaN   NaN  \n6592       NaN        NaN        NaN           NaN   NaN  \n6593       NaN        NaN        NaN           NaN   NaN  \n6594       NaN        NaN        NaN           NaN   NaN  \n6595       NaN        NaN        NaN           NaN   NaN  \n\n[6596 rows x 32 columns]\n     MobileNumber   FirstName MiddleName           LastName Addr1 Addr2 City  \\\n0             NaN  MUTHUKUMAR        NaN         THANGAMANI   NaN   NaN  NaN   \n1             NaN       TANYA        NaN               TANG   NaN   NaN  NaN   \n2             NaN      CARLOS        NaN  CLAUDIO RODRIGUEZ   NaN   NaN  NaN   \n3             NaN        ANIL        NaN             RAMESH   NaN   NaN  NaN   \n4             NaN      DEBBIE        NaN             KURIAN   NaN   NaN  NaN   \n...           ...         ...        ...                ...   ...   ...  ...   \n6591          NaN        ERIC        NaN              CHANG   NaN   NaN  NaN   \n6592          NaN       RAJAT        NaN              LAMBA   NaN   NaN  NaN   \n6593          NaN      YASSER        NaN            ALDURRA   NaN   NaN  NaN   \n6594          NaN       SURJU        NaN              PATEL   NaN   NaN  NaN   \n6595          NaN    BUTHAYNA        NaN             DINARY   NaN   NaN  NaN   \n\n     State  Zip OptOutURL Var1 Var2 Var3 Var4         CID  \\\n0      NaN  NaN       NaN  NaN   MD  NaN  NaN     3183200   \n1      NaN  NaN       NaN  NaN   DO  NaN  NaN     3185280   \n2      NaN  NaN       NaN  NaN   MD  NaN  NaN     3197899   \n3      NaN  NaN       NaN  NaN   MD  NaN  NaN     3213692   \n4      NaN  NaN       NaN  NaN   MD  NaN  NaN     3216656   \n...    ...  ...       ...  ...  ...  ...  ...         ...   \n6591   NaN  NaN       NaN  NaN   MD  NaN  NaN  1002417132   \n6592   NaN  NaN       NaN  NaN   MD  NaN  NaN  2003651016   \n6593   NaN  NaN       NaN  NaN   MD  NaN  NaN  2003881037   \n6594   NaN  NaN       NaN  NaN   MD  NaN  NaN  2003927133   \n6595   NaN  NaN       NaN  NaN   MD  NaN  NaN  2007648581   \n\n                                   SubscriberKey          CreatedDate  \\\n0     ADHOC-BD56F9C6-E6C7-4BF2-8CF0-443F0D7D85DC  2022-11-30 08:10:00   \n1     ADHOC-3266D95D-953F-411B-BACC-F52C897F3F1A  2022-11-30 08:10:00   \n2     ADHOC-7534B849-A5ED-4366-8115-02D7FE395E21  2022-11-30 08:10:00   \n3     ADHOC-F6EB4600-33B6-4B0E-AB77-DCC81D93F4AF  2022-11-30 08:10:00   \n4     ADHOC-A10F150D-8203-4375-9CAA-54E3EBB92B8E  2022-11-30 08:10:00   \n...                                          ...                  ...   \n6591  ADHOC-92622E2E-49EF-45EC-9AD8-CCE9087C0047  2022-11-30 08:10:00   \n6592  ADHOC-1C37E2CF-33DC-46B0-BF3F-B48BB7438F73  2022-11-30 08:10:00   \n6593  ADHOC-636C0019-2AB9-4155-8B62-572EB3C9042B  2022-11-30 08:10:00   \n6594  ADHOC-A9A8A65C-14E7-4106-9560-7198674D6CDD  2022-11-30 08:10:00   \n6595  ADHOC-75FEDD73-A956-4655-B9E5-EC91657C1772  2022-11-30 08:10:00   \n\n         LastModifiedDate CampaignName       SuppressionType CampaignRunID  \\\n0     2022-11-30 07:13:00   A338-ADHOC                   NaN          A338   \n1     2022-11-30 07:13:00   A338-ADHOC          Gigya OptOut          A338   \n2     2022-11-30 07:13:00   A338-ADHOC                   NaN          A338   \n3     2022-11-30 07:13:00   A338-ADHOC                   NaN          A338   \n4     2022-11-30 07:17:00   A338-ADHOC  Blacklist/Quarantine          A338   \n...                   ...          ...                   ...           ...   \n6591  2022-11-30 07:13:00   A338-ADHOC                   NaN          A338   \n6592  2022-11-30 07:13:00   A338-ADHOC                   NaN          A338   \n6593  2022-11-30 07:13:00   A338-ADHOC                   NaN          A338   \n6594  2022-11-30 07:13:00   A338-ADHOC                   NaN          A338   \n6595  2022-11-30 07:13:00   A338-ADHOC                   NaN          A338   \n\n     HighLowIP Sent             sentdate Open            open date click  \\\n0         High    Y  2022-11-30 07:30:00  NaN                  NaN   NaN   \n1         High  NaN                  NaN  NaN                  NaN   NaN   \n2         High    Y  2022-11-30 07:30:00  NaN                  NaN   NaN   \n3         High    Y  2022-11-30 07:30:00  NaN                  NaN   NaN   \n4         High  NaN                  NaN  NaN                  NaN   NaN   \n...        ...  ...                  ...  ...                  ...   ...   \n6591      High    Y  2022-11-30 08:31:00    Y  2022-11-30 11:58:00   NaN   \n6592      High    Y  2022-11-30 08:31:00    Y  2022-11-30 17:45:00   NaN   \n6593      High    Y  2022-11-30 08:31:00  NaN                  NaN   NaN   \n6594      High    Y  2022-11-30 08:31:00  NaN                  NaN   NaN   \n6595      High    Y  2022-11-30 10:33:00  NaN                  NaN   NaN   \n\n     clickdate Hardbounce softbounce blockedbounce unsub  \n0          NaN        NaN        NaN           NaN   NaN  \n1          NaN        NaN        NaN           NaN   NaN  \n2          NaN        NaN        NaN           NaN   NaN  \n3          NaN        NaN        NaN           NaN   NaN  \n4          NaN        NaN        NaN           NaN   NaN  \n...        ...        ...        ...           ...   ...  \n6591       NaN        NaN        NaN           NaN   NaN  \n6592       NaN        NaN        NaN           NaN   NaN  \n6593       NaN        NaN        NaN           NaN   NaN  \n6594       NaN        NaN        NaN           NaN   NaN  \n6595       NaN        NaN        NaN           NaN   NaN  \n\n[6596 rows x 32 columns]\n     MobileNumber   FirstName MiddleName           LastName Addr1 Addr2 City  \\\n0             NaN  MUTHUKUMAR        NaN         THANGAMANI   NaN   NaN  NaN   \n1             NaN       TANYA        NaN               TANG   NaN   NaN  NaN   \n2             NaN      CARLOS        NaN  CLAUDIO RODRIGUEZ   NaN   NaN  NaN   \n3             NaN        ANIL        NaN             RAMESH   NaN   NaN  NaN   \n4             NaN      DEBBIE        NaN             KURIAN   NaN   NaN  NaN   \n...           ...         ...        ...                ...   ...   ...  ...   \n6591          NaN        ERIC        NaN              CHANG   NaN   NaN  NaN   \n6592          NaN       RAJAT        NaN              LAMBA   NaN   NaN  NaN   \n6593          NaN      YASSER        NaN            ALDURRA   NaN   NaN  NaN   \n6594          NaN       SURJU        NaN              PATEL   NaN   NaN  NaN   \n6595          NaN    BUTHAYNA        NaN             DINARY   NaN   NaN  NaN   \n\n     State  Zip OptOutURL Var1 Var2 Var3 Var4         CID  \\\n0      NaN  NaN       NaN  NaN   MD  NaN  NaN     3183200   \n1      NaN  NaN       NaN  NaN   DO  NaN  NaN     3185280   \n2      NaN  NaN       NaN  NaN   MD  NaN  NaN     3197899   \n3      NaN  NaN       NaN  NaN   MD  NaN  NaN     3213692   \n4      NaN  NaN       NaN  NaN   MD  NaN  NaN     3216656   \n...    ...  ...       ...  ...  ...  ...  ...         ...   \n6591   NaN  NaN       NaN  NaN   MD  NaN  NaN  1002417132   \n6592   NaN  NaN       NaN  NaN   MD  NaN  NaN  2003651016   \n6593   NaN  NaN       NaN  NaN   MD  NaN  NaN  2003881037   \n6594   NaN  NaN       NaN  NaN   MD  NaN  NaN  2003927133   \n6595   NaN  NaN       NaN  NaN   MD  NaN  NaN  2007648581   \n\n                                   SubscriberKey          CreatedDate  \\\n0     ADHOC-BD56F9C6-E6C7-4BF2-8CF0-443F0D7D85DC  2022-11-30 08:10:00   \n1     ADHOC-3266D95D-953F-411B-BACC-F52C897F3F1A  2022-11-30 08:10:00   \n2     ADHOC-7534B849-A5ED-4366-8115-02D7FE395E21  2022-11-30 08:10:00   \n3     ADHOC-F6EB4600-33B6-4B0E-AB77-DCC81D93F4AF  2022-11-30 08:10:00   \n4     ADHOC-A10F150D-8203-4375-9CAA-54E3EBB92B8E  2022-11-30 08:10:00   \n...                                          ...                  ...   \n6591  ADHOC-92622E2E-49EF-45EC-9AD8-CCE9087C0047  2022-11-30 08:10:00   \n6592  ADHOC-1C37E2CF-33DC-46B0-BF3F-B48BB7438F73  2022-11-30 08:10:00   \n6593  ADHOC-636C0019-2AB9-4155-8B62-572EB3C9042B  2022-11-30 08:10:00   \n6594  ADHOC-A9A8A65C-14E7-4106-9560-7198674D6CDD  2022-11-30 08:10:00   \n6595  ADHOC-75FEDD73-A956-4655-B9E5-EC91657C1772  2022-11-30 08:10:00   \n\n         LastModifiedDate CampaignName       SuppressionType CampaignRunID  \\\n0     2022-11-30 07:13:00   A338-ADHOC                   NaN          A338   \n1     2022-11-30 07:13:00   A338-ADHOC          Gigya OptOut          A338   \n2     2022-11-30 07:13:00   A338-ADHOC                   NaN          A338   \n3     2022-11-30 07:13:00   A338-ADHOC                   NaN          A338   \n4     2022-11-30 07:17:00   A338-ADHOC  Blacklist/Quarantine          A338   \n...                   ...          ...                   ...           ...   \n6591  2022-11-30 07:13:00   A338-ADHOC                   NaN          A338   \n6592  2022-11-30 07:13:00   A338-ADHOC                   NaN          A338   \n6593  2022-11-30 07:13:00   A338-ADHOC                   NaN          A338   \n6594  2022-11-30 07:13:00   A338-ADHOC                   NaN          A338   \n6595  2022-11-30 07:13:00   A338-ADHOC                   NaN          A338   \n\n     HighLowIP Sent             sentdate Open            open date click  \\\n0         High    Y  2022-11-30 07:30:00  NaN                  NaN   NaN   \n1         High  NaN                  NaN  NaN                  NaN   NaN   \n2         High    Y  2022-11-30 07:30:00  NaN                  NaN   NaN   \n3         High    Y  2022-11-30 07:30:00  NaN                  NaN   NaN   \n4         High  NaN                  NaN  NaN                  NaN   NaN   \n...        ...  ...                  ...  ...                  ...   ...   \n6591      High    Y  2022-11-30 08:31:00    Y  2022-11-30 11:58:00   NaN   \n6592      High    Y  2022-11-30 08:31:00    Y  2022-11-30 17:45:00   NaN   \n6593      High    Y  2022-11-30 08:31:00  NaN                  NaN   NaN   \n6594      High    Y  2022-11-30 08:31:00  NaN                  NaN   NaN   \n6595      High    Y  2022-11-30 10:33:00  NaN                  NaN   NaN   \n\n     clickdate Hardbounce softbounce blockedbounce unsub  \n0          NaN        NaN        NaN           NaN   NaN  \n1          NaN        NaN        NaN           NaN   NaN  \n2          NaN        NaN        NaN           NaN   NaN  \n3          NaN        NaN        NaN           NaN   NaN  \n4          NaN        NaN        NaN           NaN   NaN  \n...        ...        ...        ...           ...   ...  \n6591       NaN        NaN        NaN           NaN   NaN  \n6592       NaN        NaN        NaN           NaN   NaN  \n6593       NaN        NaN        NaN           NaN   NaN  \n6594       NaN        NaN        NaN           NaN   NaN  \n6595       NaN        NaN        NaN           NaN   NaN  \n\n[6596 rows x 32 columns]\n     MobileNumber   FirstName MiddleName           LastName Addr1 Addr2 City  \\\n0             NaN  MUTHUKUMAR        NaN         THANGAMANI   NaN   NaN  NaN   \n1             NaN       TANYA        NaN               TANG   NaN   NaN  NaN   \n2             NaN      CARLOS        NaN  CLAUDIO RODRIGUEZ   NaN   NaN  NaN   \n3             NaN        ANIL        NaN             RAMESH   NaN   NaN  NaN   \n4             NaN      DEBBIE        NaN             KURIAN   NaN   NaN  NaN   \n...           ...         ...        ...                ...   ...   ...  ...   \n6591          NaN        ERIC        NaN              CHANG   NaN   NaN  NaN   \n6592          NaN       RAJAT        NaN              LAMBA   NaN   NaN  NaN   \n6593          NaN      YASSER        NaN            ALDURRA   NaN   NaN  NaN   \n6594          NaN       SURJU        NaN              PATEL   NaN   NaN  NaN   \n6595          NaN    BUTHAYNA        NaN             DINARY   NaN   NaN  NaN   \n\n     State  Zip OptOutURL Var1 Var2 Var3 Var4         CID  \\\n0      NaN  NaN       NaN  NaN   MD  NaN  NaN     3183200   \n1      NaN  NaN       NaN  NaN   DO  NaN  NaN     3185280   \n2      NaN  NaN       NaN  NaN   MD  NaN  NaN     3197899   \n3      NaN  NaN       NaN  NaN   MD  NaN  NaN     3213692   \n4      NaN  NaN       NaN  NaN   MD  NaN  NaN     3216656   \n...    ...  ...       ...  ...  ...  ...  ...         ...   \n6591   NaN  NaN       NaN  NaN   MD  NaN  NaN  1002417132   \n6592   NaN  NaN       NaN  NaN   MD  NaN  NaN  2003651016   \n6593   NaN  NaN       NaN  NaN   MD  NaN  NaN  2003881037   \n6594   NaN  NaN       NaN  NaN   MD  NaN  NaN  2003927133   \n6595   NaN  NaN       NaN  NaN   MD  NaN  NaN  2007648581   \n\n                                   SubscriberKey          CreatedDate  \\\n0     ADHOC-BD56F9C6-E6C7-4BF2-8CF0-443F0D7D85DC  2022-11-30 08:10:00   \n1     ADHOC-3266D95D-953F-411B-BACC-F52C897F3F1A  2022-11-30 08:10:00   \n2     ADHOC-7534B849-A5ED-4366-8115-02D7FE395E21  2022-11-30 08:10:00   \n3     ADHOC-F6EB4600-33B6-4B0E-AB77-DCC81D93F4AF  2022-11-30 08:10:00   \n4     ADHOC-A10F150D-8203-4375-9CAA-54E3EBB92B8E  2022-11-30 08:10:00   \n...                                          ...                  ...   \n6591  ADHOC-92622E2E-49EF-45EC-9AD8-CCE9087C0047  2022-11-30 08:10:00   \n6592  ADHOC-1C37E2CF-33DC-46B0-BF3F-B48BB7438F73  2022-11-30 08:10:00   \n6593  ADHOC-636C0019-2AB9-4155-8B62-572EB3C9042B  2022-11-30 08:10:00   \n6594  ADHOC-A9A8A65C-14E7-4106-9560-7198674D6CDD  2022-11-30 08:10:00   \n6595  ADHOC-75FEDD73-A956-4655-B9E5-EC91657C1772  2022-11-30 08:10:00   \n\n         LastModifiedDate CampaignName       SuppressionType CampaignRunID  \\\n0     2022-11-30 07:13:00   A338-ADHOC                   NaN          A338   \n1     2022-11-30 07:13:00   A338-ADHOC          Gigya OptOut          A338   \n2     2022-11-30 07:13:00   A338-ADHOC                   NaN          A338   \n3     2022-11-30 07:13:00   A338-ADHOC                   NaN          A338   \n4     2022-11-30 07:17:00   A338-ADHOC  Blacklist/Quarantine          A338   \n...                   ...          ...                   ...           ...   \n6591  2022-11-30 07:13:00   A338-ADHOC                   NaN          A338   \n6592  2022-11-30 07:13:00   A338-ADHOC                   NaN          A338   \n6593  2022-11-30 07:13:00   A338-ADHOC                   NaN          A338   \n6594  2022-11-30 07:13:00   A338-ADHOC                   NaN          A338   \n6595  2022-11-30 07:13:00   A338-ADHOC                   NaN          A338   \n\n     HighLowIP Sent             sentdate Open            open date click  \\\n0         High    Y  2022-11-30 07:30:00  NaN                  NaN   NaN   \n1         High  NaN                  NaN  NaN                  NaN   NaN   \n2         High    Y  2022-11-30 07:30:00  NaN                  NaN   NaN   \n3         High    Y  2022-11-30 07:30:00  NaN                  NaN   NaN   \n4         High  NaN                  NaN  NaN                  NaN   NaN   \n...        ...  ...                  ...  ...                  ...   ...   \n6591      High    Y  2022-11-30 08:31:00    Y  2022-11-30 11:58:00   NaN   \n6592      High    Y  2022-11-30 08:31:00    Y  2022-11-30 17:45:00   NaN   \n6593      High    Y  2022-11-30 08:31:00  NaN                  NaN   NaN   \n6594      High    Y  2022-11-30 08:31:00  NaN                  NaN   NaN   \n6595      High    Y  2022-11-30 10:33:00  NaN                  NaN   NaN   \n\n     clickdate Hardbounce softbounce blockedbounce unsub  \n0          NaN        NaN        NaN           NaN   NaN  \n1          NaN        NaN        NaN           NaN   NaN  \n2          NaN        NaN        NaN           NaN   NaN  \n3          NaN        NaN        NaN           NaN   NaN  \n4          NaN        NaN        NaN           NaN   NaN  \n...        ...        ...        ...           ...   ...  \n6591       NaN        NaN        NaN           NaN   NaN  \n6592       NaN        NaN        NaN           NaN   NaN  \n6593       NaN        NaN        NaN           NaN   NaN  \n6594       NaN        NaN        NaN           NaN   NaN  \n6595       NaN        NaN        NaN           NaN   NaN  \n\n[6596 rows x 32 columns]\n     MobileNumber   FirstName MiddleName           LastName Addr1 Addr2 City  \\\n0             NaN  MUTHUKUMAR        NaN         THANGAMANI   NaN   NaN  NaN   \n1             NaN       TANYA        NaN               TANG   NaN   NaN  NaN   \n2             NaN      CARLOS        NaN  CLAUDIO RODRIGUEZ   NaN   NaN  NaN   \n3             NaN        ANIL        NaN             RAMESH   NaN   NaN  NaN   \n4             NaN      DEBBIE        NaN             KURIAN   NaN   NaN  NaN   \n...           ...         ...        ...                ...   ...   ...  ...   \n6591          NaN        ERIC        NaN              CHANG   NaN   NaN  NaN   \n6592          NaN       RAJAT        NaN              LAMBA   NaN   NaN  NaN   \n6593          NaN      YASSER        NaN            ALDURRA   NaN   NaN  NaN   \n6594          NaN       SURJU        NaN              PATEL   NaN   NaN  NaN   \n6595          NaN    BUTHAYNA        NaN             DINARY   NaN   NaN  NaN   \n\n     State  Zip OptOutURL Var1 Var2 Var3 Var4         CID  \\\n0      NaN  NaN       NaN  NaN   MD  NaN  NaN     3183200   \n1      NaN  NaN       NaN  NaN   DO  NaN  NaN     3185280   \n2      NaN  NaN       NaN  NaN   MD  NaN  NaN     3197899   \n3      NaN  NaN       NaN  NaN   MD  NaN  NaN     3213692   \n4      NaN  NaN       NaN  NaN   MD  NaN  NaN     3216656   \n...    ...  ...       ...  ...  ...  ...  ...         ...   \n6591   NaN  NaN       NaN  NaN   MD  NaN  NaN  1002417132   \n6592   NaN  NaN       NaN  NaN   MD  NaN  NaN  2003651016   \n6593   NaN  NaN       NaN  NaN   MD  NaN  NaN  2003881037   \n6594   NaN  NaN       NaN  NaN   MD  NaN  NaN  2003927133   \n6595   NaN  NaN       NaN  NaN   MD  NaN  NaN  2007648581   \n\n                                   SubscriberKey          CreatedDate  \\\n0     ADHOC-BD56F9C6-E6C7-4BF2-8CF0-443F0D7D85DC  2022-11-30 08:10:00   \n1     ADHOC-3266D95D-953F-411B-BACC-F52C897F3F1A  2022-11-30 08:10:00   \n2     ADHOC-7534B849-A5ED-4366-8115-02D7FE395E21  2022-11-30 08:10:00   \n3     ADHOC-F6EB4600-33B6-4B0E-AB77-DCC81D93F4AF  2022-11-30 08:10:00   \n4     ADHOC-A10F150D-8203-4375-9CAA-54E3EBB92B8E  2022-11-30 08:10:00   \n...                                          ...                  ...   \n6591  ADHOC-92622E2E-49EF-45EC-9AD8-CCE9087C0047  2022-11-30 08:10:00   \n6592  ADHOC-1C37E2CF-33DC-46B0-BF3F-B48BB7438F73  2022-11-30 08:10:00   \n6593  ADHOC-636C0019-2AB9-4155-8B62-572EB3C9042B  2022-11-30 08:10:00   \n6594  ADHOC-A9A8A65C-14E7-4106-9560-7198674D6CDD  2022-11-30 08:10:00   \n6595  ADHOC-75FEDD73-A956-4655-B9E5-EC91657C1772  2022-11-30 08:10:00   \n\n         LastModifiedDate CampaignName       SuppressionType CampaignRunID  \\\n0     2022-11-30 07:13:00   A338-ADHOC                   NaN          A338   \n1     2022-11-30 07:13:00   A338-ADHOC          Gigya OptOut          A338   \n2     2022-11-30 07:13:00   A338-ADHOC                   NaN          A338   \n3     2022-11-30 07:13:00   A338-ADHOC                   NaN          A338   \n4     2022-11-30 07:17:00   A338-ADHOC  Blacklist/Quarantine          A338   \n...                   ...          ...                   ...           ...   \n6591  2022-11-30 07:13:00   A338-ADHOC                   NaN          A338   \n6592  2022-11-30 07:13:00   A338-ADHOC                   NaN          A338   \n6593  2022-11-30 07:13:00   A338-ADHOC                   NaN          A338   \n6594  2022-11-30 07:13:00   A338-ADHOC                   NaN          A338   \n6595  2022-11-30 07:13:00   A338-ADHOC                   NaN          A338   \n\n     HighLowIP Sent             sentdate Open            open date click  \\\n0         High    Y  2022-11-30 07:30:00  NaN                  NaN   NaN   \n1         High  NaN                  NaN  NaN                  NaN   NaN   \n2         High    Y  2022-11-30 07:30:00  NaN                  NaN   NaN   \n3         High    Y  2022-11-30 07:30:00  NaN                  NaN   NaN   \n4         High  NaN                  NaN  NaN                  NaN   NaN   \n...        ...  ...                  ...  ...                  ...   ...   \n6591      High    Y  2022-11-30 08:31:00    Y  2022-11-30 11:58:00   NaN   \n6592      High    Y  2022-11-30 08:31:00    Y  2022-11-30 17:45:00   NaN   \n6593      High    Y  2022-11-30 08:31:00  NaN                  NaN   NaN   \n6594      High    Y  2022-11-30 08:31:00  NaN \n\n*** WARNING: max output size exceeded, skipping output. ***\n\nNaN   NaN  \n8874        NaN        NaN           NaN   NaN  \n8875        NaN        NaN           NaN   NaN  \n8876        NaN        NaN           NaN   NaN  \n\n[8877 rows x 32 columns]\n     MobileNumber  FirstName MiddleName      LastName Addr1 Addr2 City State  \\\n0             NaN      IMRAN        NaN         SHUJA   NaN   NaN  NaN   NaN   \n1             NaN      IMRAN        NaN        JAWAID   NaN   NaN  NaN   NaN   \n2             NaN      VINNY        NaN         ANAND   NaN   NaN  NaN   NaN   \n3             NaN  INDRANEEL        NaN      MOGARALA   NaN   NaN  NaN   NaN   \n4             NaN    ORLANDO        NaN  CAMACHO GECK   NaN   NaN  NaN   NaN   \n...           ...        ...        ...           ...   ...   ...  ...   ...   \n8872          NaN   THEODORE        NaN          SAAD   NaN   NaN  NaN   NaN   \n8873          NaN  TAKAMITSU        NaN       SAIGUSA   NaN   NaN  NaN   NaN   \n8874          NaN      TAHIR        NaN        SAJJAD   NaN   NaN  NaN   NaN   \n8875          NaN      TAREK        NaN       SALKINI   NaN   NaN  NaN   NaN   \n8876          NaN      TSEGA        NaN       DENNEKE   NaN   NaN  NaN   NaN   \n\n      Zip OptOutURL Var1 Var2 Var3 Var4         CID  \\\n0     NaN       NaN  NaN  NaN  NaN  NaN  2021140067   \n1     NaN       NaN  NaN  NaN  NaN  NaN  2023053865   \n2     NaN       NaN  NaN  NaN  NaN  NaN      666710   \n3     NaN       NaN  NaN  NaN  NaN  NaN     3054905   \n4     NaN       NaN  NaN  NaN  NaN  NaN  2033634412   \n...   ...       ...  ...  ...  ...  ...         ...   \n8872  NaN       NaN  NaN  NaN  NaN  NaN      463163   \n8873  NaN       NaN  NaN  NaN  NaN  NaN     3183903   \n8874  NaN       NaN  NaN  NaN  NaN  NaN      736779   \n8875  NaN       NaN  NaN  NaN  NaN  NaN      787178   \n8876  NaN       NaN  NaN  NaN  NaN  NaN      651486   \n\n                                   SubscriberKey          CreatedDate  \\\n0     ADHOC-12134071-A2F4-4496-817A-D489878EA01C  2023-10-18 04:28:03   \n1     ADHOC-D2D61A01-6735-41CD-AEE5-B6A69083E5B5  2023-10-18 04:28:03   \n2     ADHOC-6D0AB25F-F2D2-4CFC-B68F-D4EDD9D4BF52  2023-10-18 04:28:03   \n3     ADHOC-F8A4E636-8619-4FB2-9F16-3A14A3CF0DE1  2023-10-18 04:28:03   \n4     ADHOC-1EAD0F62-CB36-46EA-8886-7F25F4FF924E  2023-10-18 04:28:03   \n...                                          ...                  ...   \n8872  ADHOC-F4D745A7-B2CB-4E7C-AD4C-174FF47C041D  2023-10-18 04:28:03   \n8873  ADHOC-F517DD7F-9C92-4728-B3B0-3D926649829B  2023-10-18 04:28:03   \n8874  ADHOC-35905432-95A3-4E8F-9594-09A77D764BF8  2023-10-18 04:28:03   \n8875  ADHOC-0B4D0B36-0379-4732-82A4-EB324ECD1078  2023-10-18 04:28:03   \n8876  ADHOC-89BDAAB3-19C5-4174-979D-D25AA3A4D677  2023-10-18 04:28:03   \n\n         LastModifiedDate        CampaignName SuppressionType CampaignRunID  \\\n0     2023-10-18 02:29:08  A743_PRE_ASN_Email    Valid Record          A743   \n1     2023-10-18 02:29:08  A743_PRE_ASN_Email    Valid Record          A743   \n2     2023-10-18 02:29:08  A743_PRE_ASN_Email    Valid Record          A743   \n3     2023-10-18 02:29:08  A743_PRE_ASN_Email    Valid Record          A743   \n4     2023-10-18 02:29:08  A743_PRE_ASN_Email    Valid Record          A743   \n...                   ...                 ...             ...           ...   \n8872  2023-10-18 02:29:08  A743_PRE_ASN_Email    Bounced/Held          A743   \n8873  2023-10-18 02:29:08  A743_PRE_ASN_Email    Valid Record          A743   \n8874  2023-10-18 02:29:08  A743_PRE_ASN_Email    Valid Record          A743   \n8875  2023-10-18 02:29:08  A743_PRE_ASN_Email    Valid Record          A743   \n8876  2023-10-18 02:29:08  A743_PRE_ASN_Email    Valid Record          A743   \n\n     HighLowIP Sent             sentdate Open open date click clickdate  \\\n0         High    Y  2023-10-18 07:05:00  NaN       NaN   NaN       NaN   \n1         High    Y  2023-10-18 07:05:00  NaN       NaN   NaN       NaN   \n2         High    Y  2023-10-18 07:05:00  NaN       NaN   NaN       NaN   \n3         High    Y  2023-10-18 07:05:00  NaN       NaN   NaN       NaN   \n4         High  NaN                  NaN  NaN       NaN   NaN       NaN   \n...        ...  ...                  ...  ...       ...   ...       ...   \n8872      High  NaN                  NaN  NaN       NaN   NaN       NaN   \n8873      High    Y  2023-10-18 07:06:00  NaN       NaN   NaN       NaN   \n8874      High    Y  2023-10-18 07:06:00  NaN       NaN   NaN       NaN   \n8875      High    Y  2023-10-18 07:06:00  NaN       NaN   NaN       NaN   \n8876      High    Y  2023-10-18 07:06:00  NaN       NaN   NaN       NaN   \n\n     Hardbounce softbounce blockedbounce unsub  \n0           NaN        NaN           NaN   NaN  \n1           NaN        NaN           NaN   NaN  \n2           NaN        NaN           NaN   NaN  \n3           NaN        NaN           NaN   NaN  \n4           NaN        NaN           NaN   NaN  \n...         ...        ...           ...   ...  \n8872        NaN        NaN           NaN   NaN  \n8873        NaN        NaN           NaN   NaN  \n8874        NaN        NaN           NaN   NaN  \n8875        NaN        NaN           NaN   NaN  \n8876        NaN        NaN           NaN   NaN  \n\n[8877 rows x 32 columns]\n     MobileNumber  FirstName MiddleName      LastName Addr1 Addr2 City State  \\\n0             NaN      IMRAN        NaN         SHUJA   NaN   NaN  NaN   NaN   \n1             NaN      IMRAN        NaN        JAWAID   NaN   NaN  NaN   NaN   \n2             NaN      VINNY        NaN         ANAND   NaN   NaN  NaN   NaN   \n3             NaN  INDRANEEL        NaN      MOGARALA   NaN   NaN  NaN   NaN   \n4             NaN    ORLANDO        NaN  CAMACHO GECK   NaN   NaN  NaN   NaN   \n...           ...        ...        ...           ...   ...   ...  ...   ...   \n8872          NaN   THEODORE        NaN          SAAD   NaN   NaN  NaN   NaN   \n8873          NaN  TAKAMITSU        NaN       SAIGUSA   NaN   NaN  NaN   NaN   \n8874          NaN      TAHIR        NaN        SAJJAD   NaN   NaN  NaN   NaN   \n8875          NaN      TAREK        NaN       SALKINI   NaN   NaN  NaN   NaN   \n8876          NaN      TSEGA        NaN       DENNEKE   NaN   NaN  NaN   NaN   \n\n      Zip OptOutURL Var1 Var2 Var3 Var4         CID  \\\n0     NaN       NaN  NaN  NaN  NaN  NaN  2021140067   \n1     NaN       NaN  NaN  NaN  NaN  NaN  2023053865   \n2     NaN       NaN  NaN  NaN  NaN  NaN      666710   \n3     NaN       NaN  NaN  NaN  NaN  NaN     3054905   \n4     NaN       NaN  NaN  NaN  NaN  NaN  2033634412   \n...   ...       ...  ...  ...  ...  ...         ...   \n8872  NaN       NaN  NaN  NaN  NaN  NaN      463163   \n8873  NaN       NaN  NaN  NaN  NaN  NaN     3183903   \n8874  NaN       NaN  NaN  NaN  NaN  NaN      736779   \n8875  NaN       NaN  NaN  NaN  NaN  NaN      787178   \n8876  NaN       NaN  NaN  NaN  NaN  NaN      651486   \n\n                                   SubscriberKey          CreatedDate  \\\n0     ADHOC-12134071-A2F4-4496-817A-D489878EA01C  2023-10-18 04:28:03   \n1     ADHOC-D2D61A01-6735-41CD-AEE5-B6A69083E5B5  2023-10-18 04:28:03   \n2     ADHOC-6D0AB25F-F2D2-4CFC-B68F-D4EDD9D4BF52  2023-10-18 04:28:03   \n3     ADHOC-F8A4E636-8619-4FB2-9F16-3A14A3CF0DE1  2023-10-18 04:28:03   \n4     ADHOC-1EAD0F62-CB36-46EA-8886-7F25F4FF924E  2023-10-18 04:28:03   \n...                                          ...                  ...   \n8872  ADHOC-F4D745A7-B2CB-4E7C-AD4C-174FF47C041D  2023-10-18 04:28:03   \n8873  ADHOC-F517DD7F-9C92-4728-B3B0-3D926649829B  2023-10-18 04:28:03   \n8874  ADHOC-35905432-95A3-4E8F-9594-09A77D764BF8  2023-10-18 04:28:03   \n8875  ADHOC-0B4D0B36-0379-4732-82A4-EB324ECD1078  2023-10-18 04:28:03   \n8876  ADHOC-89BDAAB3-19C5-4174-979D-D25AA3A4D677  2023-10-18 04:28:03   \n\n         LastModifiedDate        CampaignName SuppressionType CampaignRunID  \\\n0     2023-10-18 02:29:08  A743_PRE_ASN_Email    Valid Record          A743   \n1     2023-10-18 02:29:08  A743_PRE_ASN_Email    Valid Record          A743   \n2     2023-10-18 02:29:08  A743_PRE_ASN_Email    Valid Record          A743   \n3     2023-10-18 02:29:08  A743_PRE_ASN_Email    Valid Record          A743   \n4     2023-10-18 02:29:08  A743_PRE_ASN_Email    Valid Record          A743   \n...                   ...                 ...             ...           ...   \n8872  2023-10-18 02:29:08  A743_PRE_ASN_Email    Bounced/Held          A743   \n8873  2023-10-18 02:29:08  A743_PRE_ASN_Email    Valid Record          A743   \n8874  2023-10-18 02:29:08  A743_PRE_ASN_Email    Valid Record          A743   \n8875  2023-10-18 02:29:08  A743_PRE_ASN_Email    Valid Record          A743   \n8876  2023-10-18 02:29:08  A743_PRE_ASN_Email    Valid Record          A743   \n\n     HighLowIP Sent             sentdate Open open date click clickdate  \\\n0         High    Y  2023-10-18 07:05:00  NaN       NaN   NaN       NaN   \n1         High    Y  2023-10-18 07:05:00  NaN       NaN   NaN       NaN   \n2         High    Y  2023-10-18 07:05:00  NaN       NaN   NaN       NaN   \n3         High    Y  2023-10-18 07:05:00  NaN       NaN   NaN       NaN   \n4         High  NaN                  NaN  NaN       NaN   NaN       NaN   \n...        ...  ...                  ...  ...       ...   ...       ...   \n8872      High  NaN                  NaN  NaN       NaN   NaN       NaN   \n8873      High    Y  2023-10-18 07:06:00  NaN       NaN   NaN       NaN   \n8874      High    Y  2023-10-18 07:06:00  NaN       NaN   NaN       NaN   \n8875      High    Y  2023-10-18 07:06:00  NaN       NaN   NaN       NaN   \n8876      High    Y  2023-10-18 07:06:00  NaN       NaN   NaN       NaN   \n\n     Hardbounce softbounce blockedbounce unsub  \n0           NaN        NaN           NaN   NaN  \n1           NaN        NaN           NaN   NaN  \n2           NaN        NaN           NaN   NaN  \n3           NaN        NaN           NaN   NaN  \n4           NaN        NaN           NaN   NaN  \n...         ...        ...           ...   ...  \n8872        NaN        NaN           NaN   NaN  \n8873        NaN        NaN           NaN   NaN  \n8874        NaN        NaN           NaN   NaN  \n8875        NaN        NaN           NaN   NaN  \n8876        NaN        NaN           NaN   NaN  \n\n[8877 rows x 32 columns]\n     MobileNumber  FirstName MiddleName      LastName Addr1 Addr2 City State  \\\n0             NaN      IMRAN        NaN         SHUJA   NaN   NaN  NaN   NaN   \n1             NaN      IMRAN        NaN        JAWAID   NaN   NaN  NaN   NaN   \n2             NaN      VINNY        NaN         ANAND   NaN   NaN  NaN   NaN   \n3             NaN  INDRANEEL        NaN      MOGARALA   NaN   NaN  NaN   NaN   \n4             NaN    ORLANDO        NaN  CAMACHO GECK   NaN   NaN  NaN   NaN   \n...           ...        ...        ...           ...   ...   ...  ...   ...   \n8872          NaN   THEODORE        NaN          SAAD   NaN   NaN  NaN   NaN   \n8873          NaN  TAKAMITSU        NaN       SAIGUSA   NaN   NaN  NaN   NaN   \n8874          NaN      TAHIR        NaN        SAJJAD   NaN   NaN  NaN   NaN   \n8875          NaN      TAREK        NaN       SALKINI   NaN   NaN  NaN   NaN   \n8876          NaN      TSEGA        NaN       DENNEKE   NaN   NaN  NaN   NaN   \n\n      Zip OptOutURL Var1 Var2 Var3 Var4         CID  \\\n0     NaN       NaN  NaN  NaN  NaN  NaN  2021140067   \n1     NaN       NaN  NaN  NaN  NaN  NaN  2023053865   \n2     NaN       NaN  NaN  NaN  NaN  NaN      666710   \n3     NaN       NaN  NaN  NaN  NaN  NaN     3054905   \n4     NaN       NaN  NaN  NaN  NaN  NaN  2033634412   \n...   ...       ...  ...  ...  ...  ...         ...   \n8872  NaN       NaN  NaN  NaN  NaN  NaN      463163   \n8873  NaN       NaN  NaN  NaN  NaN  NaN     3183903   \n8874  NaN       NaN  NaN  NaN  NaN  NaN      736779   \n8875  NaN       NaN  NaN  NaN  NaN  NaN      787178   \n8876  NaN       NaN  NaN  NaN  NaN  NaN      651486   \n\n                                   SubscriberKey          CreatedDate  \\\n0     ADHOC-12134071-A2F4-4496-817A-D489878EA01C  2023-10-18 04:28:03   \n1     ADHOC-D2D61A01-6735-41CD-AEE5-B6A69083E5B5  2023-10-18 04:28:03   \n2     ADHOC-6D0AB25F-F2D2-4CFC-B68F-D4EDD9D4BF52  2023-10-18 04:28:03   \n3     ADHOC-F8A4E636-8619-4FB2-9F16-3A14A3CF0DE1  2023-10-18 04:28:03   \n4     ADHOC-1EAD0F62-CB36-46EA-8886-7F25F4FF924E  2023-10-18 04:28:03   \n...                                          ...                  ...   \n8872  ADHOC-F4D745A7-B2CB-4E7C-AD4C-174FF47C041D  2023-10-18 04:28:03   \n8873  ADHOC-F517DD7F-9C92-4728-B3B0-3D926649829B  2023-10-18 04:28:03   \n8874  ADHOC-35905432-95A3-4E8F-9594-09A77D764BF8  2023-10-18 04:28:03   \n8875  ADHOC-0B4D0B36-0379-4732-82A4-EB324ECD1078  2023-10-18 04:28:03   \n8876  ADHOC-89BDAAB3-19C5-4174-979D-D25AA3A4D677  2023-10-18 04:28:03   \n\n         LastModifiedDate        CampaignName SuppressionType CampaignRunID  \\\n0     2023-10-18 02:29:08  A743_PRE_ASN_Email    Valid Record          A743   \n1     2023-10-18 02:29:08  A743_PRE_ASN_Email    Valid Record          A743   \n2     2023-10-18 02:29:08  A743_PRE_ASN_Email    Valid Record          A743   \n3     2023-10-18 02:29:08  A743_PRE_ASN_Email    Valid Record          A743   \n4     2023-10-18 02:29:08  A743_PRE_ASN_Email    Valid Record          A743   \n...                   ...                 ...             ...           ...   \n8872  2023-10-18 02:29:08  A743_PRE_ASN_Email    Bounced/Held          A743   \n8873  2023-10-18 02:29:08  A743_PRE_ASN_Email    Valid Record          A743   \n8874  2023-10-18 02:29:08  A743_PRE_ASN_Email    Valid Record          A743   \n8875  2023-10-18 02:29:08  A743_PRE_ASN_Email    Valid Record          A743   \n8876  2023-10-18 02:29:08  A743_PRE_ASN_Email    Valid Record          A743   \n\n     HighLowIP Sent             sentdate Open open date click clickdate  \\\n0         High    Y  2023-10-18 07:05:00  NaN       NaN   NaN       NaN   \n1         High    Y  2023-10-18 07:05:00  NaN       NaN   NaN       NaN   \n2         High    Y  2023-10-18 07:05:00  NaN       NaN   NaN       NaN   \n3         High    Y  2023-10-18 07:05:00  NaN       NaN   NaN       NaN   \n4         High  NaN                  NaN  NaN       NaN   NaN       NaN   \n...        ...  ...                  ...  ...       ...   ...       ...   \n8872      High  NaN                  NaN  NaN       NaN   NaN       NaN   \n8873      High    Y  2023-10-18 07:06:00  NaN       NaN   NaN       NaN   \n8874      High    Y  2023-10-18 07:06:00  NaN       NaN   NaN       NaN   \n8875      High    Y  2023-10-18 07:06:00  NaN       NaN   NaN       NaN   \n8876      High    Y  2023-10-18 07:06:00  NaN       NaN   NaN       NaN   \n\n     Hardbounce softbounce blockedbounce unsub  \n0           NaN        NaN           NaN   NaN  \n1           NaN        NaN           NaN   NaN  \n2           NaN        NaN           NaN   NaN  \n3           NaN        NaN           NaN   NaN  \n4           NaN        NaN           NaN   NaN  \n...         ...        ...           ...   ...  \n8872        NaN        NaN           NaN   NaN  \n8873        NaN        NaN           NaN   NaN  \n8874        NaN        NaN           NaN   NaN  \n8875        NaN        NaN           NaN   NaN  \n8876        NaN        NaN           NaN   NaN  \n\n[8877 rows x 32 columns]\n     MobileNumber  FirstName MiddleName      LastName Addr1 Addr2 City State  \\\n0             NaN      IMRAN        NaN         SHUJA   NaN   NaN  NaN   NaN   \n1             NaN      IMRAN        NaN        JAWAID   NaN   NaN  NaN   NaN   \n2             NaN      VINNY        NaN         ANAND   NaN   NaN  NaN   NaN   \n3             NaN  INDRANEEL        NaN      MOGARALA   NaN   NaN  NaN   NaN   \n4             NaN    ORLANDO        NaN  CAMACHO GECK   NaN   NaN  NaN   NaN   \n...           ...        ...        ...           ...   ...   ...  ...   ...   \n8872          NaN   THEODORE        NaN          SAAD   NaN   NaN  NaN   NaN   \n8873          NaN  TAKAMITSU        NaN       SAIGUSA   NaN   NaN  NaN   NaN   \n8874          NaN      TAHIR        NaN        SAJJAD   NaN   NaN  NaN   NaN   \n8875          NaN      TAREK        NaN       SALKINI   NaN   NaN  NaN   NaN   \n8876          NaN      TSEGA        NaN       DENNEKE   NaN   NaN  NaN   NaN   \n\n      Zip OptOutURL Var1 Var2 Var3 Var4         CID  \\\n0     NaN       NaN  NaN  NaN  NaN  NaN  2021140067   \n1     NaN       NaN  NaN  NaN  NaN  NaN  2023053865   \n2     NaN       NaN  NaN  NaN  NaN  NaN      666710   \n3     NaN       NaN  NaN  NaN  NaN  NaN     3054905   \n4     NaN       NaN  NaN  NaN  NaN  NaN  2033634412   \n...   ...       ...  ...  ...  ...  ...         ...   \n8872  NaN       NaN  NaN  NaN  NaN  NaN      463163   \n8873  NaN       NaN  NaN  NaN  NaN  NaN     3183903   \n8874  NaN       NaN  NaN  NaN  NaN  NaN      736779   \n8875  NaN       NaN  NaN  NaN  NaN  NaN      787178   \n8876  NaN       NaN  NaN  NaN  NaN  NaN      651486   \n\n                                   SubscriberKey          CreatedDate  \\\n0     ADHOC-12134071-A2F4-4496-817A-D489878EA01C  2023-10-18 04:28:03   \n1     ADHOC-D2D61A01-6735-41CD-AEE5-B6A69083E5B5  2023-10-18 04:28:03   \n2     ADHOC-6D0AB25F-F2D2-4CFC-B68F-D4EDD9D4BF52  2023-10-18 04:28:03   \n3     ADHOC-F8A4E636-8619-4FB2-9F16-3A14A3CF0DE1  2023-10-18 04:28:03   \n4     ADHOC-1EAD0F62-CB36-46EA-8886-7F25F4FF924E  2023-10-18 04:28:03   \n...                                          ...                  ...   \n8872  ADHOC-F4D745A7-B2CB-4E7C-AD4C-174FF47C041D  2023-10-18 04:28:03   \n8873  ADHOC-F517DD7F-9C92-4728-B3B0-3D926649829B  2023-10-18 04:28:03   \n8874  ADHOC-35905432-95A3-4E8F-9594-09A77D764BF8  2023-10-18 04:28:03   \n8875  ADHOC-0B4D0B36-0379-4732-82A4-EB324ECD1078  2023-10-18 04:28:03   \n8876  ADHOC-89BDAAB3-19C5-4174-979D-D25AA3A4D677  2023-10-18 04:28:03   \n\n         LastModifiedDate        CampaignName SuppressionType CampaignRunID  \\\n0     2023-10-18 02:29:08  A743_PRE_ASN_Email    Valid Record          A743   \n1     2023-10-18 02:29:08  A743_PRE_ASN_Email    Valid Record          A743   \n2     2023-10-18 02:29:08  A743_PRE_ASN_Email    Valid Record          A743   \n3     2023-10-18 02:29:08  A743_PRE_ASN_Email    Valid Record          A743   \n4     2023-10-18 02:29:08  A743_PRE_ASN_Email    Valid Record          A743   \n...                   ...                 ...             ...           ...   \n8872  2023-10-18 02:29:08  A743_PRE_ASN_Email    Bounced/Held          A743   \n8873  2023-10-18 02:29:08  A743_PRE_ASN_Email    Valid Record          A743   \n8874  2023-10-18 02:29:08  A743_PRE_ASN_Email    Valid Record          A743   \n8875  2023-10-18 02:29:08  A743_PRE_ASN_Email    Valid Record          A743   \n8876  2023-10-18 02:29:08  A743_PRE_ASN_Email    Valid Record          A743   \n\n     HighLowIP Sent             sentdate Open open date click clickdate  \\\n0         High    Y  2023-10-18 07:05:00  NaN       NaN   NaN       NaN   \n1         High    Y  2023-10-18 07:05:00  NaN       NaN   NaN       NaN   \n2         High    Y  2023-10-18 07:05:00  NaN       NaN   NaN       NaN   \n3         High    Y  2023-10-18 07:05:00  NaN       NaN   NaN       NaN   \n4         High  NaN                  NaN  NaN       NaN   NaN       NaN   \n...        ...  ...                  ...  ...       ...   ...       ...   \n8872      High  NaN                  NaN  NaN       NaN   NaN       NaN   \n8873      High    Y  2023-10-18 07:06:00  NaN       NaN   NaN       NaN   \n8874      High    Y  2023-10-18 07:06:00  NaN       NaN   NaN       NaN   \n8875      High    Y  2023-10-18 07:06:00  NaN       NaN   NaN       NaN   \n8876      High    Y  2023-10-18 07:06:00  NaN       NaN   NaN       NaN   \n\n     Hardbounce softbounce blockedbounce unsub  \n0           NaN        NaN           NaN   NaN  \n1           NaN        NaN           NaN   NaN  \n2           NaN        NaN           NaN   NaN  \n3           NaN        NaN           NaN   NaN  \n4           NaN        NaN           NaN   NaN  \n...         ...        ...           ...   ...  \n8872        NaN        NaN           NaN   NaN  \n8873        NaN        NaN           NaN   NaN  \n8874        NaN        NaN           NaN   NaN  \n8875        NaN        NaN           NaN   NaN  \n8876        NaN        NaN           NaN   NaN  \n\n[8877 rows x 32 columns]\n     MobileNumber  FirstName MiddleName      LastName Addr1 Addr2 City State  \\\n0             NaN      IMRAN        NaN         SHUJA   NaN   NaN  NaN   NaN   \n1             NaN      IMRAN        NaN        JAWAID   NaN   NaN  NaN   NaN   \n2             NaN      VINNY        NaN         ANAND   NaN   NaN  NaN   NaN   \n3             NaN  INDRANEEL        NaN      MOGARALA   NaN   NaN  NaN   NaN   \n4             NaN    ORLANDO        NaN  CAMACHO GECK   NaN   NaN  NaN   NaN   \n...           ...        ...        ...           ...   ...   ...  ...   ...   \n8872          NaN   THEODORE        NaN          SAAD   NaN   NaN  NaN   NaN   \n8873          NaN  TAKAMITSU        NaN       SAIGUSA   NaN   NaN  NaN   NaN   \n8874          NaN      TAHIR        NaN        SAJJAD   NaN   NaN  NaN   NaN   \n8875          NaN      TAREK        NaN       SALKINI   NaN   NaN  NaN   NaN   \n8876          NaN      TSEGA        NaN       DENNEKE   NaN   NaN  NaN   NaN   \n\n      Zip OptOutURL Var1 Var2 Var3 Var4         CID  \\\n0     NaN       NaN  NaN  NaN  NaN  NaN  2021140067   \n1     NaN       NaN  NaN  NaN  NaN  NaN  2023053865   \n2     NaN       NaN  NaN  NaN  NaN  NaN      666710   \n3     NaN       NaN  NaN  NaN  NaN  NaN     3054905   \n4     NaN       NaN  NaN  NaN  NaN  NaN  2033634412   \n...   ...       ...  ...  ...  ...  ...         ...   \n8872  NaN       NaN  NaN  NaN  NaN  NaN      463163   \n8873  NaN       NaN  NaN  NaN  NaN  NaN     3183903   \n8874  NaN       NaN  NaN  NaN  NaN  NaN      736779   \n8875  NaN       NaN  NaN  NaN  NaN  NaN      787178   \n8876  NaN       NaN  NaN  NaN  NaN  NaN      651486   \n\n                                   SubscriberKey          CreatedDate  \\\n0     ADHOC-12134071-A2F4-4496-817A-D489878EA01C  2023-10-18 04:28:03   \n1     ADHOC-D2D61A01-6735-41CD-AEE5-B6A69083E5B5  2023-10-18 04:28:03   \n2     ADHOC-6D0AB25F-F2D2-4CFC-B68F-D4EDD9D4BF52  2023-10-18 04:28:03   \n3     ADHOC-F8A4E636-8619-4FB2-9F16-3A14A3CF0DE1  2023-10-18 04:28:03   \n4     ADHOC-1EAD0F62-CB36-46EA-8886-7F25F4FF924E  2023-10-18 04:28:03   \n...                                          ...                  ...   \n8872  ADHOC-F4D745A7-B2CB-4E7C-AD4C-174FF47C041D  2023-10-18 04:28:03   \n8873  ADHOC-F517DD7F-9C92-4728-B3B0-3D926649829B  2023-10-18 04:28:03   \n8874  ADHOC-35905432-95A3-4E8F-9594-09A77D764BF8  2023-10-18 04:28:03   \n8875  ADHOC-0B4D0B36-0379-4732-82A4-EB324ECD1078  2023-10-18 04:28:03   \n8876  ADHOC-89BDAAB3-19C5-4174-979D-D25AA3A4D677  2023-10-18 04:28:03   \n\n         LastModifiedDate        CampaignName SuppressionType CampaignRunID  \\\n0     2023-10-18 02:29:08  A743_PRE_ASN_Email    Valid Record          A743   \n1     2023-10-18 02:29:08  A743_PRE_ASN_Email    Valid Record          A743   \n2     2023-10-18 02:29:08  A743_PRE_ASN_Email    Valid Record          A743   \n3     2023-10-18 02:29:08  A743_PRE_ASN_Email    Valid Record          A743   \n4     2023-10-18 02:29:08  A743_PRE_ASN_Email    Valid Record          A743   \n...                   ...                 ...             ...           ...   \n8872  2023-10-18 02:29:08  A743_PRE_ASN_Email    Bounced/Held          A743   \n8873  2023-10-18 02:29:08  A743_PRE_ASN_Email    Valid Record          A743   \n8874  2023-10-18 02:29:08  A743_PRE_ASN_Email    Valid Record          A743   \n8875  2023-10-18 02:29:08  A743_PRE_ASN_Email    Valid Record          A743   \n8876  2023-10-18 02:29:08  A743_PRE_ASN_Email    Valid Record          A743   \n\n     HighLowIP Sent             sentdate Open open date click clickdate  \\\n0         High    Y  2023-10-18 07:05:00  NaN       NaN   NaN       NaN   \n1         High    Y  2023-10-18 07:05:00  NaN       NaN   NaN       NaN   \n2         High    Y  2023-10-18 07:05:00  NaN       NaN   NaN       NaN   \n3         High    Y  2023-10-18 07:05:00  NaN       NaN   NaN       NaN   \n4         High  NaN                  NaN  NaN       NaN   NaN       NaN   \n...        ...  ...                  ...  ...       ...   ...       ...   \n8872      High  NaN                  NaN  NaN       NaN   NaN       NaN   \n8873      High    Y  2023-10-18 07:06:00  NaN       NaN   NaN       NaN   \n8874      High    Y  2023-10-18 07:06:00  NaN       NaN   NaN       NaN   \n8875      High    Y  2023-10-18 07:06:00  NaN       NaN   NaN       NaN   \n8876      High    Y  2023-10-18 07:06:00  NaN       NaN   NaN       NaN   \n\n     Hardbounce softbounce blockedbounce unsub  \n0           NaN        NaN           NaN   NaN  \n1           NaN        NaN           NaN   NaN  \n2           NaN        NaN           NaN   NaN  \n3           NaN        NaN           NaN   NaN  \n4           NaN        NaN           NaN   NaN  \n...         ...        ...           ...   ...  \n8872        NaN        NaN           NaN   NaN  \n8873        NaN        NaN           NaN   NaN  \n8874        NaN        NaN           NaN   NaN  \n8875        NaN        NaN           NaN   NaN  \n8876        NaN        NaN           NaN   NaN  \n\n[8877 rows x 32 columns]\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Prepare a new Dictionary to bring the data in Silver Layer\n",
    "\n",
    "# Create an empty dictionary to store DataFrames in silver layer\n",
    "silver_dataframe = pd.DataFrame()\n",
    "\n",
    "for file_name, df in bronze_dataframes.items():  \n",
    "  print(file_name)\n",
    "\n",
    "  # Create an empty DataFrame to store matched columns\n",
    "  silver_df = pd.DataFrame()\n",
    "\n",
    "  ##############################################################################################################################################################\n",
    "  # Create dummy dataframe columns\n",
    "\n",
    "  ##############################################################################################################################################################\n",
    "  # Create an empty DataFrame to store tuples\n",
    "\n",
    "  best_match = pd.DataFrame(columns=[\"Column_Name_Search_Query\", \"Dataframe_Column_Name_Found\", \"Dataframe_Column_Fuzzy_Match_Score\", \"Dataframe_Column_Row_Count\"])  # Replace with your column names\n",
    "  \n",
    "  #start\n",
    "  flag = 0\n",
    "  for search_col in search_column_name_emailaddress:\n",
    "\n",
    "    data_to_add = fuzzy_column_match(df, search_col)\n",
    "\n",
    "    new_row = pd.DataFrame([data_to_add]) # converting to pandas dataframe\n",
    "\n",
    "    best_match = pd.concat([best_match, new_row], ignore_index=True) # append to a new dataframe\n",
    "    \n",
    "  #print(best_match)\n",
    "  # Find the index of the row with the maximum value in Dataframe_Column_Fuzzy_Match_Score\n",
    "  best_match['Dataframe_Column_Fuzzy_Match_Score'] = best_match['Dataframe_Column_Fuzzy_Match_Score'].astype(float)\n",
    "  max_index_fuzzy_score = best_match['Dataframe_Column_Fuzzy_Match_Score'].idxmax()\n",
    "    \n",
    "  # Find the index of the row with the maximum value in Dataframe_Column_Row_Count\n",
    "  best_match['Dataframe_Column_Row_Count'] = best_match['Dataframe_Column_Row_Count'].astype(float)\n",
    "  max_index_row_count = best_match['Dataframe_Column_Row_Count'].idxmax()  \n",
    "\n",
    "  if max_index_fuzzy_score == max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_fuzzy_score] > 0 and best_match['Dataframe_Column_Fuzzy_Match_Score'][max_index_fuzzy_score] > 80:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Fuzzy_Match_Score'][max_index_fuzzy_score] > 80:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_fuzzy_score] > 0:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "    flag = 1\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_row_count] > 0:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_row_count]\n",
    "    flag = 1   \n",
    "  else:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_row_count]\n",
    "    flag = 1\n",
    "  #end\n",
    "\n",
    "  if flag == 0:\n",
    "    silver_df['emailaddress'] = df[max_row_column_name].fillna('')\n",
    "  else:\n",
    "    silver_df['emailaddress'] = np.nan\n",
    "\n",
    "  ##############################################################################################################################################################\n",
    "  # Create an empty DataFrame to store tuples\n",
    "\n",
    "  best_match = pd.DataFrame(columns=[\"Column_Name_Search_Query\", \"Dataframe_Column_Name_Found\", \"Dataframe_Column_Fuzzy_Match_Score\", \"Dataframe_Column_Row_Count\"])  # Replace with your column names  \n",
    "  \n",
    "  #start\n",
    "  flag = 0\n",
    "  for search_col in search_column_name_mobilenumber:\n",
    "  \n",
    "    data_to_add = fuzzy_column_match(df, search_col)\n",
    "\n",
    "    new_row = pd.DataFrame([data_to_add]) # converting to pandas dataframe\n",
    "\n",
    "    best_match = pd.concat([best_match, new_row], ignore_index=True) # append to a new dataframe\n",
    "\n",
    "  # Find the index of the row with the maximum value in Dataframe_Column_Fuzzy_Match_Score\n",
    "  best_match['Dataframe_Column_Fuzzy_Match_Score'] = best_match['Dataframe_Column_Fuzzy_Match_Score'].astype(float)\n",
    "  max_index_fuzzy_score = best_match['Dataframe_Column_Fuzzy_Match_Score'].idxmax()\n",
    "    \n",
    "  # Find the index of the row with the maximum value in Dataframe_Column_Row_Count\n",
    "  best_match['Dataframe_Column_Row_Count'] = best_match['Dataframe_Column_Row_Count'].astype(float)\n",
    "  max_index_row_count = best_match['Dataframe_Column_Row_Count'].idxmax()\n",
    "\n",
    "  if max_index_fuzzy_score == max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_fuzzy_score] > 0 and best_match['Dataframe_Column_Fuzzy_Match_Score'][max_index_fuzzy_score] > 80:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Fuzzy_Match_Score'][max_index_fuzzy_score] > 80:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_fuzzy_score] > 0:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "    flag = 1\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_row_count] > 0:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_row_count]\n",
    "    flag = 1\n",
    "  else:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_row_count]\n",
    "    flag = 1\n",
    "  #end\n",
    "\n",
    "  if flag == 0:\n",
    "    silver_df['mobilenumber'] = df[max_row_column_name].fillna('')\n",
    "  else:\n",
    "    silver_df['mobilenumber'] = np.nan\n",
    "\n",
    "  ##############################################################################################################################################################\n",
    "  # Create an empty DataFrame to store tuples\n",
    "\n",
    "  best_match = pd.DataFrame(columns=[\"Column_Name_Search_Query\", \"Dataframe_Column_Name_Found\", \"Dataframe_Column_Fuzzy_Match_Score\", \"Dataframe_Column_Row_Count\"])  # Replace with your column names  \n",
    "  \n",
    "  #start\n",
    "  flag = 0\n",
    "  for search_col in search_column_name_firstname:\n",
    "  \n",
    "    data_to_add = fuzzy_column_match(df, search_col)\n",
    "\n",
    "    new_row = pd.DataFrame([data_to_add]) # converting to pandas dataframe\n",
    "\n",
    "    best_match = pd.concat([best_match, new_row], ignore_index=True) # append to a new dataframe\n",
    "\n",
    "  # Find the index of the row with the maximum value in Dataframe_Column_Fuzzy_Match_Score\n",
    "  best_match['Dataframe_Column_Fuzzy_Match_Score'] = best_match['Dataframe_Column_Fuzzy_Match_Score'].astype(float)\n",
    "  max_index_fuzzy_score = best_match['Dataframe_Column_Fuzzy_Match_Score'].idxmax()\n",
    "    \n",
    "  # Find the index of the row with the maximum value in Dataframe_Column_Row_Count\n",
    "  best_match['Dataframe_Column_Row_Count'] = best_match['Dataframe_Column_Row_Count'].astype(float)\n",
    "  max_index_row_count = best_match['Dataframe_Column_Row_Count'].idxmax()  \n",
    "\n",
    "  if max_index_fuzzy_score == max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_fuzzy_score] > 0 and best_match['Dataframe_Column_Fuzzy_Match_Score'][max_index_fuzzy_score] > 80:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Fuzzy_Match_Score'][max_index_fuzzy_score] > 80:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_fuzzy_score] > 0:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "    flag = 1\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_row_count] > 0:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_row_count]\n",
    "    flag = 1\n",
    "  else:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_row_count]\n",
    "    flag = 1\n",
    "  #end\n",
    "\n",
    "  if flag == 0:\n",
    "    silver_df['firstname'] = df[max_row_column_name].fillna('')\n",
    "  else:\n",
    "    silver_df['firstname'] = np.nan\n",
    "\n",
    "  ##############################################################################################################################################################\n",
    "  # Create an empty DataFrame to store tuples\n",
    "\n",
    "  best_match = pd.DataFrame(columns=[\"Column_Name_Search_Query\", \"Dataframe_Column_Name_Found\", \"Dataframe_Column_Fuzzy_Match_Score\", \"Dataframe_Column_Row_Count\"])  # Replace with your column names    \n",
    "  \n",
    "  #start\n",
    "  flag = 0\n",
    "  for search_col in search_column_name_middlename:\n",
    "  \n",
    "    data_to_add = fuzzy_column_match(df, search_col)\n",
    "\n",
    "    new_row = pd.DataFrame([data_to_add]) # converting to pandas dataframe\n",
    "\n",
    "    best_match = pd.concat([best_match, new_row], ignore_index=True) # append to a new dataframe\n",
    "\n",
    "  # Find the index of the row with the maximum value in Dataframe_Column_Fuzzy_Match_Score\n",
    "  best_match['Dataframe_Column_Fuzzy_Match_Score'] = best_match['Dataframe_Column_Fuzzy_Match_Score'].astype(float)\n",
    "  max_index_fuzzy_score = best_match['Dataframe_Column_Fuzzy_Match_Score'].idxmax()\n",
    "    \n",
    "  # Find the index of the row with the maximum value in Dataframe_Column_Row_Count\n",
    "  best_match['Dataframe_Column_Row_Count'] = best_match['Dataframe_Column_Row_Count'].astype(float)\n",
    "  max_index_row_count = best_match['Dataframe_Column_Row_Count'].idxmax()  \n",
    "\n",
    "  if max_index_fuzzy_score == max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_fuzzy_score] > 0 and best_match['Dataframe_Column_Fuzzy_Match_Score'][max_index_fuzzy_score] > 80:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Fuzzy_Match_Score'][max_index_fuzzy_score] > 80:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score] \n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_fuzzy_score] > 0:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "    flag = 1\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_row_count] > 0:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_row_count]\n",
    "    flag = 1\n",
    "  else:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_row_count]\n",
    "    flag = 1\n",
    "  #end\n",
    "\n",
    "  if flag == 0:\n",
    "    silver_df['middlename'] = df[max_row_column_name].fillna('')\n",
    "  else:\n",
    "    silver_df['middlename'] = np.nan\n",
    "\n",
    "  ##############################################################################################################################################################\n",
    "  # Create an empty DataFrame to store tuples\n",
    "\n",
    "  best_match = pd.DataFrame(columns=[\"Column_Name_Search_Query\", \"Dataframe_Column_Name_Found\", \"Dataframe_Column_Fuzzy_Match_Score\", \"Dataframe_Column_Row_Count\"])  # Replace with your column names\n",
    "  \n",
    "  #start\n",
    "  flag = 0\n",
    "  for search_col in search_column_name_lastname:\n",
    "\n",
    "    data_to_add = fuzzy_column_match(df, search_col)\n",
    "\n",
    "    new_row = pd.DataFrame([data_to_add]) # converting to pandas dataframe\n",
    "\n",
    "    best_match = pd.concat([best_match, new_row], ignore_index=True) # append to a new dataframe\n",
    "\n",
    "  # Find the index of the row with the maximum value in Dataframe_Column_Fuzzy_Match_Score\n",
    "  best_match['Dataframe_Column_Fuzzy_Match_Score'] = best_match['Dataframe_Column_Fuzzy_Match_Score'].astype(float)\n",
    "  max_index_fuzzy_score = best_match['Dataframe_Column_Fuzzy_Match_Score'].idxmax()\n",
    "    \n",
    "  # Find the index of the row with the maximum value in Dataframe_Column_Row_Count\n",
    "  best_match['Dataframe_Column_Row_Count'] = best_match['Dataframe_Column_Row_Count'].astype(float)\n",
    "  max_index_row_count = best_match['Dataframe_Column_Row_Count'].idxmax()  \n",
    "\n",
    "  if max_index_fuzzy_score == max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_fuzzy_score] > 0 and best_match['Dataframe_Column_Fuzzy_Match_Score'][max_index_fuzzy_score] > 80:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Fuzzy_Match_Score'][max_index_fuzzy_score] > 80:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_fuzzy_score] > 0:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "    flag = 1\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_row_count] > 0:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_row_count]\n",
    "    flag = 1\n",
    "  else:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_row_count]\n",
    "    flag = 1\n",
    "  #end\n",
    "\n",
    "  if flag == 0:\n",
    "    silver_df['lastname'] = df[max_row_column_name].fillna('')\n",
    "  else:\n",
    "    silver_df['lastname'] = np.nan\n",
    "\n",
    "  ##############################################################################################################################################################\n",
    "  # Create an empty DataFrame to store tuples\n",
    "\n",
    "  best_match = pd.DataFrame(columns=[\"Column_Name_Search_Query\", \"Dataframe_Column_Name_Found\", \"Dataframe_Column_Fuzzy_Match_Score\", \"Dataframe_Column_Row_Count\"])  # Replace with your column names\n",
    "\n",
    "  #start\n",
    "  flag = 0\n",
    "  for search_col in search_column_name_addr1:\n",
    "\n",
    "    data_to_add = fuzzy_column_match(df, search_col)\n",
    "\n",
    "    new_row = pd.DataFrame([data_to_add]) # converting to pandas dataframe\n",
    "\n",
    "    best_match = pd.concat([best_match, new_row], ignore_index=True) # append to a new dataframe\n",
    "\n",
    "  # Find the index of the row with the maximum value in Dataframe_Column_Fuzzy_Match_Score\n",
    "  best_match['Dataframe_Column_Fuzzy_Match_Score'] = best_match['Dataframe_Column_Fuzzy_Match_Score'].astype(float)\n",
    "  max_index_fuzzy_score = best_match['Dataframe_Column_Fuzzy_Match_Score'].idxmax()\n",
    "    \n",
    "  # Find the index of the row with the maximum value in Dataframe_Column_Row_Count\n",
    "  best_match['Dataframe_Column_Row_Count'] = best_match['Dataframe_Column_Row_Count'].astype(float)\n",
    "  max_index_row_count = best_match['Dataframe_Column_Row_Count'].idxmax()  \n",
    "\n",
    "  if max_index_fuzzy_score == max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_fuzzy_score] > 0 and best_match['Dataframe_Column_Fuzzy_Match_Score'][max_index_fuzzy_score] > 80:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Fuzzy_Match_Score'][max_index_fuzzy_score] > 80:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]   \n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_fuzzy_score] > 0:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "    flag = 1\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_row_count] > 0:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_row_count]\n",
    "    flag = 1\n",
    "  else:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_row_count]\n",
    "    flag = 1\n",
    "  #end\n",
    "\n",
    "  if flag == 0:\n",
    "    silver_df['addr1'] = df[max_row_column_name].fillna('')\n",
    "  else:\n",
    "    silver_df['addr1'] = np.nan\n",
    "\n",
    "  ##############################################################################################################################################################\n",
    "  # Create an empty DataFrame to store tuples\n",
    "\n",
    "  best_match = pd.DataFrame(columns=[\"Column_Name_Search_Query\", \"Dataframe_Column_Name_Found\", \"Dataframe_Column_Fuzzy_Match_Score\", \"Dataframe_Column_Row_Count\"])  # Replace with your column names\n",
    "\n",
    "  #start\n",
    "  flag = 0\n",
    "  for search_col in search_column_name_addr2:\n",
    "\n",
    "    data_to_add = fuzzy_column_match(df, search_col)\n",
    "\n",
    "    new_row = pd.DataFrame([data_to_add]) # converting to pandas dataframe\n",
    "\n",
    "    best_match = pd.concat([best_match, new_row], ignore_index=True) # append to a new dataframe\n",
    "\n",
    "  # Find the index of the row with the maximum value in Dataframe_Column_Fuzzy_Match_Score\n",
    "  best_match['Dataframe_Column_Fuzzy_Match_Score'] = best_match['Dataframe_Column_Fuzzy_Match_Score'].astype(float)\n",
    "  max_index_fuzzy_score = best_match['Dataframe_Column_Fuzzy_Match_Score'].idxmax()\n",
    "    \n",
    "  # Find the index of the row with the maximum value in Dataframe_Column_Row_Count\n",
    "  best_match['Dataframe_Column_Row_Count'] = best_match['Dataframe_Column_Row_Count'].astype(float)\n",
    "  max_index_row_count = best_match['Dataframe_Column_Row_Count'].idxmax()  \n",
    "\n",
    "  if max_index_fuzzy_score == max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_fuzzy_score] > 0 and best_match['Dataframe_Column_Fuzzy_Match_Score'][max_index_fuzzy_score] > 80:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Fuzzy_Match_Score'][max_index_fuzzy_score] > 80:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_fuzzy_score] > 0:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "    flag = 1\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_row_count] > 0:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_row_count]\n",
    "    flag = 1\n",
    "  else:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_row_count]\n",
    "    flag = 1\n",
    "  #end\n",
    "\n",
    "  if flag == 0:\n",
    "    silver_df['addr2'] = df[max_row_column_name].fillna('')\n",
    "  else:\n",
    "    silver_df['addr2'] = np.nan\n",
    "\n",
    "  ##############################################################################################################################################################\n",
    "  # Create an empty DataFrame to store tuples\n",
    "\n",
    "  best_match = pd.DataFrame(columns=[\"Column_Name_Search_Query\", \"Dataframe_Column_Name_Found\", \"Dataframe_Column_Fuzzy_Match_Score\", \"Dataframe_Column_Row_Count\"])  # Replace with your column names\n",
    "\n",
    "  #start\n",
    "  flag = 0\n",
    "  for search_col in search_column_name_city:\n",
    "\n",
    "    data_to_add = fuzzy_column_match(df, search_col)\n",
    "\n",
    "    new_row = pd.DataFrame([data_to_add]) # converting to pandas dataframe\n",
    "\n",
    "    best_match = pd.concat([best_match, new_row], ignore_index=True) # append to a new dataframe\n",
    "\n",
    "  # Find the index of the row with the maximum value in Dataframe_Column_Fuzzy_Match_Score\n",
    "  best_match['Dataframe_Column_Fuzzy_Match_Score'] = best_match['Dataframe_Column_Fuzzy_Match_Score'].astype(float)\n",
    "  max_index_fuzzy_score = best_match['Dataframe_Column_Fuzzy_Match_Score'].idxmax()\n",
    "    \n",
    "  # Find the index of the row with the maximum value in Dataframe_Column_Row_Count\n",
    "  best_match['Dataframe_Column_Row_Count'] = best_match['Dataframe_Column_Row_Count'].astype(float)\n",
    "  max_index_row_count = best_match['Dataframe_Column_Row_Count'].idxmax()  \n",
    "\n",
    "  if max_index_fuzzy_score == max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_fuzzy_score] > 0 and best_match['Dataframe_Column_Fuzzy_Match_Score'][max_index_fuzzy_score] > 80:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Fuzzy_Match_Score'][max_index_fuzzy_score] > 80:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_fuzzy_score] > 0:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "    flag = 1\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_row_count] > 0:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_row_count]\n",
    "    flag = 1\n",
    "  else:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_row_count]\n",
    "    flag = 1\n",
    "  #end\n",
    "\n",
    "  if flag == 0:\n",
    "    silver_df['city'] = df[max_row_column_name].fillna('')\n",
    "  else:\n",
    "    silver_df['city'] = np.nan\n",
    "\n",
    "  ##############################################################################################################################################################\n",
    "  # Create an empty DataFrame to store tuples\n",
    "\n",
    "  best_match = pd.DataFrame(columns=[\"Column_Name_Search_Query\", \"Dataframe_Column_Name_Found\", \"Dataframe_Column_Fuzzy_Match_Score\", \"Dataframe_Column_Row_Count\"])  # Replace with your column names\n",
    "\n",
    "  #start\n",
    "  flag = 0\n",
    "  for search_col in search_column_name_state:\n",
    "\n",
    "    data_to_add = fuzzy_column_match(df, search_col)\n",
    "\n",
    "    new_row = pd.DataFrame([data_to_add]) # converting to pandas dataframe\n",
    "\n",
    "    best_match = pd.concat([best_match, new_row], ignore_index=True) # append to a new dataframe\n",
    "\n",
    "  # Find the index of the row with the maximum value in Dataframe_Column_Fuzzy_Match_Score\n",
    "  best_match['Dataframe_Column_Fuzzy_Match_Score'] = best_match['Dataframe_Column_Fuzzy_Match_Score'].astype(float)\n",
    "  max_index_fuzzy_score = best_match['Dataframe_Column_Fuzzy_Match_Score'].idxmax()\n",
    "    \n",
    "  # Find the index of the row with the maximum value in Dataframe_Column_Row_Count\n",
    "  best_match['Dataframe_Column_Row_Count'] = best_match['Dataframe_Column_Row_Count'].astype(float)\n",
    "  max_index_row_count = best_match['Dataframe_Column_Row_Count'].idxmax()  \n",
    "\n",
    "  if max_index_fuzzy_score == max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_fuzzy_score] > 0 and best_match['Dataframe_Column_Fuzzy_Match_Score'][max_index_fuzzy_score] > 80:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Fuzzy_Match_Score'][max_index_fuzzy_score] > 80:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_fuzzy_score] > 0:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "    flag = 1\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_row_count] > 0:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_row_count]\n",
    "    flag = 1\n",
    "  else:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_row_count]\n",
    "    flag = 1\n",
    "  #end\n",
    "\n",
    "  if flag == 0:\n",
    "    silver_df['state'] = df[max_row_column_name].fillna('')\n",
    "  else:\n",
    "    silver_df['state'] = np.nan\n",
    "\n",
    "  ##############################################################################################################################################################\n",
    "  # Create an empty DataFrame to store tuples\n",
    "\n",
    "  best_match = pd.DataFrame(columns=[\"Column_Name_Search_Query\", \"Dataframe_Column_Name_Found\", \"Dataframe_Column_Fuzzy_Match_Score\", \"Dataframe_Column_Row_Count\"])  # Replace with your column names\n",
    "\n",
    "  #start\n",
    "  flag = 0\n",
    "  for search_col in search_column_name_zip:\n",
    "\n",
    "    data_to_add = fuzzy_column_match(df, search_col)\n",
    "\n",
    "    new_row = pd.DataFrame([data_to_add]) # converting to pandas dataframe\n",
    "\n",
    "    best_match = pd.concat([best_match, new_row], ignore_index=True) # append to a new dataframe\n",
    "\n",
    "  # Find the index of the row with the maximum value in Dataframe_Column_Fuzzy_Match_Score\n",
    "  best_match['Dataframe_Column_Fuzzy_Match_Score'] = best_match['Dataframe_Column_Fuzzy_Match_Score'].astype(float)\n",
    "  max_index_fuzzy_score = best_match['Dataframe_Column_Fuzzy_Match_Score'].idxmax()\n",
    "    \n",
    "  # Find the index of the row with the maximum value in Dataframe_Column_Row_Count\n",
    "  best_match['Dataframe_Column_Row_Count'] = best_match['Dataframe_Column_Row_Count'].astype(float)\n",
    "  max_index_row_count = best_match['Dataframe_Column_Row_Count'].idxmax()  \n",
    "\n",
    "  if max_index_fuzzy_score == max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_fuzzy_score] > 0 and best_match['Dataframe_Column_Fuzzy_Match_Score'][max_index_fuzzy_score] > 80:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Fuzzy_Match_Score'][max_index_fuzzy_score] > 80:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_fuzzy_score] > 0:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "    flag = 1\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_row_count] > 0:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_row_count]\n",
    "    flag = 1\n",
    "  else:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_row_count]\n",
    "    flag = 1\n",
    "  #end\n",
    "\n",
    "  if flag == 0:\n",
    "    silver_df['zip'] = df[max_row_column_name].fillna('')\n",
    "  else:\n",
    "    silver_df['zip'] = np.nan\n",
    "\n",
    "  ##############################################################################################################################################################\n",
    "  # Create an empty DataFrame to store tuples\n",
    "\n",
    "  best_match = pd.DataFrame(columns=[\"Column_Name_Search_Query\", \"Dataframe_Column_Name_Found\", \"Dataframe_Column_Fuzzy_Match_Score\", \"Dataframe_Column_Row_Count\"])  # Replace with your column names\n",
    "\n",
    "  #start\n",
    "  flag = 0\n",
    "  for search_col in search_column_name_optouturl:\n",
    "\n",
    "    data_to_add = fuzzy_column_match(df, search_col)\n",
    "\n",
    "    new_row = pd.DataFrame([data_to_add]) # converting to pandas dataframe\n",
    "\n",
    "    best_match = pd.concat([best_match, new_row], ignore_index=True) # append to a new dataframe\n",
    "\n",
    "  # Find the index of the row with the maximum value in Dataframe_Column_Fuzzy_Match_Score\n",
    "  best_match['Dataframe_Column_Fuzzy_Match_Score'] = best_match['Dataframe_Column_Fuzzy_Match_Score'].astype(float)\n",
    "  max_index_fuzzy_score = best_match['Dataframe_Column_Fuzzy_Match_Score'].idxmax()\n",
    "    \n",
    "  # Find the index of the row with the maximum value in Dataframe_Column_Row_Count\n",
    "  best_match['Dataframe_Column_Row_Count'] = best_match['Dataframe_Column_Row_Count'].astype(float)\n",
    "  max_index_row_count = best_match['Dataframe_Column_Row_Count'].idxmax()  \n",
    "\n",
    "  if max_index_fuzzy_score == max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_fuzzy_score] > 0 and best_match['Dataframe_Column_Fuzzy_Match_Score'][max_index_fuzzy_score] > 80:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Fuzzy_Match_Score'][max_index_fuzzy_score] > 80:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_fuzzy_score] > 0:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "    flag = 1\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_row_count] > 0:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_row_count]\n",
    "    flag = 1\n",
    "  else:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_row_count]\n",
    "    flag = 1\n",
    "  #end\n",
    "\n",
    "  if flag == 0:\n",
    "    silver_df['optouturl'] = df[max_row_column_name].fillna('')\n",
    "  else:\n",
    "    silver_df['optouturl'] = np.nan\n",
    "\n",
    "  ##############################################################################################################################################################\n",
    "  # Create an empty DataFrame to store tuples\n",
    "\n",
    "  best_match = pd.DataFrame(columns=[\"Column_Name_Search_Query\", \"Dataframe_Column_Name_Found\", \"Dataframe_Column_Fuzzy_Match_Score\", \"Dataframe_Column_Row_Count\"])  # Replace with your column names\n",
    "\n",
    "  #start\n",
    "  flag = 0\n",
    "  for search_col in search_column_name_var1:\n",
    "\n",
    "    data_to_add = fuzzy_column_match(df, search_col)\n",
    "\n",
    "    new_row = pd.DataFrame([data_to_add]) # converting to pandas dataframe\n",
    "\n",
    "    best_match = pd.concat([best_match, new_row], ignore_index=True) # append to a new dataframe\n",
    "\n",
    "  # Find the index of the row with the maximum value in Dataframe_Column_Fuzzy_Match_Score\n",
    "  best_match['Dataframe_Column_Fuzzy_Match_Score'] = best_match['Dataframe_Column_Fuzzy_Match_Score'].astype(float)\n",
    "  max_index_fuzzy_score = best_match['Dataframe_Column_Fuzzy_Match_Score'].idxmax()\n",
    "    \n",
    "  # Find the index of the row with the maximum value in Dataframe_Column_Row_Count\n",
    "  best_match['Dataframe_Column_Row_Count'] = best_match['Dataframe_Column_Row_Count'].astype(float)\n",
    "  max_index_row_count = best_match['Dataframe_Column_Row_Count'].idxmax()  \n",
    "\n",
    "  if max_index_fuzzy_score == max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_fuzzy_score] > 0 and best_match['Dataframe_Column_Fuzzy_Match_Score'][max_index_fuzzy_score] > 80:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Fuzzy_Match_Score'][max_index_fuzzy_score] > 80:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_fuzzy_score] > 0:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "    flag = 1\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_row_count] > 0:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_row_count]\n",
    "    flag = 1\n",
    "  else:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_row_count]\n",
    "    flag = 1\n",
    "  #end\n",
    "\n",
    "  if flag == 0:\n",
    "    silver_df['var1'] = df[max_row_column_name].fillna('')\n",
    "  else:\n",
    "    silver_df['var1'] = np.nan\n",
    "\n",
    "  ##############################################################################################################################################################\n",
    "  # Create an empty DataFrame to store tuples\n",
    "\n",
    "  best_match = pd.DataFrame(columns=[\"Column_Name_Search_Query\", \"Dataframe_Column_Name_Found\", \"Dataframe_Column_Fuzzy_Match_Score\", \"Dataframe_Column_Row_Count\"])  # Replace with your column names\n",
    "\n",
    "  #start\n",
    "  flag = 0\n",
    "  for search_col in search_column_name_var2:\n",
    "\n",
    "    data_to_add = fuzzy_column_match(df, search_col)\n",
    "\n",
    "    new_row = pd.DataFrame([data_to_add]) # converting to pandas dataframe\n",
    "\n",
    "    best_match = pd.concat([best_match, new_row], ignore_index=True) # append to a new dataframe\n",
    "\n",
    "  # Find the index of the row with the maximum value in Dataframe_Column_Fuzzy_Match_Score\n",
    "  best_match['Dataframe_Column_Fuzzy_Match_Score'] = best_match['Dataframe_Column_Fuzzy_Match_Score'].astype(float)\n",
    "  max_index_fuzzy_score = best_match['Dataframe_Column_Fuzzy_Match_Score'].idxmax()\n",
    "    \n",
    "  # Find the index of the row with the maximum value in Dataframe_Column_Row_Count\n",
    "  best_match['Dataframe_Column_Row_Count'] = best_match['Dataframe_Column_Row_Count'].astype(float)\n",
    "  max_index_row_count = best_match['Dataframe_Column_Row_Count'].idxmax()  \n",
    "\n",
    "  if max_index_fuzzy_score == max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_fuzzy_score] > 0 and best_match['Dataframe_Column_Fuzzy_Match_Score'][max_index_fuzzy_score] > 80:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Fuzzy_Match_Score'][max_index_fuzzy_score] > 80:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_fuzzy_score] > 0:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "    flag = 1\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_row_count] > 0:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_row_count]\n",
    "    flag = 1\n",
    "  else:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_row_count]\n",
    "    flag = 1\n",
    "  #end\n",
    "\n",
    "  if flag == 0:\n",
    "    silver_df['var2'] = df[max_row_column_name].fillna('')\n",
    "  else:\n",
    "    silver_df['var2'] = np.nan\n",
    "\n",
    "  ##############################################################################################################################################################\n",
    "  # Create an empty DataFrame to store tuples\n",
    "\n",
    "  best_match = pd.DataFrame(columns=[\"Column_Name_Search_Query\", \"Dataframe_Column_Name_Found\", \"Dataframe_Column_Fuzzy_Match_Score\", \"Dataframe_Column_Row_Count\"])  # Replace with your column names\n",
    "\n",
    "  #start\n",
    "  flag = 0\n",
    "  for search_col in search_column_name_var3:\n",
    "\n",
    "    data_to_add = fuzzy_column_match(df, search_col)\n",
    "\n",
    "    new_row = pd.DataFrame([data_to_add]) # converting to pandas dataframe\n",
    "\n",
    "    best_match = pd.concat([best_match, new_row], ignore_index=True) # append to a new dataframe\n",
    "\n",
    "  # Find the index of the row with the maximum value in Dataframe_Column_Fuzzy_Match_Score\n",
    "  best_match['Dataframe_Column_Fuzzy_Match_Score'] = best_match['Dataframe_Column_Fuzzy_Match_Score'].astype(float)\n",
    "  max_index_fuzzy_score = best_match['Dataframe_Column_Fuzzy_Match_Score'].idxmax()\n",
    "    \n",
    "  # Find the index of the row with the maximum value in Dataframe_Column_Row_Count\n",
    "  best_match['Dataframe_Column_Row_Count'] = best_match['Dataframe_Column_Row_Count'].astype(float)\n",
    "  max_index_row_count = best_match['Dataframe_Column_Row_Count'].idxmax()  \n",
    "\n",
    "  if max_index_fuzzy_score == max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_fuzzy_score] > 0 and best_match['Dataframe_Column_Fuzzy_Match_Score'][max_index_fuzzy_score] > 80:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Fuzzy_Match_Score'][max_index_fuzzy_score] > 80:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_fuzzy_score] > 0:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "    flag = 1\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_row_count] > 0:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_row_count]\n",
    "    flag = 1\n",
    "  else:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_row_count]\n",
    "    flag = 1\n",
    "  #end\n",
    "\n",
    "  if flag == 0:\n",
    "    silver_df['var3'] = df[max_row_column_name].fillna('')\n",
    "  else:\n",
    "    silver_df['var3'] = np.nan\n",
    "\n",
    "  ##############################################################################################################################################################\n",
    "  # Create an empty DataFrame to store tuples\n",
    "\n",
    "  best_match = pd.DataFrame(columns=[\"Column_Name_Search_Query\", \"Dataframe_Column_Name_Found\", \"Dataframe_Column_Fuzzy_Match_Score\", \"Dataframe_Column_Row_Count\"])  # Replace with your column names\n",
    "\n",
    "  #start\n",
    "  flag = 0\n",
    "  for search_col in search_column_name_var4:\n",
    "\n",
    "    data_to_add = fuzzy_column_match(df, search_col)\n",
    "\n",
    "    new_row = pd.DataFrame([data_to_add]) # converting to pandas dataframe\n",
    "\n",
    "    best_match = pd.concat([best_match, new_row], ignore_index=True) # append to a new dataframe\n",
    "\n",
    "  # Find the index of the row with the maximum value in Dataframe_Column_Fuzzy_Match_Score\n",
    "  best_match['Dataframe_Column_Fuzzy_Match_Score'] = best_match['Dataframe_Column_Fuzzy_Match_Score'].astype(float)\n",
    "  max_index_fuzzy_score = best_match['Dataframe_Column_Fuzzy_Match_Score'].idxmax()\n",
    "    \n",
    "  # Find the index of the row with the maximum value in Dataframe_Column_Row_Count\n",
    "  best_match['Dataframe_Column_Row_Count'] = best_match['Dataframe_Column_Row_Count'].astype(float)\n",
    "  max_index_row_count = best_match['Dataframe_Column_Row_Count'].idxmax()  \n",
    "\n",
    "  if max_index_fuzzy_score == max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_fuzzy_score] > 0 and best_match['Dataframe_Column_Fuzzy_Match_Score'][max_index_fuzzy_score] > 80:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Fuzzy_Match_Score'][max_index_fuzzy_score] > 80:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_fuzzy_score] > 0:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "    flag = 1\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_row_count] > 0:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_row_count]\n",
    "    flag = 1\n",
    "  else:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_row_count]\n",
    "    flag = 1\n",
    "  #end\n",
    "\n",
    "  if flag == 0:\n",
    "    silver_df['var4'] = df[max_row_column_name].fillna('')\n",
    "  else:\n",
    "    silver_df['var4'] = np.nan\n",
    "\n",
    "  ##############################################################################################################################################################\n",
    "  # Create an empty DataFrame to store tuples\n",
    "\n",
    "  best_match = pd.DataFrame(columns=[\"Column_Name_Search_Query\", \"Dataframe_Column_Name_Found\", \"Dataframe_Column_Fuzzy_Match_Score\", \"Dataframe_Column_Row_Count\"])  # Replace with your column names\n",
    "\n",
    "  #start\n",
    "  flag = 0\n",
    "  for search_col in search_column_name_cid:\n",
    "\n",
    "    data_to_add = fuzzy_column_match(df, search_col)\n",
    "\n",
    "    new_row = pd.DataFrame([data_to_add]) # converting to pandas dataframe\n",
    "\n",
    "    best_match = pd.concat([best_match, new_row], ignore_index=True) # append to a new dataframe\n",
    "\n",
    "  # Find the index of the row with the maximum value in Dataframe_Column_Fuzzy_Match_Score\n",
    "  best_match['Dataframe_Column_Fuzzy_Match_Score'] = best_match['Dataframe_Column_Fuzzy_Match_Score'].astype(float)\n",
    "  max_index_fuzzy_score = best_match['Dataframe_Column_Fuzzy_Match_Score'].idxmax()\n",
    "    \n",
    "  # Find the index of the row with the maximum value in Dataframe_Column_Row_Count\n",
    "  best_match['Dataframe_Column_Row_Count'] = best_match['Dataframe_Column_Row_Count'].astype(float)\n",
    "  max_index_row_count = best_match['Dataframe_Column_Row_Count'].idxmax()  \n",
    "\n",
    "  if max_index_fuzzy_score == max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_fuzzy_score] > 0 and best_match['Dataframe_Column_Fuzzy_Match_Score'][max_index_fuzzy_score] > 80:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Fuzzy_Match_Score'][max_index_fuzzy_score] > 80:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_fuzzy_score] > 0:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "    flag = 1\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_row_count] > 0:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_row_count]\n",
    "    flag = 1\n",
    "  else:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_row_count]\n",
    "    flag = 1\n",
    "  #end\n",
    "\n",
    "  if flag == 0:\n",
    "    silver_df['cid'] = df[max_row_column_name].fillna('')\n",
    "  else:\n",
    "    silver_df['cid'] = np.nan\n",
    "\n",
    "  ##############################################################################################################################################################\n",
    "  # Create an empty DataFrame to store tuples\n",
    "\n",
    "  best_match = pd.DataFrame(columns=[\"Column_Name_Search_Query\", \"Dataframe_Column_Name_Found\", \"Dataframe_Column_Fuzzy_Match_Score\", \"Dataframe_Column_Row_Count\"])  # Replace with your column names\n",
    "\n",
    "  #start\n",
    "  flag = 0\n",
    "  for search_col in search_column_name_subscriberkey:\n",
    "\n",
    "    data_to_add = fuzzy_column_match(df, search_col)\n",
    "\n",
    "    new_row = pd.DataFrame([data_to_add]) # converting to pandas dataframe\n",
    "\n",
    "    best_match = pd.concat([best_match, new_row], ignore_index=True) # append to a new dataframe\n",
    "\n",
    "  # Find the index of the row with the maximum value in Dataframe_Column_Fuzzy_Match_Score\n",
    "  best_match['Dataframe_Column_Fuzzy_Match_Score'] = best_match['Dataframe_Column_Fuzzy_Match_Score'].astype(float)\n",
    "  max_index_fuzzy_score = best_match['Dataframe_Column_Fuzzy_Match_Score'].idxmax()\n",
    "    \n",
    "  # Find the index of the row with the maximum value in Dataframe_Column_Row_Count\n",
    "  best_match['Dataframe_Column_Row_Count'] = best_match['Dataframe_Column_Row_Count'].astype(float)\n",
    "  max_index_row_count = best_match['Dataframe_Column_Row_Count'].idxmax()  \n",
    "\n",
    "  if max_index_fuzzy_score == max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_fuzzy_score] > 0 and best_match['Dataframe_Column_Fuzzy_Match_Score'][max_index_fuzzy_score] > 80:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Fuzzy_Match_Score'][max_index_fuzzy_score] > 80:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_fuzzy_score] > 0:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "    flag = 1\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_row_count] > 0:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_row_count]\n",
    "    flag = 1\n",
    "  else:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_row_count]\n",
    "    flag = 1\n",
    "  #end\n",
    "\n",
    "  if flag == 0:\n",
    "    silver_df['subscriberkey'] = df[max_row_column_name].fillna('')\n",
    "  else:\n",
    "    silver_df['subscriberkey'] = np.nan\n",
    "\n",
    "  ##############################################################################################################################################################\n",
    "  # Create an empty DataFrame to store tuples\n",
    "\n",
    "  best_match = pd.DataFrame(columns=[\"Column_Name_Search_Query\", \"Dataframe_Column_Name_Found\", \"Dataframe_Column_Fuzzy_Match_Score\", \"Dataframe_Column_Row_Count\"])  # Replace with your column names\n",
    "\n",
    "  #start\n",
    "  flag = 0\n",
    "  for search_col in search_column_name_createddate:\n",
    "\n",
    "    data_to_add = fuzzy_column_match(df, search_col)\n",
    "\n",
    "    new_row = pd.DataFrame([data_to_add]) # converting to pandas dataframe\n",
    "\n",
    "    best_match = pd.concat([best_match, new_row], ignore_index=True) # append to a new dataframe\n",
    "\n",
    "  # Find the index of the row with the maximum value in Dataframe_Column_Fuzzy_Match_Score\n",
    "  best_match['Dataframe_Column_Fuzzy_Match_Score'] = best_match['Dataframe_Column_Fuzzy_Match_Score'].astype(float)\n",
    "  max_index_fuzzy_score = best_match['Dataframe_Column_Fuzzy_Match_Score'].idxmax()\n",
    "    \n",
    "  # Find the index of the row with the maximum value in Dataframe_Column_Row_Count\n",
    "  best_match['Dataframe_Column_Row_Count'] = best_match['Dataframe_Column_Row_Count'].astype(float)\n",
    "  max_index_row_count = best_match['Dataframe_Column_Row_Count'].idxmax()  \n",
    "\n",
    "  if max_index_fuzzy_score == max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_fuzzy_score] > 0 and best_match['Dataframe_Column_Fuzzy_Match_Score'][max_index_fuzzy_score] > 80:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Fuzzy_Match_Score'][max_index_fuzzy_score] > 80:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_fuzzy_score] > 0:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "    flag = 1\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_row_count] > 0:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_row_count]\n",
    "    flag = 1\n",
    "  else:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_row_count]\n",
    "    flag = 1\n",
    "  #end\n",
    "\n",
    "  if flag == 0:\n",
    "    silver_df['createddate'] = df[max_row_column_name].fillna('')\n",
    "  else:\n",
    "    silver_df['createddate'] = np.nan\n",
    "\n",
    "  ##############################################################################################################################################################\n",
    "  # Create an empty DataFrame to store tuples\n",
    "\n",
    "  best_match = pd.DataFrame(columns=[\"Column_Name_Search_Query\", \"Dataframe_Column_Name_Found\", \"Dataframe_Column_Fuzzy_Match_Score\", \"Dataframe_Column_Row_Count\"])  # Replace with your column names\n",
    "\n",
    "  #start\n",
    "  flag = 0  \n",
    "  for search_col in search_column_name_lastmodifieddate:\n",
    "\n",
    "    data_to_add = fuzzy_column_match(df, search_col)\n",
    "\n",
    "    new_row = pd.DataFrame([data_to_add]) # converting to pandas dataframe\n",
    "\n",
    "    best_match = pd.concat([best_match, new_row], ignore_index=True) # append to a new dataframe\n",
    "\n",
    "  # Find the index of the row with the maximum value in Dataframe_Column_Fuzzy_Match_Score\n",
    "  best_match['Dataframe_Column_Fuzzy_Match_Score'] = best_match['Dataframe_Column_Fuzzy_Match_Score'].astype(float)\n",
    "  max_index_fuzzy_score = best_match['Dataframe_Column_Fuzzy_Match_Score'].idxmax()\n",
    "    \n",
    "  # Find the index of the row with the maximum value in Dataframe_Column_Row_Count\n",
    "  best_match['Dataframe_Column_Row_Count'] = best_match['Dataframe_Column_Row_Count'].astype(float)\n",
    "  max_index_row_count = best_match['Dataframe_Column_Row_Count'].idxmax()  \n",
    "\n",
    "  if max_index_fuzzy_score == max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_fuzzy_score] > 0 and best_match['Dataframe_Column_Fuzzy_Match_Score'][max_index_fuzzy_score] > 80:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Fuzzy_Match_Score'][max_index_fuzzy_score] > 80:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_fuzzy_score] > 0:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "    flag = 1\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_row_count] > 0:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_row_count]\n",
    "    flag = 1\n",
    "  else:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_row_count]\n",
    "    flag = 1\n",
    "  #end\n",
    "\n",
    "  if flag == 0:\n",
    "    silver_df['lastmodifieddate'] = df[max_row_column_name].fillna('')\n",
    "  else:\n",
    "    silver_df['lastmodifieddate'] = np.nan\n",
    "\n",
    "  ##############################################################################################################################################################\n",
    "  # Create an empty DataFrame to store tuples\n",
    "\n",
    "  best_match = pd.DataFrame(columns=[\"Column_Name_Search_Query\", \"Dataframe_Column_Name_Found\", \"Dataframe_Column_Fuzzy_Match_Score\", \"Dataframe_Column_Row_Count\"])  # Replace with your column names\n",
    "\n",
    "  #start\n",
    "  flag = 0\n",
    "  for search_col in search_column_name_campaignname:\n",
    "  \n",
    "    data_to_add = fuzzy_column_match(df, search_col)\n",
    "\n",
    "    new_row = pd.DataFrame([data_to_add]) # converting to pandas dataframe\n",
    "\n",
    "    best_match = pd.concat([best_match, new_row], ignore_index=True) # append to a new dataframe\n",
    "\n",
    "  # Find the index of the row with the maximum value in Dataframe_Column_Fuzzy_Match_Score\n",
    "  best_match['Dataframe_Column_Fuzzy_Match_Score'] = best_match['Dataframe_Column_Fuzzy_Match_Score'].astype(float)\n",
    "  max_index_fuzzy_score = best_match['Dataframe_Column_Fuzzy_Match_Score'].idxmax()\n",
    "    \n",
    "  # Find the index of the row with the maximum value in Dataframe_Column_Row_Count\n",
    "  best_match['Dataframe_Column_Row_Count'] = best_match['Dataframe_Column_Row_Count'].astype(float)\n",
    "  max_index_row_count = best_match['Dataframe_Column_Row_Count'].idxmax()  \n",
    "\n",
    "  if max_index_fuzzy_score == max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_fuzzy_score] > 0 and best_match['Dataframe_Column_Fuzzy_Match_Score'][max_index_fuzzy_score] > 80:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Fuzzy_Match_Score'][max_index_fuzzy_score] > 80:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_fuzzy_score] > 0:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "    flag = 1\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_row_count] > 0:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_row_count]\n",
    "    flag = 1\n",
    "  else:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_row_count]\n",
    "    flag = 1\n",
    "  #end\n",
    "\n",
    "  if flag == 0:\n",
    "    silver_df['campaignname'] = df[max_row_column_name].fillna('')\n",
    "  else:\n",
    "    silver_df['campaignname'] = np.nan\n",
    "\n",
    "  ##############################################################################################################################################################\n",
    "  # Create an empty DataFrame to store tuples\n",
    "\n",
    "  best_match = pd.DataFrame(columns=[\"Column_Name_Search_Query\", \"Dataframe_Column_Name_Found\", \"Dataframe_Column_Fuzzy_Match_Score\", \"Dataframe_Column_Row_Count\"])  # Replace with your column names\n",
    "\n",
    "  #start\n",
    "  flag = 0\n",
    "  for search_col in search_column_name_suppressiontype:\n",
    "  \n",
    "    data_to_add = fuzzy_column_match(df, search_col)\n",
    "\n",
    "    new_row = pd.DataFrame([data_to_add]) # converting to pandas dataframe\n",
    "\n",
    "    best_match = pd.concat([best_match, new_row], ignore_index=True) # append to a new dataframe\n",
    "\n",
    "  # Find the index of the row with the maximum value in Dataframe_Column_Fuzzy_Match_Score\n",
    "  best_match['Dataframe_Column_Fuzzy_Match_Score'] = best_match['Dataframe_Column_Fuzzy_Match_Score'].astype(float)\n",
    "  max_index_fuzzy_score = best_match['Dataframe_Column_Fuzzy_Match_Score'].idxmax()\n",
    "    \n",
    "  # Find the index of the row with the maximum value in Dataframe_Column_Row_Count\n",
    "  best_match['Dataframe_Column_Row_Count'] = best_match['Dataframe_Column_Row_Count'].astype(float)\n",
    "  max_index_row_count = best_match['Dataframe_Column_Row_Count'].idxmax()  \n",
    "\n",
    "  if max_index_fuzzy_score == max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_fuzzy_score] > 0 and best_match['Dataframe_Column_Fuzzy_Match_Score'][max_index_fuzzy_score] > 80:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Fuzzy_Match_Score'][max_index_fuzzy_score] > 80:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_fuzzy_score] > 0:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "    flag = 1\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_row_count] > 0:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_row_count]\n",
    "    flag = 1\n",
    "  else:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_row_count]\n",
    "    flag = 1\n",
    "  #end\n",
    "\n",
    "  if flag == 0:\n",
    "    silver_df['suppressiontype'] = df[max_row_column_name].fillna('')\n",
    "  else:\n",
    "    silver_df['suppressiontype'] = np.nan\n",
    "\n",
    "  ##############################################################################################################################################################\n",
    "  # Create an empty DataFrame to store tuples\n",
    "\n",
    "  best_match = pd.DataFrame(columns=[\"Column_Name_Search_Query\", \"Dataframe_Column_Name_Found\", \"Dataframe_Column_Fuzzy_Match_Score\", \"Dataframe_Column_Row_Count\"])  # Replace with your column names\n",
    "\n",
    "  #start\n",
    "  flag = 0\n",
    "  for search_col in search_column_name_campaignrunid:\n",
    "\n",
    "    data_to_add = fuzzy_column_match(df, search_col)\n",
    "\n",
    "    new_row = pd.DataFrame([data_to_add]) # converting to pandas dataframe\n",
    "\n",
    "    best_match = pd.concat([best_match, new_row], ignore_index=True) # append to a new dataframe\n",
    "\n",
    "  # Find the index of the row with the maximum value in Dataframe_Column_Fuzzy_Match_Score\n",
    "  best_match['Dataframe_Column_Fuzzy_Match_Score'] = best_match['Dataframe_Column_Fuzzy_Match_Score'].astype(float)\n",
    "  max_index_fuzzy_score = best_match['Dataframe_Column_Fuzzy_Match_Score'].idxmax()\n",
    "    \n",
    "  # Find the index of the row with the maximum value in Dataframe_Column_Row_Count\n",
    "  best_match['Dataframe_Column_Row_Count'] = best_match['Dataframe_Column_Row_Count'].astype(float)\n",
    "  max_index_row_count = best_match['Dataframe_Column_Row_Count'].idxmax()  \n",
    "\n",
    "  if max_index_fuzzy_score == max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_fuzzy_score] > 0 and best_match['Dataframe_Column_Fuzzy_Match_Score'][max_index_fuzzy_score] > 80:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Fuzzy_Match_Score'][max_index_fuzzy_score] > 80:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_fuzzy_score] > 0:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "    flag = 1\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_row_count] > 0:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_row_count]\n",
    "    flag = 1\n",
    "  else:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_row_count]\n",
    "    flag = 1\n",
    "  #end\n",
    "\n",
    "  if flag == 0:\n",
    "    silver_df['campaignrunid'] = df[max_row_column_name].fillna('')\n",
    "  else:\n",
    "    silver_df['campaignrunid'] = np.nan\n",
    "\n",
    "  ##############################################################################################################################################################\n",
    "  # Create an empty DataFrame to store tuples\n",
    "\n",
    "  best_match = pd.DataFrame(columns=[\"Column_Name_Search_Query\", \"Dataframe_Column_Name_Found\", \"Dataframe_Column_Fuzzy_Match_Score\", \"Dataframe_Column_Row_Count\"])  # Replace with your column names\n",
    "\n",
    "  #start\n",
    "  flag = 0\n",
    "  for search_col in search_column_name_highlowip:\n",
    "\n",
    "    data_to_add = fuzzy_column_match(df, search_col)\n",
    "\n",
    "    new_row = pd.DataFrame([data_to_add]) # converting to pandas dataframe\n",
    "\n",
    "    best_match = pd.concat([best_match, new_row], ignore_index=True) # append to a new dataframe\n",
    "\n",
    "  # Find the index of the row with the maximum value in Dataframe_Column_Fuzzy_Match_Score\n",
    "  best_match['Dataframe_Column_Fuzzy_Match_Score'] = best_match['Dataframe_Column_Fuzzy_Match_Score'].astype(float)\n",
    "  max_index_fuzzy_score = best_match['Dataframe_Column_Fuzzy_Match_Score'].idxmax()\n",
    "    \n",
    "  # Find the index of the row with the maximum value in Dataframe_Column_Row_Count\n",
    "  best_match['Dataframe_Column_Row_Count'] = best_match['Dataframe_Column_Row_Count'].astype(float)\n",
    "  max_index_row_count = best_match['Dataframe_Column_Row_Count'].idxmax()  \n",
    "\n",
    "  if max_index_fuzzy_score == max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_fuzzy_score] > 0 and best_match['Dataframe_Column_Fuzzy_Match_Score'][max_index_fuzzy_score] > 80:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "    silver_df['highlowip'] = df[max_row_column_name].fillna('')    \n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Fuzzy_Match_Score'][max_index_fuzzy_score] > 80:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "    silver_df['highlowip'] = df[max_row_column_name].fillna('')\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_fuzzy_score] > 0:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "    flag = 1\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_row_count] > 0:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_row_count]\n",
    "    flag = 1\n",
    "  else:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_row_count]\n",
    "    flag = 1\n",
    "  #end\n",
    "  \n",
    "  if flag == 0:\n",
    "    silver_df['highlowip'] = df[max_row_column_name].fillna('')\n",
    "  else:\n",
    "    silver_df['highlowip'] = np.nan\n",
    "    \n",
    "  ##############################################################################################################################################################\n",
    "  # Create an empty DataFrame to store tuples\n",
    "\n",
    "  best_match = pd.DataFrame(columns=[\"Column_Name_Search_Query\", \"Dataframe_Column_Name_Found\", \"Dataframe_Column_Fuzzy_Match_Score\", \"Dataframe_Column_Row_Count\"])  # Replace with your column names\n",
    "\n",
    "  #start\n",
    "  flag = 0    \n",
    "  for search_col in search_column_name_sent:\n",
    "\n",
    "    data_to_add = fuzzy_column_match(df, search_col)\n",
    "\n",
    "    new_row = pd.DataFrame([data_to_add]) # converting to pandas dataframe\n",
    "\n",
    "    best_match = pd.concat([best_match, new_row], ignore_index=True) # append to a new dataframe\n",
    "\n",
    "  # Find the index of the row with the maximum value in Dataframe_Column_Fuzzy_Match_Score\n",
    "  best_match['Dataframe_Column_Fuzzy_Match_Score'] = best_match['Dataframe_Column_Fuzzy_Match_Score'].astype(float)\n",
    "  max_index_fuzzy_score = best_match['Dataframe_Column_Fuzzy_Match_Score'].idxmax()\n",
    "    \n",
    "  # Find the index of the row with the maximum value in Dataframe_Column_Row_Count\n",
    "  best_match['Dataframe_Column_Row_Count'] = best_match['Dataframe_Column_Row_Count'].astype(float)\n",
    "  max_index_row_count = best_match['Dataframe_Column_Row_Count'].idxmax()  \n",
    "\n",
    "  if max_index_fuzzy_score == max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_fuzzy_score] > 0 and best_match['Dataframe_Column_Fuzzy_Match_Score'][max_index_fuzzy_score] > 80:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "    silver_df['sent'] = df[max_row_column_name].fillna('')       \n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Fuzzy_Match_Score'][max_index_fuzzy_score] > 80:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "    silver_df['sent'] = df[max_row_column_name].fillna('')    \n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_fuzzy_score] > 0:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "    flag = 1\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_row_count] > 0:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_row_count]\n",
    "    flag = 1\n",
    "  else:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_row_count]\n",
    "    flag = 1\n",
    "  #end\n",
    "\n",
    "  if flag == 0:\n",
    "    silver_df['sent'] = df[max_row_column_name].fillna('')\n",
    "  else:\n",
    "    silver_df['sent'] = np.nan\n",
    "\n",
    "  ##############################################################################################################################################################\n",
    "  # Create an empty DataFrame to store tuples\n",
    "\n",
    "  best_match = pd.DataFrame(columns=[\"Column_Name_Search_Query\", \"Dataframe_Column_Name_Found\", \"Dataframe_Column_Fuzzy_Match_Score\", \"Dataframe_Column_Row_Count\"])  # Replace with your column names\n",
    "\n",
    "  #start\n",
    "  flag = 0\n",
    "  for search_col in search_column_name_sentdate:\n",
    "\n",
    "    data_to_add = fuzzy_column_match(df, search_col)\n",
    "\n",
    "    new_row = pd.DataFrame([data_to_add]) # converting to pandas dataframe\n",
    "\n",
    "    best_match = pd.concat([best_match, new_row], ignore_index=True) # append to a new dataframe\n",
    "\n",
    "  # Find the index of the row with the maximum value in Dataframe_Column_Fuzzy_Match_Score\n",
    "  best_match['Dataframe_Column_Fuzzy_Match_Score'] = best_match['Dataframe_Column_Fuzzy_Match_Score'].astype(float)\n",
    "  max_index_fuzzy_score = best_match['Dataframe_Column_Fuzzy_Match_Score'].idxmax()\n",
    "    \n",
    "  # Find the index of the row with the maximum value in Dataframe_Column_Row_Count\n",
    "  best_match['Dataframe_Column_Row_Count'] = best_match['Dataframe_Column_Row_Count'].astype(float)\n",
    "  max_index_row_count = best_match['Dataframe_Column_Row_Count'].idxmax()  \n",
    "\n",
    "  if max_index_fuzzy_score == max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_fuzzy_score] > 0 and best_match['Dataframe_Column_Fuzzy_Match_Score'][max_index_fuzzy_score] > 80:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Fuzzy_Match_Score'][max_index_fuzzy_score] > 80:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_fuzzy_score] > 0:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "    flag = 1\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_row_count] > 0:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_row_count]\n",
    "    flag = 1\n",
    "  else:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_row_count]\n",
    "    flag = 1\n",
    "  #end\n",
    "\n",
    "  if flag == 0:\n",
    "    silver_df['sentdate'] = df[max_row_column_name].fillna('')\n",
    "  else:\n",
    "    silver_df['sentdate'] = np.nan\n",
    "\n",
    "  ##############################################################################################################################################################\n",
    "  # Create an empty DataFrame to store tuples\n",
    "\n",
    "  best_match = pd.DataFrame(columns=[\"Column_Name_Search_Query\", \"Dataframe_Column_Name_Found\", \"Dataframe_Column_Fuzzy_Match_Score\", \"Dataframe_Column_Row_Count\"])  # Replace with your column names\n",
    "\n",
    "  #start\n",
    "  flag = 0\n",
    "  for search_col in search_column_name_open:\n",
    "\n",
    "    data_to_add = fuzzy_column_match(df, search_col)\n",
    "\n",
    "    new_row = pd.DataFrame([data_to_add]) # converting to pandas dataframe\n",
    "\n",
    "    best_match = pd.concat([best_match, new_row], ignore_index=True) # append to a new dataframe\n",
    "\n",
    "  #print(best_match)\n",
    "  # Find the index of the row with the maximum value in Dataframe_Column_Fuzzy_Match_Score\n",
    "  best_match['Dataframe_Column_Fuzzy_Match_Score'] = best_match['Dataframe_Column_Fuzzy_Match_Score'].astype(float)\n",
    "  max_index_fuzzy_score = best_match['Dataframe_Column_Fuzzy_Match_Score'].idxmax()\n",
    "    \n",
    "  # Find the index of the row with the maximum value in Dataframe_Column_Row_Count\n",
    "  best_match['Dataframe_Column_Row_Count'] = best_match['Dataframe_Column_Row_Count'].astype(float)\n",
    "  max_index_row_count = best_match['Dataframe_Column_Row_Count'].idxmax()  \n",
    "\n",
    "  if max_index_fuzzy_score == max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_fuzzy_score] > 0 and best_match['Dataframe_Column_Fuzzy_Match_Score'][max_index_fuzzy_score] > 80:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Fuzzy_Match_Score'][max_index_fuzzy_score] > 80:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_fuzzy_score] > 0:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "    flag = 1\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_row_count] > 0:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_row_count]\n",
    "    flag = 1\n",
    "  else:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_row_count]\n",
    "    flag = 1\n",
    "  #end\n",
    "\n",
    "  if flag == 0:\n",
    "    silver_df['open'] = df[max_row_column_name].fillna('')\n",
    "  else:\n",
    "    silver_df['open'] = np.nan\n",
    "\n",
    "  ##############################################################################################################################################################\n",
    "  # Create an empty DataFrame to store tuples\n",
    "\n",
    "  best_match = pd.DataFrame(columns=[\"Column_Name_Search_Query\", \"Dataframe_Column_Name_Found\", \"Dataframe_Column_Fuzzy_Match_Score\", \"Dataframe_Column_Row_Count\"])  # Replace with your column names\n",
    "\n",
    "  #start\n",
    "  flag = 0\n",
    "  for search_col in search_column_name_opendate:\n",
    "\n",
    "    data_to_add = fuzzy_column_match(df, search_col)\n",
    "\n",
    "    new_row = pd.DataFrame([data_to_add]) # converting to pandas dataframe\n",
    "\n",
    "    best_match = pd.concat([best_match, new_row], ignore_index=True) # append to a new dataframe\n",
    "\n",
    "  #print(best_match)\n",
    "  # Find the index of the row with the maximum value in Dataframe_Column_Fuzzy_Match_Score\n",
    "  best_match['Dataframe_Column_Fuzzy_Match_Score'] = best_match['Dataframe_Column_Fuzzy_Match_Score'].astype(float)\n",
    "  max_index_fuzzy_score = best_match['Dataframe_Column_Fuzzy_Match_Score'].idxmax()\n",
    "    \n",
    "  # Find the index of the row with the maximum value in Dataframe_Column_Row_Count\n",
    "  best_match['Dataframe_Column_Row_Count'] = best_match['Dataframe_Column_Row_Count'].astype(float)\n",
    "  max_index_row_count = best_match['Dataframe_Column_Row_Count'].idxmax()  \n",
    "\n",
    "  if max_index_fuzzy_score == max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_fuzzy_score] > 0 and best_match['Dataframe_Column_Fuzzy_Match_Score'][max_index_fuzzy_score] > 80:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Fuzzy_Match_Score'][max_index_fuzzy_score] > 80:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_fuzzy_score] > 0:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "    flag = 1\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_row_count] > 0:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_row_count]\n",
    "    flag = 1\n",
    "  else:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_row_count]\n",
    "    flag = 1\n",
    "  #end\n",
    "\n",
    "  if flag == 0:\n",
    "    silver_df['opendate'] = df[max_row_column_name].fillna('')\n",
    "  else:\n",
    "    silver_df['opendate'] = np.nan\n",
    "\n",
    "  ##############################################################################################################################################################\n",
    "  # Create an empty DataFrame to store tuples\n",
    "\n",
    "  best_match = pd.DataFrame(columns=[\"Column_Name_Search_Query\", \"Dataframe_Column_Name_Found\", \"Dataframe_Column_Fuzzy_Match_Score\", \"Dataframe_Column_Row_Count\"])  # Replace with your column names\n",
    "\n",
    "  #start\n",
    "  flag = 0\n",
    "  for search_col in search_column_name_click:\n",
    "\n",
    "    data_to_add = fuzzy_column_match(df, search_col)\n",
    "\n",
    "    new_row = pd.DataFrame([data_to_add]) # converting to pandas dataframe\n",
    "\n",
    "    best_match = pd.concat([best_match, new_row], ignore_index=True) # append to a new dataframe\n",
    "\n",
    "  #print(best_match)\n",
    "  # Find the index of the row with the maximum value in Dataframe_Column_Fuzzy_Match_Score\n",
    "  best_match['Dataframe_Column_Fuzzy_Match_Score'] = best_match['Dataframe_Column_Fuzzy_Match_Score'].astype(float)\n",
    "  max_index_fuzzy_score = best_match['Dataframe_Column_Fuzzy_Match_Score'].idxmax()\n",
    "    \n",
    "  # Find the index of the row with the maximum value in Dataframe_Column_Row_Count\n",
    "  best_match['Dataframe_Column_Row_Count'] = best_match['Dataframe_Column_Row_Count'].astype(float)\n",
    "  max_index_row_count = best_match['Dataframe_Column_Row_Count'].idxmax()  \n",
    "\n",
    "  if max_index_fuzzy_score == max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_fuzzy_score] > 0 and best_match['Dataframe_Column_Fuzzy_Match_Score'][max_index_fuzzy_score] > 80:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Fuzzy_Match_Score'][max_index_fuzzy_score] > 80:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_fuzzy_score] > 0:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "    flag = 1\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_row_count] > 0:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_row_count]\n",
    "    flag = 1\n",
    "  else:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_row_count]\n",
    "    flag = 1\n",
    "  #end\n",
    "\n",
    "  if flag == 0:\n",
    "    silver_df['click'] = df[max_row_column_name].fillna('')\n",
    "  else:\n",
    "    silver_df['click'] = np.nan\n",
    "\n",
    "  ##############################################################################################################################################################\n",
    "  # Create an empty DataFrame to store tuples\n",
    "\n",
    "  best_match = pd.DataFrame(columns=[\"Column_Name_Search_Query\", \"Dataframe_Column_Name_Found\", \"Dataframe_Column_Fuzzy_Match_Score\", \"Dataframe_Column_Row_Count\"])  # Replace with your column names\n",
    "\n",
    "  #start\n",
    "  flag = 0\n",
    "  for search_col in search_column_name_clickdate:\n",
    "\n",
    "    data_to_add = fuzzy_column_match(df, search_col)\n",
    "\n",
    "    new_row = pd.DataFrame([data_to_add]) # converting to pandas dataframe\n",
    "\n",
    "    best_match = pd.concat([best_match, new_row], ignore_index=True) # append to a new dataframe\n",
    "\n",
    "  #print(best_match)\n",
    "  # Find the index of the row with the maximum value in Dataframe_Column_Fuzzy_Match_Score\n",
    "  best_match['Dataframe_Column_Fuzzy_Match_Score'] = best_match['Dataframe_Column_Fuzzy_Match_Score'].astype(float)\n",
    "  max_index_fuzzy_score = best_match['Dataframe_Column_Fuzzy_Match_Score'].idxmax()\n",
    "    \n",
    "  # Find the index of the row with the maximum value in Dataframe_Column_Row_Count\n",
    "  best_match['Dataframe_Column_Row_Count'] = best_match['Dataframe_Column_Row_Count'].astype(float)\n",
    "  max_index_row_count = best_match['Dataframe_Column_Row_Count'].idxmax()  \n",
    "\n",
    "  if max_index_fuzzy_score == max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_fuzzy_score] > 0 and best_match['Dataframe_Column_Fuzzy_Match_Score'][max_index_fuzzy_score] > 80:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Fuzzy_Match_Score'][max_index_fuzzy_score] > 80:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_fuzzy_score] > 0:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "    flag = 1\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_row_count] > 0:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_row_count]\n",
    "    flag = 1\n",
    "  else:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_row_count]\n",
    "    flag = 1\n",
    "  #end\n",
    "\n",
    "  if flag == 0:\n",
    "    silver_df['clickdate'] = df[max_row_column_name].fillna('')\n",
    "  else:\n",
    "    silver_df['clickdate'] = np.nan\n",
    "\n",
    "  ##############################################################################################################################################################\n",
    "  # Create an empty DataFrame to store tuples\n",
    "\n",
    "  best_match = pd.DataFrame(columns=[\"Column_Name_Search_Query\", \"Dataframe_Column_Name_Found\", \"Dataframe_Column_Fuzzy_Match_Score\", \"Dataframe_Column_Row_Count\"])  # Replace with your column names\n",
    "\n",
    "  #start\n",
    "  flag = 0\n",
    "  for search_col in search_column_name_hardbounce:\n",
    "\n",
    "    data_to_add = fuzzy_column_match(df, search_col)\n",
    "\n",
    "    new_row = pd.DataFrame([data_to_add]) # converting to pandas dataframe\n",
    "\n",
    "    best_match = pd.concat([best_match, new_row], ignore_index=True) # append to a new dataframe\n",
    "\n",
    "  #print(best_match)\n",
    "  # Find the index of the row with the maximum value in Dataframe_Column_Fuzzy_Match_Score\n",
    "  best_match['Dataframe_Column_Fuzzy_Match_Score'] = best_match['Dataframe_Column_Fuzzy_Match_Score'].astype(float)\n",
    "  max_index_fuzzy_score = best_match['Dataframe_Column_Fuzzy_Match_Score'].idxmax()\n",
    "    \n",
    "  # Find the index of the row with the maximum value in Dataframe_Column_Row_Count\n",
    "  best_match['Dataframe_Column_Row_Count'] = best_match['Dataframe_Column_Row_Count'].astype(float)\n",
    "  max_index_row_count = best_match['Dataframe_Column_Row_Count'].idxmax()  \n",
    "\n",
    "  if max_index_fuzzy_score == max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_fuzzy_score] > 0 and best_match['Dataframe_Column_Fuzzy_Match_Score'][max_index_fuzzy_score] > 80:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Fuzzy_Match_Score'][max_index_fuzzy_score] > 80:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_fuzzy_score] > 0:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "    flag = 1\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_row_count] > 0:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_row_count]\n",
    "    flag = 1\n",
    "  else:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_row_count]\n",
    "    flag = 1\n",
    "  #end\n",
    "\n",
    "  if flag == 0:\n",
    "    silver_df['hardbounce'] = df[max_row_column_name].fillna('')\n",
    "  else:\n",
    "    silver_df['hardbounce'] = np.nan\n",
    "\n",
    "  ##############################################################################################################################################################\n",
    "  # Create an empty DataFrame to store tuples\n",
    "\n",
    "  best_match = pd.DataFrame(columns=[\"Column_Name_Search_Query\", \"Dataframe_Column_Name_Found\", \"Dataframe_Column_Fuzzy_Match_Score\", \"Dataframe_Column_Row_Count\"])  # Replace with your column names\n",
    "\n",
    "  #start\n",
    "  flag = 0\n",
    "  for search_col in search_column_name_softbounce:\n",
    "\n",
    "    data_to_add = fuzzy_column_match(df, search_col)\n",
    "\n",
    "    new_row = pd.DataFrame([data_to_add]) # converting to pandas dataframe\n",
    "\n",
    "    best_match = pd.concat([best_match, new_row], ignore_index=True) # append to a new dataframe\n",
    "\n",
    "  #print(best_match)\n",
    "  # Find the index of the row with the maximum value in Dataframe_Column_Fuzzy_Match_Score\n",
    "  best_match['Dataframe_Column_Fuzzy_Match_Score'] = best_match['Dataframe_Column_Fuzzy_Match_Score'].astype(float)\n",
    "  max_index_fuzzy_score = best_match['Dataframe_Column_Fuzzy_Match_Score'].idxmax()\n",
    "    \n",
    "  # Find the index of the row with the maximum value in Dataframe_Column_Row_Count\n",
    "  best_match['Dataframe_Column_Row_Count'] = best_match['Dataframe_Column_Row_Count'].astype(float)\n",
    "  max_index_row_count = best_match['Dataframe_Column_Row_Count'].idxmax()  \n",
    "\n",
    "  if max_index_fuzzy_score == max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_fuzzy_score] > 0 and best_match['Dataframe_Column_Fuzzy_Match_Score'][max_index_fuzzy_score] > 80:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Fuzzy_Match_Score'][max_index_fuzzy_score] > 80:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_fuzzy_score] > 0:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "    flag = 1\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_row_count] > 0:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_row_count]\n",
    "    flag = 1\n",
    "  else:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_row_count]\n",
    "    flag = 1\n",
    "  #end\n",
    "\n",
    "  if flag == 0:\n",
    "    silver_df['softbounce'] = df[max_row_column_name].fillna('')\n",
    "  else:\n",
    "    silver_df['softbounce'] = np.nan\n",
    "\n",
    "  ##############################################################################################################################################################\n",
    "  # Create an empty DataFrame to store tuples\n",
    "\n",
    "  best_match = pd.DataFrame(columns=[\"Column_Name_Search_Query\", \"Dataframe_Column_Name_Found\", \"Dataframe_Column_Fuzzy_Match_Score\", \"Dataframe_Column_Row_Count\"])  # Replace with your column names\n",
    "\n",
    "  #start\n",
    "  flag = 0\n",
    "  for search_col in search_column_name_blockedbounce:\n",
    "\n",
    "    data_to_add = fuzzy_column_match(df, search_col)\n",
    "\n",
    "    new_row = pd.DataFrame([data_to_add]) # converting to pandas dataframe\n",
    "\n",
    "    best_match = pd.concat([best_match, new_row], ignore_index=True) # append to a new dataframe\n",
    "\n",
    "  #print(best_match)\n",
    "  # Find the index of the row with the maximum value in Dataframe_Column_Fuzzy_Match_Score\n",
    "  best_match['Dataframe_Column_Fuzzy_Match_Score'] = best_match['Dataframe_Column_Fuzzy_Match_Score'].astype(float)\n",
    "  max_index_fuzzy_score = best_match['Dataframe_Column_Fuzzy_Match_Score'].idxmax()\n",
    "    \n",
    "  # Find the index of the row with the maximum value in Dataframe_Column_Row_Count\n",
    "  best_match['Dataframe_Column_Row_Count'] = best_match['Dataframe_Column_Row_Count'].astype(float)\n",
    "  max_index_row_count = best_match['Dataframe_Column_Row_Count'].idxmax()  \n",
    "\n",
    "  if max_index_fuzzy_score == max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_fuzzy_score] > 0 and best_match['Dataframe_Column_Fuzzy_Match_Score'][max_index_fuzzy_score] > 80:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Fuzzy_Match_Score'][max_index_fuzzy_score] > 80:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_fuzzy_score] > 0:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "    flag = 1\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_row_count] > 0:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_row_count]\n",
    "    flag = 1\n",
    "  else:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_row_count]\n",
    "    flag = 1\n",
    "  #end\n",
    "\n",
    "  if flag == 0:\n",
    "    silver_df['blockedbounce'] = df[max_row_column_name].fillna('')\n",
    "  else:\n",
    "    silver_df['blockedbounce'] = np.nan\n",
    "\n",
    "  ##############################################################################################################################################################\n",
    "  # Create an empty DataFrame to store tuples\n",
    "\n",
    "  best_match = pd.DataFrame(columns=[\"Column_Name_Search_Query\", \"Dataframe_Column_Name_Found\", \"Dataframe_Column_Fuzzy_Match_Score\", \"Dataframe_Column_Row_Count\"])  # Replace with your column names\n",
    "\n",
    "  #start\n",
    "  flag = 0\n",
    "  for search_col in search_column_name_unsub:\n",
    "\n",
    "    data_to_add = fuzzy_column_match(df, search_col)\n",
    "\n",
    "    new_row = pd.DataFrame([data_to_add]) # converting to pandas dataframe\n",
    "\n",
    "    best_match = pd.concat([best_match, new_row], ignore_index=True) # append to a new dataframe\n",
    "\n",
    "  #print(best_match)\n",
    "  # Find the index of the row with the maximum value in Dataframe_Column_Fuzzy_Match_Score\n",
    "  best_match['Dataframe_Column_Fuzzy_Match_Score'] = best_match['Dataframe_Column_Fuzzy_Match_Score'].astype(float)\n",
    "  max_index_fuzzy_score = best_match['Dataframe_Column_Fuzzy_Match_Score'].idxmax()\n",
    "    \n",
    "  # Find the index of the row with the maximum value in Dataframe_Column_Row_Count\n",
    "  best_match['Dataframe_Column_Row_Count'] = best_match['Dataframe_Column_Row_Count'].astype(float)\n",
    "  max_index_row_count = best_match['Dataframe_Column_Row_Count'].idxmax()  \n",
    "\n",
    "  if max_index_fuzzy_score == max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_fuzzy_score] > 0 and best_match['Dataframe_Column_Fuzzy_Match_Score'][max_index_fuzzy_score] > 80:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "    silver_df['unsub'] = df[max_row_column_name].fillna('')    \n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Fuzzy_Match_Score'][max_index_fuzzy_score] > 80:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "    silver_df['unsub'] = df[max_row_column_name].fillna('')\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_fuzzy_score] > 0:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_fuzzy_score]\n",
    "    flag = 1\n",
    "  elif max_index_fuzzy_score != max_index_row_count and best_match['Dataframe_Column_Row_Count'][max_index_row_count] > 0:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_row_count]\n",
    "    flag = 1\n",
    "  else:\n",
    "    max_row_column_name = best_match['Dataframe_Column_Name_Found'].loc[max_index_row_count]\n",
    "    flag = 1\n",
    "  #end\n",
    "\n",
    "  if flag == 0:\n",
    "    silver_df['unsub'] = df[max_row_column_name].fillna('')\n",
    "  else:\n",
    "    silver_df['unsub'] = np.nan\n",
    "\n",
    "  ##############################################################################################################################################################\n",
    "\n",
    "  \n",
    "  silver_df['file_name'] = file_name\n",
    "  silver_df['file_name'] = silver_df['file_name'].fillna('')\n",
    "\n",
    "  ##############################################################################################################################################################\n",
    "\n",
    "  silver_dataframe = pd.concat([silver_dataframe, silver_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b5a3798-ac4e-4243-84ee-76726365f331",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#####################################################################################################################################\n",
    "#Step 0\n",
    "#####################################################################################################################################\n",
    "# Create rowid column in dataframe\n",
    "silver_dataframe['rowid'] = np.arange(silver_dataframe.shape[0])\n",
    "\n",
    "#####################################################################################################################################\n",
    "# Step 1\n",
    "#####################################################################################################################################\n",
    "# Converting the date column\n",
    "silver_dataframe['createddate'] = pd.to_datetime(silver_dataframe['createddate'])\n",
    "# Extract just the date part for non-null values\n",
    "silver_dataframe['createddate'] = silver_dataframe['createddate'].dt.date\n",
    "silver_dataframe['createddate'] = silver_dataframe['createddate'].astype(str)\n",
    "\n",
    "# Converting the date column\n",
    "silver_dataframe['lastmodifieddate'] = pd.to_datetime(silver_dataframe['lastmodifieddate'])\n",
    "# Extract just the date part for non-null values\n",
    "silver_dataframe['lastmodifieddate'] = silver_dataframe['lastmodifieddate'].dt.date\n",
    "silver_dataframe['lastmodifieddate'] = silver_dataframe['lastmodifieddate'].astype(str)\n",
    "\n",
    "# Converting the date column\n",
    "silver_dataframe['sentdate'] = pd.to_datetime(silver_dataframe['sentdate'])\n",
    "# Extract just the date part for non-null values\n",
    "silver_dataframe['sentdate'] = silver_dataframe['sentdate'].dt.date\n",
    "silver_dataframe['sentdate'] = silver_dataframe['sentdate'].astype(str)\n",
    "\n",
    "\n",
    "# Converting the date column\n",
    "silver_dataframe['opendate'] = pd.to_datetime(silver_dataframe['opendate'])\n",
    "# Extract just the date part for non-null values\n",
    "silver_dataframe['opendate'] = silver_dataframe['opendate'].dt.date\n",
    "silver_dataframe['opendate'] = silver_dataframe['opendate'].astype(str)\n",
    "\n",
    "# Converting the date column\n",
    "silver_dataframe['clickdate'] = pd.to_datetime(silver_dataframe['clickdate'])\n",
    "# Extract just the date part for non-null values\n",
    "silver_dataframe['clickdate'] = silver_dataframe['clickdate'].dt.date\n",
    "silver_dataframe['clickdate'] = silver_dataframe['clickdate'].astype(str)\n",
    "\n",
    "# Converting the date column\n",
    "silver_dataframe['hardbounce'] = pd.to_datetime(silver_dataframe['hardbounce'])\n",
    "# Extract just the date part for non-null values\n",
    "silver_dataframe['hardbounce'] = silver_dataframe['hardbounce'].dt.date\n",
    "silver_dataframe['hardbounce'] = silver_dataframe['hardbounce'].astype(str)\n",
    "\n",
    "# Converting the date column\n",
    "silver_dataframe['softbounce'] = pd.to_datetime(silver_dataframe['softbounce'])\n",
    "# Extract just the date part for non-null values\n",
    "silver_dataframe['softbounce'] = silver_dataframe['softbounce'].dt.date\n",
    "silver_dataframe['softbounce'] = silver_dataframe['softbounce'].astype(str)\n",
    "\n",
    "# Converting the date column\n",
    "silver_dataframe['blockedbounce'] = pd.to_datetime(silver_dataframe['blockedbounce'])\n",
    "# Extract just the date part for non-null values\n",
    "silver_dataframe['blockedbounce'] = silver_dataframe['blockedbounce'].dt.date\n",
    "silver_dataframe['blockedbounce'] = silver_dataframe['blockedbounce'].astype(str)\n",
    "\n",
    "# Converting the date column\n",
    "silver_dataframe['unsub'] = pd.to_datetime(silver_dataframe['unsub'])\n",
    "# Extract just the date part for non-null values\n",
    "silver_dataframe['unsub'] = silver_dataframe['unsub'].dt.date\n",
    "silver_dataframe['unsub'] = silver_dataframe['unsub'].astype(str)\n",
    "\n",
    "#####################################################################################################################################\n",
    "# Step 2\n",
    "#####################################################################################################################################\n",
    "def split_name(row):\n",
    "    if pd.isnull(row['lastname']) and isinstance(row['firstname'], str) and ' ' in row['firstname'].strip():  # first check for string type before checking for spaces\n",
    "        parts = row['firstname'].split()\n",
    "        row['firstname'] = parts[0]  # Take the first part for the first name\n",
    "        row['lastname'] = ' '.join(parts[1:])  # Combine the remaining parts for the last name\n",
    "    return row\n",
    "\n",
    "# Apply the function to each row\n",
    "silver_dataframe = silver_dataframe.apply(split_name, axis=1)\n",
    "\n",
    "#####################################################################################################################################\n",
    "# Step 3\n",
    "#####################################################################################################################################\n",
    "# Function to update 'cid' if it's null\n",
    "def update_cid(row):\n",
    "    if pd.isnull(row['cid']):\n",
    "        # Concatenate non-null values from var1 to var4\n",
    "        row['cid'] = ''.join([str(row[var]) for var in ['var1', 'var2', 'var3', 'var4'] if pd.notnull(row[var])])\n",
    "    return row\n",
    "\n",
    "# Apply the function to each row\n",
    "silver_dataframe = silver_dataframe.apply(update_cid, axis=1)\n",
    "\n",
    "#####################################################################################################################################\n",
    "# Step 4\n",
    "#####################################################################################################################################\n",
    "\n",
    "#change order of columns\n",
    "new_order = ['rowid', 'emailaddress', 'mobilenumber', 'firstname', 'middlename', 'lastname', 'addr1', 'addr2', 'city', 'state', 'zip', 'optouturl', 'var1', 'var2', 'var3', 'var4', 'cid', 'subscriberkey', 'createddate', 'lastmodifieddate', 'campaignname', 'suppressiontype', 'campaignrunid', 'highlowip', 'sent', 'sentdate', 'open', 'opendate', 'click', 'clickdate', 'hardbounce', 'softbounce', 'blockedbounce', 'unsub', 'file_name']\n",
    "\n",
    "# Rearrange the columns\n",
    "silver_dataframe = silver_dataframe[new_order]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "399475ca-8420-4d4e-aa03-be689f073062",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Silver Layer - Move to Delta Lake table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b28f72d-f5ed-4d9a-9995-fc22d045aee0",
     "showTitle": true,
     "title": "Create Schema so that errors can be captured"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(silver_dataframe, schema=\"rowid BIGINT, emailaddress STRING, mobilenumber STRING, firstname STRING, middlename STRING, lastname STRING, addr1 STRING, addr2 STRING, city STRING, state STRING, zip STRING, optouturl STRING, var1 STRING, var2 STRING, var3 STRING, var4 STRING, cid STRING, subscriberkey STRING, createddate STRING, lastmodifieddate STRING, campaignname STRING, suppressiontype STRING, campaignrunid STRING, highlowip STRING, sent STRING, sentdate STRING, open STRING, opendate STRING, click STRING, clickdate STRING, hardbounce STRING, softbounce STRING, blockedbounce STRING, unsub STRING, file_name STRING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "327ca3b2-61de-4b1b-8c10-1313edaf12e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.format(\"delta\").mode(\"append\").partitionBy(\"file_name\").saveAsTable(\"sle_mun_data_silver_layer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e777d759-597a-4891-8d53-6a66b9f6bdb1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Gold Layer - Harmonize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99522a45-cbff-4f02-9d9b-264a170a774f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>metrics</th></tr></thead><tbody><tr><td>dbfs:/user/hive/warehouse/ma_mun.db/sle_mun_data_gold_layer</td><td>List(1, 67, List(4344073, 4344073, 4344073.0, 1, 4344073), List(66451, 387753, 82870.55223880598, 67, 5552327), 0, null, 1, 67, 0, true, 0, 0, 1701437483874, 1701437492508, 64, 1, null, List(0, 0), 35, 32, 5614, 0, null)</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/user/hive/warehouse/ma_mun.db/sle_mun_data_gold_layer",
         [
          1,
          67,
          [
           4344073,
           4344073,
           4344073.0,
           1,
           4344073
          ],
          [
           66451,
           387753,
           82870.55223880598,
           67,
           5552327
          ],
          0,
          null,
          1,
          67,
          0,
          true,
          0,
          0,
          1701437483874,
          1701437492508,
          64,
          1,
          null,
          [
           0,
           0
          ],
          35,
          32,
          5614,
          0,
          null
         ]
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": "_sqldf",
        "executionCount": 14
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "metrics",
         "type": "{\"type\":\"struct\",\"fields\":[{\"name\":\"numFilesAdded\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"numFilesRemoved\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"filesAdded\",\"type\":{\"type\":\"struct\",\"fields\":[{\"name\":\"min\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"max\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"avg\",\"type\":\"double\",\"nullable\":false,\"metadata\":{}},{\"name\":\"totalFiles\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"totalSize\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}}]},\"nullable\":true,\"metadata\":{}},{\"name\":\"filesRemoved\",\"type\":{\"type\":\"struct\",\"fields\":[{\"name\":\"min\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"max\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"avg\",\"type\":\"double\",\"nullable\":false,\"metadata\":{}},{\"name\":\"totalFiles\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"totalSize\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}}]},\"nullable\":true,\"metadata\":{}},{\"name\":\"partitionsOptimized\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"zOrderStats\",\"type\":{\"type\":\"struct\",\"fields\":[{\"name\":\"strategyName\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"inputCubeFiles\",\"type\":{\"type\":\"struct\",\"fields\":[{\"name\":\"num\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"size\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}}]},\"nullable\":true,\"metadata\":{}},{\"name\":\"inputOtherFiles\",\"type\":{\"type\":\"struct\",\"fields\":[{\"name\":\"num\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"size\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}}]},\"nullable\":true,\"metadata\":{}},{\"name\":\"inputNumCubes\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"mergedFiles\",\"type\":{\"type\":\"struct\",\"fields\":[{\"name\":\"num\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"size\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}}]},\"nullable\":true,\"metadata\":{}},{\"name\":\"numOutputCubes\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"mergedNumCubes\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}}]},\"nullable\":true,\"metadata\":{}},{\"name\":\"numBatches\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"totalConsideredFiles\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"totalFilesSkipped\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"preserveInsertionOrder\",\"type\":\"boolean\",\"nullable\":false,\"metadata\":{}},{\"name\":\"numFilesSkippedToReduceWriteAmplification\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"numBytesSkippedToReduceWriteAmplification\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"startTimeMs\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"endTimeMs\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"totalClusterParallelism\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"totalScheduledTasks\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"autoCompactParallelismStats\",\"type\":{\"type\":\"struct\",\"fields\":[{\"name\":\"maxClusterActiveParallelism\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"minClusterActiveParallelism\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"maxSessionActiveParallelism\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"minSessionActiveParallelism\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}}]},\"nullable\":true,\"metadata\":{}},{\"name\":\"deletionVectorStats\",\"type\":{\"type\":\"struct\",\"fields\":[{\"name\":\"numDeletionVectorsRemoved\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"numDeletionVectorRowsRemoved\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}}]},\"nullable\":true,\"metadata\":{}},{\"name\":\"numTableColumns\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"numTableColumnsWithStats\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"totalTaskExecutionTimeMs\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"skippedArchivedFiles\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"clusteringMetrics\",\"type\":{\"type\":\"struct\",\"fields\":[{\"name\":\"sizeOfTableInBytesBeforeLazyClustering\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"isNewMetadataCreated\",\"type\":\"boolean\",\"nullable\":false,\"metadata\":{}},{\"name\":\"numFilesClassifiedToIntermediateNodes\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"sizeOfFilesClassifiedToIntermediateNodesInBytes\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"logicalSizeOfFilesClassifiedToIntermediateNodesInBytes\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"numFilesClassifiedToLeafNodes\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"sizeOfFilesClassifiedToLeafNodesInBytes\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"logicalSizeOfFilesClassifiedToLeafNodesInBytes\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"clusterThresholdStrategy\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"numClusteringTasksPlanned\",\"type\":\"integer\",\"nullable\":false,\"metadata\":{}},{\"name\":\"numCompactionTasksPlanned\",\"type\":\"integer\",\"nullable\":false,\"metadata\":{}},{\"name\":\"numOptimizeBatchesPlanned\",\"type\":\"integer\",\"nullable\":false,\"metadata\":{}},{\"name\":\"numLeafNodesExpanded\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"numLeafNodesClustered\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"numLeafNodesCompacted\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"numIntermediateNodesCompacted\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"totalSizeOfDataToCompactInBytes\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"totalLogicalSizeOfDataToCompactInBytes\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"numIntermediateNodesClustered\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"numFilesSkippedAfterExpansion\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"totalSizeOfFilesSkippedAfterExpansionInBytes\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"totalLogicalSizeOfFilesSkippedAfterExpansionInBytes\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"totalSizeOfDataToRewriteInBytes\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"totalLogicalSizeOfDataToRewriteInBytes\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"timeMetrics\",\"type\":{\"type\":\"struct\",\"fields\":[{\"name\":\"classifierTimeMs\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"optimizerTimeMs\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"metadataLoadTimeMs\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"metadataCreationTimeMs\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}}]},\"nullable\":true,\"metadata\":{}}]},\"nullable\":true,\"metadata\":{}}]}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "\n",
    "CREATE OR REPLACE table sle_mun_data_gold_layer using delta as\n",
    "\n",
    "select \n",
    "\n",
    "a.cid as cid,\n",
    "upper(a.firstname) as firstname,\n",
    "upper(a.middlename) as middlename,\n",
    "upper(a.lastname) as lastname,\n",
    "a.subscriberkey as subscriberkey,\n",
    "(case when a.createddate like 'NaT' then null else a.createddate end) as createddate,\n",
    "(case when a.lastmodifieddate like 'NaT' then null else a.lastmodifieddate end)  as lastmodifieddate,\n",
    "a.campaignname as campaignname,\n",
    "a.suppressiontype as suppressiontype,\n",
    "a.campaignrunid as campaignrunid,\n",
    "a.highlowip as highlowip,\n",
    "a.sent as sent,\n",
    "(case when a.sentdate like 'NaT' then null else a.sentdate end)  as sentdate,\n",
    "\n",
    "a.open as open,\n",
    "(case when a.opendate like 'NaT' then null else a.opendate end)  as opendate,\n",
    "a.click as click,\n",
    "(case when a.clickdate like 'NaT' then null else a.clickdate end) as clickdate,\n",
    "(case when a.hardbounce like 'NaT' then null else a.hardbounce end) as hardbounce,\n",
    "(case when a.softbounce like 'NaT' then null else a.softbounce end) as softbounce,\n",
    "(case when a.blockedbounce like 'NaT' then null else a.blockedbounce end)  as blockedbounce,\n",
    "(case when a.unsub like 'NaT' then null else a.unsub end) as unsub,\n",
    "a.file_name as sourcefile,\n",
    "\n",
    "\n",
    "from  sle_mun_data_silver_layer a;\n",
    "optimize sle_mun_data_gold_layer;"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 905542424631740,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Python Based Sweep ETL to read multiple local files in xlsx, csv, xls format",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
